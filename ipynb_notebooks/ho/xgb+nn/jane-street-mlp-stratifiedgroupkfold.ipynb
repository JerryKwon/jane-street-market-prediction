{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:27.671695Z",
     "iopub.status.busy": "2021-02-18T16:02:27.671045Z",
     "iopub.status.idle": "2021-02-18T16:02:33.690202Z",
     "shell.execute_reply": "2021-02-18T16:02:33.689121Z"
    },
    "papermill": {
     "duration": 6.038873,
     "end_time": "2021-02-18T16:02:33.690318",
     "exception": false,
     "start_time": "2021-02-18T16:02:27.651445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from random import choices\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:33.720824Z",
     "iopub.status.busy": "2021-02-18T16:02:33.720222Z",
     "iopub.status.idle": "2021-02-18T16:02:33.723415Z",
     "shell.execute_reply": "2021-02-18T16:02:33.723833Z"
    },
    "papermill": {
     "duration": 0.022084,
     "end_time": "2021-02-18T16:02:33.723939",
     "exception": false,
     "start_time": "2021-02-18T16:02:33.701855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.3.1\n",
      "Accelerated Linear Algebra enabled\n"
     ]
    }
   ],
   "source": [
    "# tf setup\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "MIXED_PRECISION = False\n",
    "XLA_ACCELERATE = True\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)\n",
    "    print('Mixed precision enabled')\n",
    "\n",
    "if XLA_ACCELERATE:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print('Accelerated Linear Algebra enabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011265,
     "end_time": "2021-02-18T16:02:33.746550",
     "exception": false,
     "start_time": "2021-02-18T16:02:33.735285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:33.774557Z",
     "iopub.status.busy": "2021-02-18T16:02:33.773706Z",
     "iopub.status.idle": "2021-02-18T16:02:33.775432Z",
     "shell.execute_reply": "2021-02-18T16:02:33.775894Z"
    },
    "papermill": {
     "duration": 0.017949,
     "end_time": "2021-02-18T16:02:33.775992",
     "exception": false,
     "start_time": "2021-02-18T16:02:33.758043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CV_STRATEGY = 'StratifiedGroupKFold' # 'StratifiedGroupKFold' # GroupKFold, PurgedGroupTimeSeriesSplit\n",
    "SEED = 2021\n",
    "START_DATE = 86\n",
    "FOLDS = 5\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011352,
     "end_time": "2021-02-18T16:02:33.798931",
     "exception": false,
     "start_time": "2021-02-18T16:02:33.787579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV Strategy\n",
    "\n",
    "## PurgedGroupTimeSeriesSplit\n",
    "Click the code button to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:33.844307Z",
     "iopub.status.busy": "2021-02-18T16:02:33.843443Z",
     "iopub.status.idle": "2021-02-18T16:02:33.846133Z",
     "shell.execute_reply": "2021-02-18T16:02:33.845698Z"
    },
    "papermill": {
     "duration": 0.035501,
     "end_time": "2021-02-18T16:02:33.846212",
     "exception": false,
     "start_time": "2021-02-18T16:02:33.810711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011854,
     "end_time": "2021-02-18T16:02:33.869348",
     "exception": false,
     "start_time": "2021-02-18T16:02:33.857494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GroupKFold, StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:33.912014Z",
     "iopub.status.busy": "2021-02-18T16:02:33.911187Z",
     "iopub.status.idle": "2021-02-18T16:02:33.920343Z",
     "shell.execute_reply": "2021-02-18T16:02:33.919817Z"
    },
    "papermill": {
     "duration": 0.038999,
     "end_time": "2021-02-18T16:02:33.920425",
     "exception": false,
     "start_time": "2021-02-18T16:02:33.881426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---- GroupKFold ----\n",
    "class GroupKFold(object):\n",
    "    \"\"\"\n",
    "    GroupKFold with random shuffle with a sklearn-like structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, group=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y, group):\n",
    "        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n",
    "        unique_ids = X[group].unique()\n",
    "        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n",
    "            # split group\n",
    "            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n",
    "            train_idx = np.where(X[group].isin(tr_group))[0]\n",
    "            val_idx = np.where(X[group].isin(va_group))[0]\n",
    "            yield train_idx, val_idx\n",
    "\n",
    "# ---- StratifiedGroupKFold ----\n",
    "class StratifiedGroupKFold(object):\n",
    "    \"\"\"\n",
    "    StratifiedGroupKFold with random shuffle with a sklearn-like structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, group=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y, group):\n",
    "        labels_num = np.max(y) + 1\n",
    "        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "        y_distr = Counter()\n",
    "        groups = X[group].values\n",
    "        for label, g in zip(y, groups):\n",
    "            y_counts_per_group[g][label] += 1\n",
    "            y_distr[label] += 1\n",
    "\n",
    "        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "        groups_per_fold = defaultdict(set)\n",
    "\n",
    "        def eval_y_counts_per_fold(y_counts, fold):\n",
    "            y_counts_per_fold[fold] += y_counts\n",
    "            std_per_label = []\n",
    "            for label in range(labels_num):\n",
    "                label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(self.n_splits)])\n",
    "                std_per_label.append(label_std)\n",
    "            y_counts_per_fold[fold] -= y_counts\n",
    "            return np.mean(std_per_label)\n",
    "        \n",
    "        groups_and_y_counts = list(y_counts_per_group.items())\n",
    "        random.Random(self.random_state).shuffle(groups_and_y_counts)\n",
    "\n",
    "        for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n",
    "            best_fold = None\n",
    "            min_eval = None\n",
    "            for i in range(self.n_splits):\n",
    "                fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "                if min_eval is None or fold_eval < min_eval:\n",
    "                    min_eval = fold_eval\n",
    "                    best_fold = i\n",
    "            y_counts_per_fold[best_fold] += y_counts\n",
    "            groups_per_fold[best_fold].add(g)\n",
    "\n",
    "        all_groups = set(groups)\n",
    "        for i in range(self.n_splits):\n",
    "            train_groups = all_groups - groups_per_fold[i]\n",
    "            test_groups = groups_per_fold[i]\n",
    "\n",
    "            train_idx = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "            test_idx = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "            yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011776,
     "end_time": "2021-02-18T16:02:33.944253",
     "exception": false,
     "start_time": "2021-02-18T16:02:33.932477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:33.976787Z",
     "iopub.status.busy": "2021-02-18T16:02:33.975930Z",
     "iopub.status.idle": "2021-02-18T16:02:45.757934Z",
     "shell.execute_reply": "2021-02-18T16:02:45.758759Z"
    },
    "papermill": {
     "duration": 11.802945,
     "end_time": "2021-02-18T16:02:45.758949",
     "exception": false,
     "start_time": "2021-02-18T16:02:33.956004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_feather('../input/jane-street-save-as-feather/train.feather') # faster data load\n",
    "train = train.query(f'date >= {START_DATE}').reset_index(drop = True) \n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "#train = train.query('weight != 0').reset_index(drop = True)\n",
    "#train['action'] = (train['resp'] > 0).astype('int')\n",
    "train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:45.805428Z",
     "iopub.status.busy": "2021-02-18T16:02:45.804584Z",
     "iopub.status.idle": "2021-02-18T16:02:46.778370Z",
     "shell.execute_reply": "2021-02-18T16:02:46.777419Z"
    },
    "papermill": {
     "duration": 1.000102,
     "end_time": "2021-02-18T16:02:46.778481",
     "exception": false,
     "start_time": "2021-02-18T16:02:45.778379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train = train.sample(10000, random_state=SEED)\n",
    "\n",
    "X = train[features].values\n",
    "y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011553,
     "end_time": "2021-02-18T16:02:46.802209",
     "exception": false,
     "start_time": "2021-02-18T16:02:46.790656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:46.836333Z",
     "iopub.status.busy": "2021-02-18T16:02:46.835704Z",
     "iopub.status.idle": "2021-02-18T16:02:46.839086Z",
     "shell.execute_reply": "2021-02-18T16:02:46.838686Z"
    },
    "papermill": {
     "duration": 0.025317,
     "end_time": "2021-02-18T16:02:46.839170",
     "exception": false,
     "start_time": "2021-02-18T16:02:46.813853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim,output_dim,noise=0.05):\n",
    "    i = tf.keras.layers.Input(input_dim)\n",
    "    encoded = tf.keras.layers.BatchNormalization()(i)\n",
    "    encoded = tf.keras.layers.GaussianNoise(noise)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(64,activation='relu')(encoded)\n",
    "    decoded = tf.keras.layers.Dropout(0.2)(encoded)\n",
    "    decoded = tf.keras.layers.Dense(input_dim,name='decoded')(decoded)\n",
    "    x = tf.keras.layers.Dense(32,activation='relu')(decoded)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(output_dim,activation='sigmoid',name='label_output')(x)\n",
    "    \n",
    "    encoder = tf.keras.models.Model(inputs=i,outputs=encoded)\n",
    "    autoencoder = tf.keras.models.Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), \n",
    "                        loss={'decoded':'mse','label_output':'binary_crossentropy'})\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:46.875061Z",
     "iopub.status.busy": "2021-02-18T16:02:46.874377Z",
     "iopub.status.idle": "2021-02-18T16:02:46.878180Z",
     "shell.execute_reply": "2021-02-18T16:02:46.877721Z"
    },
    "papermill": {
     "duration": 0.027223,
     "end_time": "2021-02-18T16:02:46.878262",
     "exception": false,
     "start_time": "2021-02-18T16:02:46.851039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mlp(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns,))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
    "        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:02:46.908021Z",
     "iopub.status.busy": "2021-02-18T16:02:46.907392Z",
     "iopub.status.idle": "2021-02-18T16:06:22.085789Z",
     "shell.execute_reply": "2021-02-18T16:06:22.084853Z"
    },
    "papermill": {
     "duration": 215.195794,
     "end_time": "2021-02-18T16:06:22.085907",
     "exception": false,
     "start_time": "2021-02-18T16:02:46.890113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "691/691 [==============================] - 5s 7ms/step - loss: 2.3690 - decoded_loss: 1.6593 - label_output_loss: 0.7097 - val_loss: 1.2557 - val_decoded_loss: 0.5654 - val_label_output_loss: 0.6904\n",
      "Epoch 2/1000\n",
      "691/691 [==============================] - 6s 8ms/step - loss: 1.8069 - decoded_loss: 1.1151 - label_output_loss: 0.6919 - val_loss: 1.1670 - val_decoded_loss: 0.4771 - val_label_output_loss: 0.6899\n",
      "Epoch 3/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.7661 - decoded_loss: 1.0750 - label_output_loss: 0.6910 - val_loss: 1.1315 - val_decoded_loss: 0.4420 - val_label_output_loss: 0.6895\n",
      "Epoch 4/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.7437 - decoded_loss: 1.0529 - label_output_loss: 0.6908 - val_loss: 1.1143 - val_decoded_loss: 0.4248 - val_label_output_loss: 0.6894\n",
      "Epoch 5/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.7241 - decoded_loss: 1.0334 - label_output_loss: 0.6907 - val_loss: 1.0937 - val_decoded_loss: 0.4044 - val_label_output_loss: 0.6893\n",
      "Epoch 6/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.7132 - decoded_loss: 1.0225 - label_output_loss: 0.6907 - val_loss: 1.0971 - val_decoded_loss: 0.4077 - val_label_output_loss: 0.6895\n",
      "Epoch 7/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.7088 - decoded_loss: 1.0182 - label_output_loss: 0.6906 - val_loss: 1.0787 - val_decoded_loss: 0.3895 - val_label_output_loss: 0.6892\n",
      "Epoch 8/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6960 - decoded_loss: 1.0055 - label_output_loss: 0.6905 - val_loss: 1.0600 - val_decoded_loss: 0.3710 - val_label_output_loss: 0.6890\n",
      "Epoch 9/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6917 - decoded_loss: 1.0012 - label_output_loss: 0.6905 - val_loss: 1.0746 - val_decoded_loss: 0.3856 - val_label_output_loss: 0.6890\n",
      "Epoch 10/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6829 - decoded_loss: 0.9925 - label_output_loss: 0.6904 - val_loss: 1.0552 - val_decoded_loss: 0.3663 - val_label_output_loss: 0.6890\n",
      "Epoch 11/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6837 - decoded_loss: 0.9934 - label_output_loss: 0.6903 - val_loss: 1.0507 - val_decoded_loss: 0.3618 - val_label_output_loss: 0.6890\n",
      "Epoch 12/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6770 - decoded_loss: 0.9867 - label_output_loss: 0.6903 - val_loss: 1.0590 - val_decoded_loss: 0.3701 - val_label_output_loss: 0.6889\n",
      "Epoch 13/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6790 - decoded_loss: 0.9888 - label_output_loss: 0.6903 - val_loss: 1.0578 - val_decoded_loss: 0.3689 - val_label_output_loss: 0.6889\n",
      "Epoch 14/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6722 - decoded_loss: 0.9819 - label_output_loss: 0.6903 - val_loss: 1.0468 - val_decoded_loss: 0.3577 - val_label_output_loss: 0.6891\n",
      "Epoch 15/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6699 - decoded_loss: 0.9796 - label_output_loss: 0.6902 - val_loss: 1.0493 - val_decoded_loss: 0.3605 - val_label_output_loss: 0.6888\n",
      "Epoch 16/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6676 - decoded_loss: 0.9775 - label_output_loss: 0.6902 - val_loss: 1.0546 - val_decoded_loss: 0.3659 - val_label_output_loss: 0.6887\n",
      "Epoch 17/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6692 - decoded_loss: 0.9790 - label_output_loss: 0.6902 - val_loss: 1.0561 - val_decoded_loss: 0.3670 - val_label_output_loss: 0.6891\n",
      "Epoch 18/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6630 - decoded_loss: 0.9729 - label_output_loss: 0.6902 - val_loss: 1.0495 - val_decoded_loss: 0.3608 - val_label_output_loss: 0.6887\n",
      "Epoch 19/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6641 - decoded_loss: 0.9740 - label_output_loss: 0.6901 - val_loss: 1.0442 - val_decoded_loss: 0.3556 - val_label_output_loss: 0.6885\n",
      "Epoch 20/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6678 - decoded_loss: 0.9777 - label_output_loss: 0.6901 - val_loss: 1.0606 - val_decoded_loss: 0.3718 - val_label_output_loss: 0.6888\n",
      "Epoch 21/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6613 - decoded_loss: 0.9712 - label_output_loss: 0.6901 - val_loss: 1.0400 - val_decoded_loss: 0.3513 - val_label_output_loss: 0.6887\n",
      "Epoch 22/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6567 - decoded_loss: 0.9667 - label_output_loss: 0.6901 - val_loss: 1.0417 - val_decoded_loss: 0.3531 - val_label_output_loss: 0.6886\n",
      "Epoch 23/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6575 - decoded_loss: 0.9675 - label_output_loss: 0.6901 - val_loss: 1.0518 - val_decoded_loss: 0.3632 - val_label_output_loss: 0.6886\n",
      "Epoch 24/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6660 - decoded_loss: 0.9759 - label_output_loss: 0.6901 - val_loss: 1.0504 - val_decoded_loss: 0.3617 - val_label_output_loss: 0.6887\n",
      "Epoch 25/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6597 - decoded_loss: 0.9696 - label_output_loss: 0.6901 - val_loss: 1.0453 - val_decoded_loss: 0.3565 - val_label_output_loss: 0.6887\n",
      "Epoch 26/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6625 - decoded_loss: 0.9725 - label_output_loss: 0.6900 - val_loss: 1.0410 - val_decoded_loss: 0.3523 - val_label_output_loss: 0.6888\n",
      "Epoch 27/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6527 - decoded_loss: 0.9627 - label_output_loss: 0.6900 - val_loss: 1.0479 - val_decoded_loss: 0.3592 - val_label_output_loss: 0.6887\n",
      "Epoch 28/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6655 - decoded_loss: 0.9755 - label_output_loss: 0.6900 - val_loss: 1.0379 - val_decoded_loss: 0.3492 - val_label_output_loss: 0.6886\n",
      "Epoch 29/1000\n",
      "691/691 [==============================] - 5s 7ms/step - loss: 1.6529 - decoded_loss: 0.9629 - label_output_loss: 0.6900 - val_loss: 1.0485 - val_decoded_loss: 0.3600 - val_label_output_loss: 0.6885\n",
      "Epoch 30/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6555 - decoded_loss: 0.9655 - label_output_loss: 0.6900 - val_loss: 1.0343 - val_decoded_loss: 0.3456 - val_label_output_loss: 0.6887\n",
      "Epoch 31/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6522 - decoded_loss: 0.9622 - label_output_loss: 0.6900 - val_loss: 1.0310 - val_decoded_loss: 0.3424 - val_label_output_loss: 0.6886\n",
      "Epoch 32/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6545 - decoded_loss: 0.9645 - label_output_loss: 0.6899 - val_loss: 1.0472 - val_decoded_loss: 0.3586 - val_label_output_loss: 0.6887\n",
      "Epoch 33/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6573 - decoded_loss: 0.9674 - label_output_loss: 0.6899 - val_loss: 1.0393 - val_decoded_loss: 0.3507 - val_label_output_loss: 0.6886\n",
      "Epoch 34/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6584 - decoded_loss: 0.9685 - label_output_loss: 0.6899 - val_loss: 1.0354 - val_decoded_loss: 0.3469 - val_label_output_loss: 0.6885\n",
      "Epoch 35/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6563 - decoded_loss: 0.9664 - label_output_loss: 0.6899 - val_loss: 1.0345 - val_decoded_loss: 0.3459 - val_label_output_loss: 0.6886\n",
      "Epoch 36/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6523 - decoded_loss: 0.9624 - label_output_loss: 0.6899 - val_loss: 1.0364 - val_decoded_loss: 0.3479 - val_label_output_loss: 0.6885\n",
      "Epoch 37/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6535 - decoded_loss: 0.9636 - label_output_loss: 0.6899 - val_loss: 1.0454 - val_decoded_loss: 0.3569 - val_label_output_loss: 0.6885\n",
      "Epoch 38/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6534 - decoded_loss: 0.9636 - label_output_loss: 0.6898 - val_loss: 1.0373 - val_decoded_loss: 0.3486 - val_label_output_loss: 0.6887\n",
      "Epoch 39/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6431 - decoded_loss: 0.9532 - label_output_loss: 0.6899 - val_loss: 1.0446 - val_decoded_loss: 0.3560 - val_label_output_loss: 0.6886\n",
      "Epoch 40/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6472 - decoded_loss: 0.9573 - label_output_loss: 0.6899 - val_loss: 1.0330 - val_decoded_loss: 0.3443 - val_label_output_loss: 0.6887\n",
      "Epoch 41/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6500 - decoded_loss: 0.9601 - label_output_loss: 0.6899 - val_loss: 1.0276 - val_decoded_loss: 0.3389 - val_label_output_loss: 0.6888\n",
      "Epoch 42/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6512 - decoded_loss: 0.9613 - label_output_loss: 0.6899 - val_loss: 1.0370 - val_decoded_loss: 0.3483 - val_label_output_loss: 0.6887\n",
      "Epoch 43/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6490 - decoded_loss: 0.9591 - label_output_loss: 0.6899 - val_loss: 1.0233 - val_decoded_loss: 0.3349 - val_label_output_loss: 0.6884\n",
      "Epoch 44/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6472 - decoded_loss: 0.9574 - label_output_loss: 0.6898 - val_loss: 1.0287 - val_decoded_loss: 0.3400 - val_label_output_loss: 0.6887\n",
      "Epoch 45/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6483 - decoded_loss: 0.9584 - label_output_loss: 0.6899 - val_loss: 1.0313 - val_decoded_loss: 0.3427 - val_label_output_loss: 0.6887\n",
      "Epoch 46/1000\n",
      "691/691 [==============================] - 5s 7ms/step - loss: 1.6424 - decoded_loss: 0.9526 - label_output_loss: 0.6898 - val_loss: 1.0285 - val_decoded_loss: 0.3399 - val_label_output_loss: 0.6886\n",
      "Epoch 47/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6454 - decoded_loss: 0.9555 - label_output_loss: 0.6899 - val_loss: 1.0293 - val_decoded_loss: 0.3408 - val_label_output_loss: 0.6885\n",
      "Epoch 48/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6463 - decoded_loss: 0.9565 - label_output_loss: 0.6898 - val_loss: 1.0324 - val_decoded_loss: 0.3439 - val_label_output_loss: 0.6886\n",
      "Epoch 49/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6482 - decoded_loss: 0.9583 - label_output_loss: 0.6899 - val_loss: 1.0388 - val_decoded_loss: 0.3501 - val_label_output_loss: 0.6887\n",
      "Epoch 50/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6413 - decoded_loss: 0.9515 - label_output_loss: 0.6898 - val_loss: 1.0451 - val_decoded_loss: 0.3565 - val_label_output_loss: 0.6886\n",
      "Epoch 51/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6418 - decoded_loss: 0.9519 - label_output_loss: 0.6898 - val_loss: 1.0442 - val_decoded_loss: 0.3557 - val_label_output_loss: 0.6885\n",
      "Epoch 52/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6434 - decoded_loss: 0.9535 - label_output_loss: 0.6899 - val_loss: 1.0360 - val_decoded_loss: 0.3474 - val_label_output_loss: 0.6886\n",
      "Epoch 53/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6393 - decoded_loss: 0.9495 - label_output_loss: 0.6898 - val_loss: 1.0361 - val_decoded_loss: 0.3474 - val_label_output_loss: 0.6887\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\n",
    "autoencoder.fit(X,(X,y),\n",
    "                epochs=1000,\n",
    "                batch_size=2048, \n",
    "                validation_split=0.1,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\n",
    "encoder.save_weights('./encoder.hdf5')\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:06:24.391397Z",
     "iopub.status.busy": "2021-02-18T16:06:24.389484Z",
     "iopub.status.idle": "2021-02-18T16:06:24.391997Z",
     "shell.execute_reply": "2021-02-18T16:06:24.392418Z"
    },
    "papermill": {
     "duration": 1.258419,
     "end_time": "2021-02-18T16:06:24.392541",
     "exception": false,
     "start_time": "2021-02-18T16:06:23.134122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train.loc[:, train.columns.str.contains('feature')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:06:27.095843Z",
     "iopub.status.busy": "2021-02-18T16:06:27.094958Z",
     "iopub.status.idle": "2021-02-18T16:16:21.193076Z",
     "shell.execute_reply": "2021-02-18T16:16:21.192559Z"
    },
    "papermill": {
     "duration": 595.666521,
     "end_time": "2021-02-18T16:16:21.193186",
     "exception": false,
     "start_time": "2021-02-18T16:06:25.526665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "384/384 - 3s - loss: 0.7171 - AUC: 0.5117\n",
      "Epoch 2/200\n",
      "384/384 - 5s - loss: 0.6945 - AUC: 0.5269\n",
      "Epoch 3/200\n",
      "384/384 - 3s - loss: 0.6913 - AUC: 0.5344\n",
      "Epoch 4/200\n",
      "384/384 - 3s - loss: 0.6904 - AUC: 0.5384\n",
      "Epoch 5/200\n",
      "384/384 - 3s - loss: 0.6899 - AUC: 0.5416\n",
      "Epoch 6/200\n",
      "384/384 - 3s - loss: 0.6896 - AUC: 0.5434\n",
      "Epoch 7/200\n",
      "384/384 - 3s - loss: 0.6894 - AUC: 0.5446\n",
      "Epoch 8/200\n",
      "384/384 - 4s - loss: 0.6891 - AUC: 0.5464\n",
      "Epoch 9/200\n",
      "384/384 - 3s - loss: 0.6889 - AUC: 0.5472\n",
      "Epoch 10/200\n",
      "384/384 - 3s - loss: 0.6888 - AUC: 0.5478\n",
      "Epoch 11/200\n",
      "384/384 - 3s - loss: 0.6885 - AUC: 0.5491\n",
      "Epoch 12/200\n",
      "384/384 - 3s - loss: 0.6884 - AUC: 0.5500\n",
      "Epoch 13/200\n",
      "384/384 - 3s - loss: 0.6883 - AUC: 0.5504\n",
      "Epoch 14/200\n",
      "384/384 - 3s - loss: 0.6882 - AUC: 0.5510\n",
      "Epoch 15/200\n",
      "384/384 - 3s - loss: 0.6880 - AUC: 0.5516\n",
      "Epoch 16/200\n",
      "384/384 - 3s - loss: 0.6879 - AUC: 0.5520\n",
      "Epoch 17/200\n",
      "384/384 - 3s - loss: 0.6878 - AUC: 0.5527\n",
      "Epoch 18/200\n",
      "384/384 - 3s - loss: 0.6876 - AUC: 0.5534\n",
      "Epoch 19/200\n",
      "384/384 - 3s - loss: 0.6875 - AUC: 0.5539\n",
      "Epoch 20/200\n",
      "384/384 - 3s - loss: 0.6873 - AUC: 0.5546\n",
      "Epoch 21/200\n",
      "384/384 - 3s - loss: 0.6872 - AUC: 0.5549\n",
      "Epoch 22/200\n",
      "384/384 - 3s - loss: 0.6872 - AUC: 0.5550\n",
      "Epoch 23/200\n",
      "384/384 - 3s - loss: 0.6870 - AUC: 0.5559\n",
      "Epoch 24/200\n",
      "384/384 - 3s - loss: 0.6869 - AUC: 0.5559\n",
      "Epoch 25/200\n",
      "384/384 - 3s - loss: 0.6869 - AUC: 0.5564\n",
      "Epoch 26/200\n",
      "384/384 - 3s - loss: 0.6868 - AUC: 0.5565\n",
      "Epoch 27/200\n",
      "384/384 - 3s - loss: 0.6866 - AUC: 0.5571\n",
      "Epoch 28/200\n",
      "384/384 - 3s - loss: 0.6866 - AUC: 0.5572\n",
      "Epoch 29/200\n",
      "384/384 - 3s - loss: 0.6864 - AUC: 0.5577\n",
      "Epoch 30/200\n",
      "384/384 - 3s - loss: 0.6864 - AUC: 0.5579\n",
      "Epoch 31/200\n",
      "384/384 - 3s - loss: 0.6863 - AUC: 0.5580\n",
      "Epoch 32/200\n",
      "384/384 - 3s - loss: 0.6862 - AUC: 0.5584\n",
      "Epoch 33/200\n",
      "384/384 - 3s - loss: 0.6861 - AUC: 0.5584\n",
      "Epoch 34/200\n",
      "384/384 - 3s - loss: 0.6861 - AUC: 0.5592\n",
      "Epoch 35/200\n",
      "384/384 - 3s - loss: 0.6859 - AUC: 0.5595\n",
      "Epoch 36/200\n",
      "384/384 - 3s - loss: 0.6858 - AUC: 0.5597\n",
      "Epoch 37/200\n",
      "384/384 - 3s - loss: 0.6859 - AUC: 0.5597\n",
      "Epoch 38/200\n",
      "384/384 - 3s - loss: 0.6858 - AUC: 0.5598\n",
      "Epoch 39/200\n",
      "384/384 - 3s - loss: 0.6858 - AUC: 0.5597\n",
      "Epoch 40/200\n",
      "384/384 - 3s - loss: 0.6856 - AUC: 0.5603\n",
      "Epoch 41/200\n",
      "384/384 - 3s - loss: 0.6856 - AUC: 0.5602\n",
      "Epoch 42/200\n",
      "384/384 - 3s - loss: 0.6854 - AUC: 0.5609\n",
      "Epoch 43/200\n",
      "384/384 - 3s - loss: 0.6854 - AUC: 0.5609\n",
      "Epoch 44/200\n",
      "384/384 - 3s - loss: 0.6853 - AUC: 0.5610\n",
      "Epoch 45/200\n",
      "384/384 - 3s - loss: 0.6853 - AUC: 0.5611\n",
      "Epoch 46/200\n",
      "384/384 - 3s - loss: 0.6853 - AUC: 0.5614\n",
      "Epoch 47/200\n",
      "384/384 - 3s - loss: 0.6852 - AUC: 0.5616\n",
      "Epoch 48/200\n",
      "384/384 - 3s - loss: 0.6852 - AUC: 0.5617\n",
      "Epoch 49/200\n",
      "384/384 - 3s - loss: 0.6851 - AUC: 0.5619\n",
      "Epoch 50/200\n",
      "384/384 - 3s - loss: 0.6850 - AUC: 0.5622\n",
      "Epoch 51/200\n",
      "384/384 - 3s - loss: 0.6851 - AUC: 0.5618\n",
      "Epoch 52/200\n",
      "384/384 - 3s - loss: 0.6850 - AUC: 0.5621\n",
      "Epoch 53/200\n",
      "384/384 - 3s - loss: 0.6849 - AUC: 0.5622\n",
      "Epoch 54/200\n",
      "384/384 - 3s - loss: 0.6849 - AUC: 0.5623\n",
      "Epoch 55/200\n",
      "384/384 - 3s - loss: 0.6848 - AUC: 0.5625\n",
      "Epoch 56/200\n",
      "384/384 - 3s - loss: 0.6847 - AUC: 0.5630\n",
      "Epoch 57/200\n",
      "384/384 - 3s - loss: 0.6848 - AUC: 0.5626\n",
      "Epoch 58/200\n",
      "384/384 - 3s - loss: 0.6847 - AUC: 0.5630\n",
      "Epoch 59/200\n",
      "384/384 - 3s - loss: 0.6846 - AUC: 0.5636\n",
      "Epoch 60/200\n",
      "384/384 - 3s - loss: 0.6846 - AUC: 0.5631\n",
      "Epoch 61/200\n",
      "384/384 - 3s - loss: 0.6846 - AUC: 0.5630\n",
      "Epoch 62/200\n",
      "384/384 - 3s - loss: 0.6845 - AUC: 0.5636\n",
      "Epoch 63/200\n",
      "384/384 - 3s - loss: 0.6845 - AUC: 0.5638\n",
      "Epoch 64/200\n",
      "384/384 - 3s - loss: 0.6844 - AUC: 0.5638\n",
      "Epoch 65/200\n",
      "384/384 - 3s - loss: 0.6844 - AUC: 0.5636\n",
      "Epoch 66/200\n",
      "384/384 - 3s - loss: 0.6844 - AUC: 0.5640\n",
      "Epoch 67/200\n",
      "384/384 - 3s - loss: 0.6843 - AUC: 0.5641\n",
      "Epoch 68/200\n",
      "384/384 - 3s - loss: 0.6844 - AUC: 0.5635\n",
      "Epoch 69/200\n",
      "384/384 - 3s - loss: 0.6843 - AUC: 0.5641\n",
      "Epoch 70/200\n",
      "384/384 - 3s - loss: 0.6843 - AUC: 0.5640\n",
      "Epoch 71/200\n",
      "384/384 - 3s - loss: 0.6842 - AUC: 0.5645\n",
      "Epoch 72/200\n",
      "384/384 - 3s - loss: 0.6841 - AUC: 0.5646\n",
      "Epoch 73/200\n",
      "384/384 - 3s - loss: 0.6842 - AUC: 0.5644\n",
      "Epoch 74/200\n",
      "384/384 - 3s - loss: 0.6842 - AUC: 0.5646\n",
      "Epoch 75/200\n",
      "384/384 - 3s - loss: 0.6840 - AUC: 0.5647\n",
      "Epoch 76/200\n",
      "384/384 - 3s - loss: 0.6840 - AUC: 0.5648\n",
      "Epoch 77/200\n",
      "384/384 - 3s - loss: 0.6840 - AUC: 0.5647\n",
      "Epoch 78/200\n",
      "384/384 - 3s - loss: 0.6841 - AUC: 0.5647\n",
      "Epoch 79/200\n",
      "384/384 - 3s - loss: 0.6839 - AUC: 0.5651\n",
      "Epoch 80/200\n",
      "384/384 - 3s - loss: 0.6840 - AUC: 0.5650\n",
      "Epoch 81/200\n",
      "384/384 - 3s - loss: 0.6839 - AUC: 0.5654\n",
      "Epoch 82/200\n",
      "384/384 - 3s - loss: 0.6838 - AUC: 0.5655\n",
      "Epoch 83/200\n",
      "384/384 - 3s - loss: 0.6838 - AUC: 0.5656\n",
      "Epoch 84/200\n",
      "384/384 - 3s - loss: 0.6838 - AUC: 0.5655\n",
      "Epoch 85/200\n",
      "384/384 - 3s - loss: 0.6838 - AUC: 0.5655\n",
      "Epoch 86/200\n",
      "384/384 - 3s - loss: 0.6838 - AUC: 0.5658\n",
      "Epoch 87/200\n",
      "384/384 - 3s - loss: 0.6838 - AUC: 0.5658\n",
      "Epoch 88/200\n",
      "384/384 - 3s - loss: 0.6838 - AUC: 0.5657\n",
      "Epoch 89/200\n",
      "384/384 - 3s - loss: 0.6837 - AUC: 0.5661\n",
      "Epoch 90/200\n",
      "384/384 - 3s - loss: 0.6836 - AUC: 0.5663\n",
      "Epoch 91/200\n",
      "384/384 - 3s - loss: 0.6837 - AUC: 0.5660\n",
      "Epoch 92/200\n",
      "384/384 - 3s - loss: 0.6836 - AUC: 0.5663\n",
      "Epoch 93/200\n",
      "384/384 - 3s - loss: 0.6836 - AUC: 0.5664\n",
      "Epoch 94/200\n",
      "384/384 - 3s - loss: 0.6835 - AUC: 0.5662\n",
      "Epoch 95/200\n",
      "384/384 - 3s - loss: 0.6835 - AUC: 0.5663\n",
      "Epoch 96/200\n",
      "384/384 - 3s - loss: 0.6836 - AUC: 0.5661\n",
      "Epoch 97/200\n",
      "384/384 - 3s - loss: 0.6835 - AUC: 0.5666\n",
      "Epoch 98/200\n",
      "384/384 - 3s - loss: 0.6834 - AUC: 0.5669\n",
      "Epoch 99/200\n",
      "384/384 - 3s - loss: 0.6834 - AUC: 0.5669\n",
      "Epoch 100/200\n",
      "384/384 - 3s - loss: 0.6834 - AUC: 0.5665\n",
      "Epoch 101/200\n",
      "384/384 - 3s - loss: 0.6834 - AUC: 0.5668\n",
      "Epoch 102/200\n",
      "384/384 - 3s - loss: 0.6833 - AUC: 0.5668\n",
      "Epoch 103/200\n",
      "384/384 - 3s - loss: 0.6834 - AUC: 0.5666\n",
      "Epoch 104/200\n",
      "384/384 - 3s - loss: 0.6833 - AUC: 0.5671\n",
      "Epoch 105/200\n",
      "384/384 - 3s - loss: 0.6833 - AUC: 0.5666\n",
      "Epoch 106/200\n",
      "384/384 - 3s - loss: 0.6833 - AUC: 0.5673\n",
      "Epoch 107/200\n",
      "384/384 - 3s - loss: 0.6833 - AUC: 0.5670\n",
      "Epoch 108/200\n",
      "384/384 - 3s - loss: 0.6832 - AUC: 0.5672\n",
      "Epoch 109/200\n",
      "384/384 - 3s - loss: 0.6832 - AUC: 0.5672\n",
      "Epoch 110/200\n",
      "384/384 - 3s - loss: 0.6832 - AUC: 0.5673\n",
      "Epoch 111/200\n",
      "384/384 - 3s - loss: 0.6833 - AUC: 0.5670\n",
      "Epoch 112/200\n",
      "384/384 - 3s - loss: 0.6832 - AUC: 0.5670\n",
      "Epoch 113/200\n",
      "384/384 - 3s - loss: 0.6831 - AUC: 0.5675\n",
      "Epoch 114/200\n",
      "384/384 - 3s - loss: 0.6832 - AUC: 0.5674\n",
      "Epoch 115/200\n",
      "384/384 - 3s - loss: 0.6831 - AUC: 0.5674\n",
      "Epoch 116/200\n",
      "384/384 - 3s - loss: 0.6830 - AUC: 0.5677\n",
      "Epoch 117/200\n",
      "384/384 - 3s - loss: 0.6831 - AUC: 0.5678\n",
      "Epoch 118/200\n",
      "384/384 - 3s - loss: 0.6831 - AUC: 0.5677\n",
      "Epoch 119/200\n",
      "384/384 - 3s - loss: 0.6830 - AUC: 0.5677\n",
      "Epoch 120/200\n",
      "384/384 - 3s - loss: 0.6831 - AUC: 0.5674\n",
      "Epoch 121/200\n",
      "384/384 - 3s - loss: 0.6831 - AUC: 0.5676\n",
      "Epoch 122/200\n",
      "384/384 - 3s - loss: 0.6829 - AUC: 0.5683\n",
      "Epoch 123/200\n",
      "384/384 - 3s - loss: 0.6830 - AUC: 0.5677\n",
      "Epoch 124/200\n",
      "384/384 - 3s - loss: 0.6829 - AUC: 0.5680\n",
      "Epoch 125/200\n",
      "384/384 - 3s - loss: 0.6829 - AUC: 0.5679\n",
      "Epoch 126/200\n",
      "384/384 - 3s - loss: 0.6830 - AUC: 0.5677\n",
      "Epoch 127/200\n",
      "384/384 - 3s - loss: 0.6829 - AUC: 0.5680\n",
      "Epoch 128/200\n",
      "384/384 - 3s - loss: 0.6829 - AUC: 0.5680\n",
      "Epoch 129/200\n",
      "384/384 - 3s - loss: 0.6829 - AUC: 0.5681\n",
      "Epoch 130/200\n",
      "384/384 - 3s - loss: 0.6829 - AUC: 0.5682\n",
      "Epoch 131/200\n",
      "384/384 - 3s - loss: 0.6829 - AUC: 0.5681\n",
      "Epoch 132/200\n",
      "384/384 - 3s - loss: 0.6829 - AUC: 0.5683\n",
      "Epoch 133/200\n",
      "384/384 - 3s - loss: 0.6827 - AUC: 0.5684\n",
      "Epoch 134/200\n",
      "384/384 - 3s - loss: 0.6830 - AUC: 0.5678\n",
      "Epoch 135/200\n",
      "384/384 - 3s - loss: 0.6827 - AUC: 0.5687\n",
      "Epoch 136/200\n",
      "384/384 - 3s - loss: 0.6827 - AUC: 0.5686\n",
      "Epoch 137/200\n",
      "384/384 - 3s - loss: 0.6827 - AUC: 0.5685\n",
      "Epoch 138/200\n",
      "384/384 - 3s - loss: 0.6828 - AUC: 0.5683\n",
      "Epoch 139/200\n",
      "384/384 - 3s - loss: 0.6828 - AUC: 0.5684\n",
      "Epoch 140/200\n",
      "384/384 - 3s - loss: 0.6827 - AUC: 0.5686\n",
      "Epoch 141/200\n",
      "384/384 - 3s - loss: 0.6827 - AUC: 0.5685\n",
      "Epoch 142/200\n",
      "384/384 - 3s - loss: 0.6827 - AUC: 0.5686\n",
      "Epoch 143/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5693\n",
      "Epoch 144/200\n",
      "384/384 - 3s - loss: 0.6827 - AUC: 0.5685\n",
      "Epoch 145/200\n",
      "384/384 - 3s - loss: 0.6826 - AUC: 0.5691\n",
      "Epoch 146/200\n",
      "384/384 - 3s - loss: 0.6826 - AUC: 0.5690\n",
      "Epoch 147/200\n",
      "384/384 - 3s - loss: 0.6827 - AUC: 0.5685\n",
      "Epoch 148/200\n",
      "384/384 - 3s - loss: 0.6826 - AUC: 0.5687\n",
      "Epoch 149/200\n",
      "384/384 - 3s - loss: 0.6826 - AUC: 0.5686\n",
      "Epoch 150/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5688\n",
      "Epoch 151/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5692\n",
      "Epoch 152/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5692\n",
      "Epoch 153/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5693\n",
      "Epoch 154/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5691\n",
      "Epoch 155/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5692\n",
      "Epoch 156/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5690\n",
      "Epoch 157/200\n",
      "384/384 - 3s - loss: 0.6824 - AUC: 0.5693\n",
      "Epoch 158/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5694\n",
      "Epoch 159/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5694\n",
      "Epoch 160/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5692\n",
      "Epoch 161/200\n",
      "384/384 - 3s - loss: 0.6824 - AUC: 0.5695\n",
      "Epoch 162/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5691\n",
      "Epoch 163/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5692\n",
      "Epoch 164/200\n",
      "384/384 - 3s - loss: 0.6825 - AUC: 0.5695\n",
      "Epoch 165/200\n",
      "384/384 - 3s - loss: 0.6824 - AUC: 0.5699\n",
      "Epoch 166/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5698\n",
      "Epoch 167/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5699\n",
      "Epoch 168/200\n",
      "384/384 - 3s - loss: 0.6824 - AUC: 0.5695\n",
      "Epoch 169/200\n",
      "384/384 - 3s - loss: 0.6824 - AUC: 0.5692\n",
      "Epoch 170/200\n",
      "384/384 - 3s - loss: 0.6824 - AUC: 0.5692\n",
      "Epoch 171/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5700\n",
      "Epoch 172/200\n",
      "384/384 - 3s - loss: 0.6824 - AUC: 0.5693\n",
      "Epoch 173/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5695\n",
      "Epoch 174/200\n",
      "384/384 - 3s - loss: 0.6824 - AUC: 0.5694\n",
      "Epoch 175/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5697\n",
      "Epoch 176/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5697\n",
      "Epoch 177/200\n",
      "384/384 - 3s - loss: 0.6822 - AUC: 0.5703\n",
      "Epoch 178/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5697\n",
      "Epoch 179/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5697\n",
      "Epoch 180/200\n",
      "384/384 - 3s - loss: 0.6822 - AUC: 0.5697\n",
      "Epoch 181/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5698\n",
      "Epoch 182/200\n",
      "384/384 - 3s - loss: 0.6822 - AUC: 0.5698\n",
      "Epoch 183/200\n",
      "384/384 - 3s - loss: 0.6821 - AUC: 0.5702\n",
      "Epoch 184/200\n",
      "384/384 - 3s - loss: 0.6823 - AUC: 0.5697\n",
      "Epoch 185/200\n",
      "384/384 - 3s - loss: 0.6822 - AUC: 0.5702\n",
      "Epoch 186/200\n",
      "384/384 - 3s - loss: 0.6822 - AUC: 0.5703\n",
      "Epoch 187/200\n",
      "384/384 - 3s - loss: 0.6822 - AUC: 0.5697\n",
      "Epoch 188/200\n",
      "384/384 - 3s - loss: 0.6821 - AUC: 0.5703\n",
      "Epoch 189/200\n",
      "384/384 - 3s - loss: 0.6822 - AUC: 0.5703\n",
      "Epoch 190/200\n",
      "384/384 - 3s - loss: 0.6822 - AUC: 0.5702\n",
      "Epoch 191/200\n",
      "384/384 - 3s - loss: 0.6820 - AUC: 0.5702\n",
      "Epoch 192/200\n",
      "384/384 - 4s - loss: 0.6821 - AUC: 0.5701\n",
      "Epoch 193/200\n",
      "384/384 - 3s - loss: 0.6821 - AUC: 0.5704\n",
      "Epoch 194/200\n",
      "384/384 - 3s - loss: 0.6820 - AUC: 0.5706\n",
      "Epoch 195/200\n",
      "384/384 - 3s - loss: 0.6820 - AUC: 0.5706\n",
      "Epoch 196/200\n",
      "384/384 - 3s - loss: 0.6821 - AUC: 0.5703\n",
      "Epoch 197/200\n",
      "384/384 - 3s - loss: 0.6821 - AUC: 0.5700\n",
      "Epoch 198/200\n",
      "384/384 - 3s - loss: 0.6820 - AUC: 0.5706\n",
      "Epoch 199/200\n",
      "384/384 - 3s - loss: 0.6820 - AUC: 0.5705\n",
      "Epoch 200/200\n",
      "384/384 - 3s - loss: 0.6820 - AUC: 0.5705\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 4096\n",
    "hidden_units = [160, 160, 160]\n",
    "dropout_rates = [0.2, 0.2, 0.2, 0.2]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(SEED)\n",
    "clf = create_mlp(\n",
    "    len(features), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "\n",
    "clf.fit(X_train, y, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "# save model\n",
    "clf.save(f'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:16:23.457419Z",
     "iopub.status.busy": "2021-02-18T16:16:23.455610Z",
     "iopub.status.idle": "2021-02-18T16:16:23.457999Z",
     "shell.execute_reply": "2021-02-18T16:16:23.458406Z"
    },
    "papermill": {
     "duration": 1.130595,
     "end_time": "2021-02-18T16:16:23.458517",
     "exception": false,
     "start_time": "2021-02-18T16:16:22.327922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:16:25.702019Z",
     "iopub.status.busy": "2021-02-18T16:16:25.700998Z",
     "iopub.status.idle": "2021-02-18T16:21:44.361771Z",
     "shell.execute_reply": "2021-02-18T16:21:44.361264Z"
    },
    "papermill": {
     "duration": 319.785659,
     "end_time": "2021-02-18T16:21:44.361886",
     "exception": false,
     "start_time": "2021-02-18T16:16:24.576227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "615/615 [==============================] - 8s 13ms/step - loss: 0.6824 - AUC: 0.5694 - val_loss: 0.6778 - val_AUC: 0.5885\n",
      "Epoch 2/192\n",
      "615/615 [==============================] - 7s 12ms/step - loss: 0.6824 - AUC: 0.5695 - val_loss: 0.6783 - val_AUC: 0.5850\n",
      "Epoch 3/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6821 - AUC: 0.5700 - val_loss: 0.6791 - val_AUC: 0.5838\n",
      "Epoch 4/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6822 - AUC: 0.5700 - val_loss: 0.6793 - val_AUC: 0.5826\n",
      "Epoch 5/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6820 - AUC: 0.5707 - val_loss: 0.6795 - val_AUC: 0.5811\n",
      "Epoch 6/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6820 - AUC: 0.5702 - val_loss: 0.6796 - val_AUC: 0.5800\n",
      "Epoch 7/192\n",
      "615/615 [==============================] - 5s 7ms/step - loss: 0.6820 - AUC: 0.5707 - val_loss: 0.6801 - val_AUC: 0.5795\n",
      "Epoch 8/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6820 - AUC: 0.5705 - val_loss: 0.6803 - val_AUC: 0.5784\n",
      "Epoch 9/192\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.6820 - AUC: 0.5705\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "615/615 [==============================] - 5s 7ms/step - loss: 0.6820 - AUC: 0.5705 - val_loss: 0.6806 - val_AUC: 0.5777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:55, 55.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6824 - AUC: 0.5698 - val_loss: 0.6757 - val_AUC: 0.5937\n",
      "Epoch 2/192\n",
      "614/614 [==============================] - 8s 13ms/step - loss: 0.6822 - AUC: 0.5705 - val_loss: 0.6756 - val_AUC: 0.5934\n",
      "Epoch 3/192\n",
      "614/614 [==============================] - 5s 7ms/step - loss: 0.6821 - AUC: 0.5706 - val_loss: 0.6758 - val_AUC: 0.5930\n",
      "Epoch 4/192\n",
      "614/614 [==============================] - 4s 7ms/step - loss: 0.6820 - AUC: 0.5709 - val_loss: 0.6759 - val_AUC: 0.5926\n",
      "Epoch 5/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5709 - val_loss: 0.6761 - val_AUC: 0.5923\n",
      "Epoch 6/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5713 - val_loss: 0.6760 - val_AUC: 0.5919\n",
      "Epoch 7/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6818 - AUC: 0.5719 - val_loss: 0.6761 - val_AUC: 0.5915\n",
      "Epoch 8/192\n",
      "614/614 [==============================] - 6s 9ms/step - loss: 0.6818 - AUC: 0.5713 - val_loss: 0.6763 - val_AUC: 0.5915\n",
      "Epoch 9/192\n",
      "609/614 [============================>.] - ETA: 0s - loss: 0.6818 - AUC: 0.5712\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6818 - AUC: 0.5712 - val_loss: 0.6762 - val_AUC: 0.5912\n",
      "Epoch 10/192\n",
      "614/614 [==============================] - 5s 7ms/step - loss: 0.6816 - AUC: 0.5719 - val_loss: 0.6763 - val_AUC: 0.5910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:50, 55.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6821 - AUC: 0.5701 - val_loss: 0.6754 - val_AUC: 0.5970\n",
      "Epoch 2/192\n",
      "614/614 [==============================] - 7s 12ms/step - loss: 0.6820 - AUC: 0.5700 - val_loss: 0.6755 - val_AUC: 0.5968\n",
      "Epoch 3/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6820 - AUC: 0.5706 - val_loss: 0.6756 - val_AUC: 0.5965\n",
      "Epoch 4/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5709 - val_loss: 0.6755 - val_AUC: 0.5969\n",
      "Epoch 5/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6820 - AUC: 0.5703 - val_loss: 0.6757 - val_AUC: 0.5964\n",
      "Epoch 6/192\n",
      "614/614 [==============================] - 5s 7ms/step - loss: 0.6820 - AUC: 0.5703 - val_loss: 0.6757 - val_AUC: 0.5965\n",
      "Epoch 7/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6818 - AUC: 0.5709 - val_loss: 0.6757 - val_AUC: 0.5965\n",
      "Epoch 8/192\n",
      "614/614 [==============================] - 5s 7ms/step - loss: 0.6819 - AUC: 0.5704 - val_loss: 0.6756 - val_AUC: 0.5965\n",
      "Epoch 9/192\n",
      "613/614 [============================>.] - ETA: 0s - loss: 0.6819 - AUC: 0.5708\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "614/614 [==============================] - 5s 9ms/step - loss: 0.6819 - AUC: 0.5708 - val_loss: 0.6757 - val_AUC: 0.5963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:39, 53.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "615/615 [==============================] - 6s 10ms/step - loss: 0.6819 - AUC: 0.5710 - val_loss: 0.6763 - val_AUC: 0.5932\n",
      "Epoch 2/192\n",
      "615/615 [==============================] - 8s 12ms/step - loss: 0.6819 - AUC: 0.5707 - val_loss: 0.6762 - val_AUC: 0.5933\n",
      "Epoch 3/192\n",
      "615/615 [==============================] - 5s 7ms/step - loss: 0.6819 - AUC: 0.5709 - val_loss: 0.6762 - val_AUC: 0.5934\n",
      "Epoch 4/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5708 - val_loss: 0.6762 - val_AUC: 0.5934\n",
      "Epoch 5/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5711 - val_loss: 0.6760 - val_AUC: 0.5935\n",
      "Epoch 6/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5707 - val_loss: 0.6762 - val_AUC: 0.5933\n",
      "Epoch 7/192\n",
      "615/615 [==============================] - 6s 9ms/step - loss: 0.6819 - AUC: 0.5712 - val_loss: 0.6762 - val_AUC: 0.5934\n",
      "Epoch 8/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6820 - AUC: 0.5708 - val_loss: 0.6762 - val_AUC: 0.5934\n",
      "Epoch 9/192\n",
      "615/615 [==============================] - 5s 7ms/step - loss: 0.6819 - AUC: 0.5711 - val_loss: 0.6762 - val_AUC: 0.5933\n",
      "Epoch 10/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5710 - val_loss: 0.6762 - val_AUC: 0.5933\n",
      "Epoch 11/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5711 - val_loss: 0.6762 - val_AUC: 0.5932\n",
      "Epoch 12/192\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5709 - val_loss: 0.6762 - val_AUC: 0.5933\n",
      "Epoch 13/192\n",
      "613/615 [============================>.] - ETA: 0s - loss: 0.6819 - AUC: 0.5707\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "615/615 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5707 - val_loss: 0.6762 - val_AUC: 0.5933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [03:49, 58.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "614/614 [==============================] - 6s 10ms/step - loss: 0.6820 - AUC: 0.5712 - val_loss: 0.6760 - val_AUC: 0.5910\n",
      "Epoch 2/192\n",
      "614/614 [==============================] - 7s 12ms/step - loss: 0.6820 - AUC: 0.5710 - val_loss: 0.6760 - val_AUC: 0.5912\n",
      "Epoch 3/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5710 - val_loss: 0.6760 - val_AUC: 0.5911\n",
      "Epoch 4/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5713 - val_loss: 0.6760 - val_AUC: 0.5909\n",
      "Epoch 5/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6820 - AUC: 0.5711 - val_loss: 0.6761 - val_AUC: 0.5910\n",
      "Epoch 6/192\n",
      "614/614 [==============================] - 4s 7ms/step - loss: 0.6819 - AUC: 0.5717 - val_loss: 0.6760 - val_AUC: 0.5910\n",
      "Epoch 7/192\n",
      "614/614 [==============================] - 6s 9ms/step - loss: 0.6819 - AUC: 0.5713 - val_loss: 0.6761 - val_AUC: 0.5910\n",
      "Epoch 8/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6820 - AUC: 0.5710 - val_loss: 0.6761 - val_AUC: 0.5908\n",
      "Epoch 9/192\n",
      "612/614 [============================>.] - ETA: 0s - loss: 0.6817 - AUC: 0.5716\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6817 - AUC: 0.5716 - val_loss: 0.6759 - val_AUC: 0.5910\n",
      "Epoch 10/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5714 - val_loss: 0.6760 - val_AUC: 0.5911\n",
      "Epoch 11/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6818 - AUC: 0.5718 - val_loss: 0.6760 - val_AUC: 0.5911\n",
      "Epoch 12/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5712 - val_loss: 0.6761 - val_AUC: 0.5910\n",
      "Epoch 13/192\n",
      "614/614 [==============================] - 5s 7ms/step - loss: 0.6818 - AUC: 0.5717 - val_loss: 0.6761 - val_AUC: 0.5909\n",
      "Epoch 14/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6818 - AUC: 0.5716 - val_loss: 0.6760 - val_AUC: 0.5911\n",
      "Epoch 15/192\n",
      "614/614 [==============================] - 5s 9ms/step - loss: 0.6819 - AUC: 0.5713 - val_loss: 0.6761 - val_AUC: 0.5910\n",
      "Epoch 16/192\n",
      "614/614 [==============================] - 5s 8ms/step - loss: 0.6819 - AUC: 0.5709 - val_loss: 0.6761 - val_AUC: 0.5910\n",
      "Epoch 17/192\n",
      "609/614 [============================>.] - ETA: 0s - loss: 0.6819 - AUC: 0.5714\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "614/614 [==============================] - 5s 7ms/step - loss: 0.6819 - AUC: 0.5713 - val_loss: 0.6760 - val_AUC: 0.5911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [05:18, 63.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 17s, sys: 35.5 s, total: 6min 52s\n",
      "Wall time: 5min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "if CV_STRATEGY == 'PurgedGroupTimeSeriesSplit':\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits=FOLDS, group_gap=20)\n",
    "    splits = list(gkf.split(y, groups=train['date'].values))    \n",
    "    \n",
    "elif CV_STRATEGY == \"GroupKFold\":\n",
    "    cv = GroupKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    splits = cv.split(train, train['resp'].values.astype(int), 'date')\n",
    "\n",
    "elif CV_STRATEGY ==  \"StratifiedGroupKFold\":\n",
    "    cv = StratifiedGroupKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    splits = cv.split(train, train['resp'].values.astype(int), 'date')\n",
    "\n",
    "models = []\n",
    "for fold, (train_indices, test_indices) in tqdm(enumerate(splits)):\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    # model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = clf\n",
    "    \n",
    "    # callbacks\n",
    "    er = tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss')\n",
    "    ReduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, mode='min')\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'./model_{SEED}_{fold}.hdf5', save_weights_only=True, verbose=0, monitor='val_loss', save_best_only=True)\n",
    "    nn_callbacks = [er, ReduceLR, model_checkpoint_callback]\n",
    "    \n",
    "    # fit\n",
    "    model.fit(X_train, y_train, validation_data=(X_test,y_test), \n",
    "              epochs=192, batch_size=2048, callbacks=nn_callbacks)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T16:21:50.048589Z",
     "iopub.status.busy": "2021-02-18T16:21:50.047829Z",
     "iopub.status.idle": "2021-02-18T16:21:50.051869Z",
     "shell.execute_reply": "2021-02-18T16:21:50.052282Z"
    },
    "papermill": {
     "duration": 2.781749,
     "end_time": "2021-02-18T16:21:50.052390",
     "exception": false,
     "start_time": "2021-02-18T16:21:47.270641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"f = np.median\\nth = 0.500\\n\\nimport janestreet\\nenv = janestreet.make_env()\\nfor (test_df, pred_df) in tqdm(env.iter_test()):\\n    if test_df['weight'].item() > 0:\\n        x_tt = test_df.loc[:, features].values\\n        \\n        # GBDT inference with treelite\\n        batch = treelite_runtime.Batch.from_npy2d(x_tt)\\n        xgb_pred = predictor.predict(batch)\\n    \\n        # NN inference\\n        if np.isnan(x_tt[:, 1:].sum()):\\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\\n        \\n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\\n        pred = f(pred)\\n        \\n        # ensemble\\n        pred_df.action = np.where(0.9*pred + 0.1*xgb_pred >= th, 1, 0).astype(int)\\n    else:\\n        pred_df.action = 0\\n    env.predict(pred_df)\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"f = np.median\n",
    "th = 0.500\n",
    "\n",
    "import janestreet\n",
    "env = janestreet.make_env()\n",
    "for (test_df, pred_df) in tqdm(env.iter_test()):\n",
    "    if test_df['weight'].item() > 0:\n",
    "        x_tt = test_df.loc[:, features].values\n",
    "        \n",
    "        # GBDT inference with treelite\n",
    "        batch = treelite_runtime.Batch.from_npy2d(x_tt)\n",
    "        xgb_pred = predictor.predict(batch)\n",
    "    \n",
    "        # NN inference\n",
    "        if np.isnan(x_tt[:, 1:].sum()):\n",
    "            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "        \n",
    "        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n",
    "        pred = f(pred)\n",
    "        \n",
    "        # ensemble\n",
    "        pred_df.action = np.where(0.9*pred + 0.1*xgb_pred >= th, 1, 0).astype(int)\n",
    "    else:\n",
    "        pred_df.action = 0\n",
    "    env.predict(pred_df)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1170.797026,
   "end_time": "2021-02-18T16:21:54.470385",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-18T16:02:23.673359",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
