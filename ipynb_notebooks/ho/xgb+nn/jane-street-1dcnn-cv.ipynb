{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:14.351458Z",
     "iopub.status.busy": "2021-02-18T15:39:14.350794Z",
     "iopub.status.idle": "2021-02-18T15:39:19.982523Z",
     "shell.execute_reply": "2021-02-18T15:39:19.981423Z"
    },
    "papermill": {
     "duration": 5.650654,
     "end_time": "2021-02-18T15:39:19.982682",
     "exception": false,
     "start_time": "2021-02-18T15:39:14.332028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from random import choices\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:20.013235Z",
     "iopub.status.busy": "2021-02-18T15:39:20.011502Z",
     "iopub.status.idle": "2021-02-18T15:39:20.015334Z",
     "shell.execute_reply": "2021-02-18T15:39:20.014894Z"
    },
    "papermill": {
     "duration": 0.021959,
     "end_time": "2021-02-18T15:39:20.015416",
     "exception": false,
     "start_time": "2021-02-18T15:39:19.993457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.3.1\n",
      "Accelerated Linear Algebra enabled\n"
     ]
    }
   ],
   "source": [
    "# tf setup\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "MIXED_PRECISION = False\n",
    "XLA_ACCELERATE = True\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)\n",
    "    print('Mixed precision enabled')\n",
    "\n",
    "if XLA_ACCELERATE:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print('Accelerated Linear Algebra enabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009748,
     "end_time": "2021-02-18T15:39:20.035060",
     "exception": false,
     "start_time": "2021-02-18T15:39:20.025312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:20.058764Z",
     "iopub.status.busy": "2021-02-18T15:39:20.058123Z",
     "iopub.status.idle": "2021-02-18T15:39:20.061168Z",
     "shell.execute_reply": "2021-02-18T15:39:20.060676Z"
    },
    "papermill": {
     "duration": 0.016474,
     "end_time": "2021-02-18T15:39:20.061254",
     "exception": false,
     "start_time": "2021-02-18T15:39:20.044780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CV_STRATEGY = 'PurgedGroupTimeSeriesSplit' # 'StratifiedGroupKFold' # GroupKFold, PurgedGroupTimeSeriesSplit\n",
    "SEED = 2021\n",
    "START_DATE = 86\n",
    "FOLDS = 5\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009674,
     "end_time": "2021-02-18T15:39:20.080995",
     "exception": false,
     "start_time": "2021-02-18T15:39:20.071321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV Strategy\n",
    "\n",
    "## PurgedGroupTimeSeriesSplit\n",
    "Click the code button to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:20.104319Z",
     "iopub.status.busy": "2021-02-18T15:39:20.103469Z",
     "iopub.status.idle": "2021-02-18T15:39:20.123906Z",
     "shell.execute_reply": "2021-02-18T15:39:20.124405Z"
    },
    "papermill": {
     "duration": 0.033363,
     "end_time": "2021-02-18T15:39:20.124498",
     "exception": false,
     "start_time": "2021-02-18T15:39:20.091135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009727,
     "end_time": "2021-02-18T15:39:20.144519",
     "exception": false,
     "start_time": "2021-02-18T15:39:20.134792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GroupKFold, StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:20.167970Z",
     "iopub.status.busy": "2021-02-18T15:39:20.167082Z",
     "iopub.status.idle": "2021-02-18T15:39:20.190715Z",
     "shell.execute_reply": "2021-02-18T15:39:20.190211Z"
    },
    "papermill": {
     "duration": 0.036091,
     "end_time": "2021-02-18T15:39:20.190795",
     "exception": false,
     "start_time": "2021-02-18T15:39:20.154704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---- GroupKFold ----\n",
    "class GroupKFold(object):\n",
    "    \"\"\"\n",
    "    GroupKFold with random shuffle with a sklearn-like structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, group=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y, group):\n",
    "        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n",
    "        unique_ids = X[group].unique()\n",
    "        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n",
    "            # split group\n",
    "            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n",
    "            train_idx = np.where(X[group].isin(tr_group))[0]\n",
    "            val_idx = np.where(X[group].isin(va_group))[0]\n",
    "            yield train_idx, val_idx\n",
    "\n",
    "# ---- StratifiedGroupKFold ----\n",
    "class StratifiedGroupKFold(object):\n",
    "    \"\"\"\n",
    "    StratifiedGroupKFold with random shuffle with a sklearn-like structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, group=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y, group):\n",
    "        labels_num = np.max(y) + 1\n",
    "        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "        y_distr = Counter()\n",
    "        groups = X[group].values\n",
    "        for label, g in zip(y, groups):\n",
    "            y_counts_per_group[g][label] += 1\n",
    "            y_distr[label] += 1\n",
    "\n",
    "        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "        groups_per_fold = defaultdict(set)\n",
    "\n",
    "        def eval_y_counts_per_fold(y_counts, fold):\n",
    "            y_counts_per_fold[fold] += y_counts\n",
    "            std_per_label = []\n",
    "            for label in range(labels_num):\n",
    "                label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(self.n_splits)])\n",
    "                std_per_label.append(label_std)\n",
    "            y_counts_per_fold[fold] -= y_counts\n",
    "            return np.mean(std_per_label)\n",
    "        \n",
    "        groups_and_y_counts = list(y_counts_per_group.items())\n",
    "        random.Random(self.random_state).shuffle(groups_and_y_counts)\n",
    "\n",
    "        for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n",
    "            best_fold = None\n",
    "            min_eval = None\n",
    "            for i in range(self.n_splits):\n",
    "                fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "                if min_eval is None or fold_eval < min_eval:\n",
    "                    min_eval = fold_eval\n",
    "                    best_fold = i\n",
    "            y_counts_per_fold[best_fold] += y_counts\n",
    "            groups_per_fold[best_fold].add(g)\n",
    "\n",
    "        all_groups = set(groups)\n",
    "        for i in range(self.n_splits):\n",
    "            train_groups = all_groups - groups_per_fold[i]\n",
    "            test_groups = groups_per_fold[i]\n",
    "\n",
    "            train_idx = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "            test_idx = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "            yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010391,
     "end_time": "2021-02-18T15:39:20.211364",
     "exception": false,
     "start_time": "2021-02-18T15:39:20.200973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:20.239890Z",
     "iopub.status.busy": "2021-02-18T15:39:20.239049Z",
     "iopub.status.idle": "2021-02-18T15:39:33.484635Z",
     "shell.execute_reply": "2021-02-18T15:39:33.483996Z"
    },
    "papermill": {
     "duration": 13.263289,
     "end_time": "2021-02-18T15:39:33.484736",
     "exception": false,
     "start_time": "2021-02-18T15:39:20.221447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_feather('../input/jane-street-save-as-feather/train.feather') # faster data load\n",
    "train = train.query(f'date >= {START_DATE}').reset_index(drop = True) \n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "#train = train.query('weight != 0').reset_index(drop = True)\n",
    "#train['action'] = (train['resp'] > 0).astype('int')\n",
    "train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:33.742908Z",
     "iopub.status.busy": "2021-02-18T15:39:33.741649Z",
     "iopub.status.idle": "2021-02-18T15:39:34.371012Z",
     "shell.execute_reply": "2021-02-18T15:39:34.370349Z"
    },
    "papermill": {
     "duration": 0.875644,
     "end_time": "2021-02-18T15:39:34.371116",
     "exception": false,
     "start_time": "2021-02-18T15:39:33.495472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train = train.sample(10000, random_state=SEED)\n",
    "\n",
    "X = train[features].values\n",
    "y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010531,
     "end_time": "2021-02-18T15:39:34.392958",
     "exception": false,
     "start_time": "2021-02-18T15:39:34.382427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:34.424865Z",
     "iopub.status.busy": "2021-02-18T15:39:34.424202Z",
     "iopub.status.idle": "2021-02-18T15:39:34.428043Z",
     "shell.execute_reply": "2021-02-18T15:39:34.427608Z"
    },
    "papermill": {
     "duration": 0.024667,
     "end_time": "2021-02-18T15:39:34.428123",
     "exception": false,
     "start_time": "2021-02-18T15:39:34.403456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim,output_dim,noise=0.05):\n",
    "    i = tf.keras.layers.Input(input_dim)\n",
    "    encoded = tf.keras.layers.BatchNormalization()(i)\n",
    "    encoded = tf.keras.layers.GaussianNoise(noise)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(64,activation='relu')(encoded)\n",
    "    decoded = tf.keras.layers.Dropout(0.2)(encoded)\n",
    "    decoded = tf.keras.layers.Dense(input_dim,name='decoded')(decoded)\n",
    "    x = tf.keras.layers.Dense(32,activation='relu')(decoded)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(output_dim,activation='sigmoid',name='label_output')(x)\n",
    "    \n",
    "    encoder = tf.keras.models.Model(inputs=i,outputs=encoded)\n",
    "    autoencoder = tf.keras.models.Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), \n",
    "                        loss={'decoded':'mse','label_output':'binary_crossentropy'})\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:34.464052Z",
     "iopub.status.busy": "2021-02-18T15:39:34.462729Z",
     "iopub.status.idle": "2021-02-18T15:39:34.465248Z",
     "shell.execute_reply": "2021-02-18T15:39:34.465674Z"
    },
    "papermill": {
     "duration": 0.027347,
     "end_time": "2021-02-18T15:39:34.465768",
     "exception": false,
     "start_time": "2021-02-18T15:39:34.438421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_1dcnn(input_dim, output_dim, encoder):\n",
    "    # input\n",
    "    inputs = tf.keras.layers.Input(input_dim)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = tf.keras.layers.Concatenate()([x,inputs]) #use both raw and encoded features\n",
    "    \n",
    "    # normalize\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    # 1dcnn\n",
    "    x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
    "    x = tf.keras.layers.Reshape((256, 16))(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=16,\n",
    "                      kernel_size=7,\n",
    "                      strides=1,\n",
    "                      activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    # ffn\n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.Dense(256 // (2 ** i), activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.GaussianNoise(0.01)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(output_dim, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs,outputs=x)\n",
    "    \n",
    "    # compile\n",
    "    opt = tfa.optimizers.RectifiedAdam(learning_rate=1e-03)\n",
    "    opt = tfa.optimizers.SWA(opt)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-02)\n",
    "    model.compile(optimizer=opt, \n",
    "                  loss=loss, \n",
    "                  metrics=[tf.keras.metrics.AUC(name = 'auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:39:34.492752Z",
     "iopub.status.busy": "2021-02-18T15:39:34.492148Z",
     "iopub.status.idle": "2021-02-18T15:41:40.046915Z",
     "shell.execute_reply": "2021-02-18T15:41:40.046297Z"
    },
    "papermill": {
     "duration": 125.570806,
     "end_time": "2021-02-18T15:41:40.047023",
     "exception": false,
     "start_time": "2021-02-18T15:39:34.476217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "691/691 [==============================] - 5s 7ms/step - loss: 2.3702 - decoded_loss: 1.6590 - label_output_loss: 0.7112 - val_loss: 1.2577 - val_decoded_loss: 0.5673 - val_label_output_loss: 0.6904\n",
      "Epoch 2/1000\n",
      "691/691 [==============================] - 6s 8ms/step - loss: 1.8165 - decoded_loss: 1.1246 - label_output_loss: 0.6920 - val_loss: 1.1871 - val_decoded_loss: 0.4972 - val_label_output_loss: 0.6898\n",
      "Epoch 3/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.7667 - decoded_loss: 1.0756 - label_output_loss: 0.6911 - val_loss: 1.1277 - val_decoded_loss: 0.4380 - val_label_output_loss: 0.6896\n",
      "Epoch 4/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.7414 - decoded_loss: 1.0505 - label_output_loss: 0.6909 - val_loss: 1.1125 - val_decoded_loss: 0.4232 - val_label_output_loss: 0.6893\n",
      "Epoch 5/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.7243 - decoded_loss: 1.0336 - label_output_loss: 0.6908 - val_loss: 1.0975 - val_decoded_loss: 0.4082 - val_label_output_loss: 0.6893\n",
      "Epoch 6/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.7113 - decoded_loss: 1.0206 - label_output_loss: 0.6906 - val_loss: 1.0796 - val_decoded_loss: 0.3905 - val_label_output_loss: 0.6891\n",
      "Epoch 7/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6968 - decoded_loss: 1.0062 - label_output_loss: 0.6905 - val_loss: 1.0853 - val_decoded_loss: 0.3962 - val_label_output_loss: 0.6890\n",
      "Epoch 8/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.7034 - decoded_loss: 1.0129 - label_output_loss: 0.6904 - val_loss: 1.0693 - val_decoded_loss: 0.3804 - val_label_output_loss: 0.6889\n",
      "Epoch 9/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6880 - decoded_loss: 0.9976 - label_output_loss: 0.6904 - val_loss: 1.0618 - val_decoded_loss: 0.3729 - val_label_output_loss: 0.6889\n",
      "Epoch 10/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6919 - decoded_loss: 1.0016 - label_output_loss: 0.6903 - val_loss: 1.0563 - val_decoded_loss: 0.3673 - val_label_output_loss: 0.6890\n",
      "Epoch 11/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6755 - decoded_loss: 0.9853 - label_output_loss: 0.6902 - val_loss: 1.0540 - val_decoded_loss: 0.3651 - val_label_output_loss: 0.6889\n",
      "Epoch 12/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6804 - decoded_loss: 0.9902 - label_output_loss: 0.6902 - val_loss: 1.0606 - val_decoded_loss: 0.3717 - val_label_output_loss: 0.6890\n",
      "Epoch 13/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6707 - decoded_loss: 0.9805 - label_output_loss: 0.6902 - val_loss: 1.0522 - val_decoded_loss: 0.3634 - val_label_output_loss: 0.6888\n",
      "Epoch 14/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6719 - decoded_loss: 0.9817 - label_output_loss: 0.6902 - val_loss: 1.0462 - val_decoded_loss: 0.3572 - val_label_output_loss: 0.6890\n",
      "Epoch 15/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6712 - decoded_loss: 0.9811 - label_output_loss: 0.6902 - val_loss: 1.0483 - val_decoded_loss: 0.3596 - val_label_output_loss: 0.6887\n",
      "Epoch 16/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6671 - decoded_loss: 0.9770 - label_output_loss: 0.6901 - val_loss: 1.0524 - val_decoded_loss: 0.3636 - val_label_output_loss: 0.6889\n",
      "Epoch 17/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6643 - decoded_loss: 0.9741 - label_output_loss: 0.6901 - val_loss: 1.0437 - val_decoded_loss: 0.3549 - val_label_output_loss: 0.6888\n",
      "Epoch 18/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6678 - decoded_loss: 0.9777 - label_output_loss: 0.6901 - val_loss: 1.0420 - val_decoded_loss: 0.3531 - val_label_output_loss: 0.6889\n",
      "Epoch 19/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6655 - decoded_loss: 0.9754 - label_output_loss: 0.6901 - val_loss: 1.0414 - val_decoded_loss: 0.3527 - val_label_output_loss: 0.6887\n",
      "Epoch 20/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6604 - decoded_loss: 0.9703 - label_output_loss: 0.6901 - val_loss: 1.0372 - val_decoded_loss: 0.3485 - val_label_output_loss: 0.6887\n",
      "Epoch 21/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6668 - decoded_loss: 0.9768 - label_output_loss: 0.6900 - val_loss: 1.0528 - val_decoded_loss: 0.3641 - val_label_output_loss: 0.6887\n",
      "Epoch 22/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6569 - decoded_loss: 0.9668 - label_output_loss: 0.6901 - val_loss: 1.0524 - val_decoded_loss: 0.3638 - val_label_output_loss: 0.6887\n",
      "Epoch 23/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6579 - decoded_loss: 0.9679 - label_output_loss: 0.6900 - val_loss: 1.0429 - val_decoded_loss: 0.3542 - val_label_output_loss: 0.6888\n",
      "Epoch 24/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6578 - decoded_loss: 0.9677 - label_output_loss: 0.6900 - val_loss: 1.0448 - val_decoded_loss: 0.3561 - val_label_output_loss: 0.6888\n",
      "Epoch 25/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6590 - decoded_loss: 0.9690 - label_output_loss: 0.6900 - val_loss: 1.0471 - val_decoded_loss: 0.3584 - val_label_output_loss: 0.6888\n",
      "Epoch 26/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6519 - decoded_loss: 0.9619 - label_output_loss: 0.6900 - val_loss: 1.0467 - val_decoded_loss: 0.3580 - val_label_output_loss: 0.6887\n",
      "Epoch 27/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6540 - decoded_loss: 0.9640 - label_output_loss: 0.6900 - val_loss: 1.0439 - val_decoded_loss: 0.3553 - val_label_output_loss: 0.6886\n",
      "Epoch 28/1000\n",
      "691/691 [==============================] - 4s 5ms/step - loss: 1.6537 - decoded_loss: 0.9637 - label_output_loss: 0.6900 - val_loss: 1.0465 - val_decoded_loss: 0.3577 - val_label_output_loss: 0.6888\n",
      "Epoch 29/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6524 - decoded_loss: 0.9625 - label_output_loss: 0.6899 - val_loss: 1.0490 - val_decoded_loss: 0.3604 - val_label_output_loss: 0.6886\n",
      "Epoch 30/1000\n",
      "691/691 [==============================] - 4s 6ms/step - loss: 1.6658 - decoded_loss: 0.9759 - label_output_loss: 0.6899 - val_loss: 1.0446 - val_decoded_loss: 0.3558 - val_label_output_loss: 0.6888\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\n",
    "autoencoder.fit(X,(X,y),\n",
    "                epochs=1000,\n",
    "                batch_size=2048, \n",
    "                validation_split=0.1,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\n",
    "encoder.save_weights('./encoder.hdf5')\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:41:41.199139Z",
     "iopub.status.busy": "2021-02-18T15:41:41.198142Z",
     "iopub.status.idle": "2021-02-18T15:48:05.087958Z",
     "shell.execute_reply": "2021-02-18T15:48:05.087482Z"
    },
    "papermill": {
     "duration": 384.468978,
     "end_time": "2021-02-18T15:48:05.088064",
     "exception": false,
     "start_time": "2021-02-18T15:41:40.619086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "71/71 [==============================] - 4s 53ms/step - loss: 0.8121 - auc: 0.5121 - val_loss: 0.7201 - val_auc: 0.5105\n",
      "Epoch 2/192\n",
      "71/71 [==============================] - 5s 77ms/step - loss: 0.7401 - auc: 0.5336 - val_loss: 0.7155 - val_auc: 0.5136\n",
      "Epoch 3/192\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 0.7181 - auc: 0.5499 - val_loss: 0.7065 - val_auc: 0.5141\n",
      "Epoch 4/192\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 0.7051 - auc: 0.5648 - val_loss: 0.6994 - val_auc: 0.5204\n",
      "Epoch 5/192\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 0.6936 - auc: 0.5803 - val_loss: 0.6991 - val_auc: 0.5222\n",
      "Epoch 6/192\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 0.6837 - auc: 0.5944 - val_loss: 0.6979 - val_auc: 0.5273\n",
      "Epoch 7/192\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 0.6755 - auc: 0.6080 - val_loss: 0.7005 - val_auc: 0.5295\n",
      "Epoch 8/192\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 0.6670 - auc: 0.6217 - val_loss: 0.7007 - val_auc: 0.5299\n",
      "Epoch 9/192\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 0.6599 - auc: 0.6345 - val_loss: 0.7100 - val_auc: 0.5277\n",
      "Epoch 10/192\n",
      "71/71 [==============================] - 2s 21ms/step - loss: 0.6538 - auc: 0.6457 - val_loss: 0.7122 - val_auc: 0.5267\n",
      "Epoch 11/192\n",
      "71/71 [==============================] - 2s 21ms/step - loss: 0.6478 - auc: 0.6567 - val_loss: 0.7193 - val_auc: 0.5305\n",
      "Epoch 12/192\n",
      "71/71 [==============================] - 2s 21ms/step - loss: 0.6417 - auc: 0.6674 - val_loss: 0.7286 - val_auc: 0.5226\n",
      "Epoch 13/192\n",
      "71/71 [==============================] - 2s 21ms/step - loss: 0.6349 - auc: 0.6768 - val_loss: 0.7330 - val_auc: 0.5232\n",
      "Epoch 14/192\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.6290 - auc: 0.6867\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 0.6290 - auc: 0.6868 - val_loss: 0.7362 - val_auc: 0.5266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:41, 41.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.7690 - auc: 0.5150 - val_loss: 0.6974 - val_auc: 0.5169\n",
      "Epoch 2/192\n",
      "182/182 [==============================] - 7s 39ms/step - loss: 0.7113 - auc: 0.5376 - val_loss: 0.6921 - val_auc: 0.5314\n",
      "Epoch 3/192\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6981 - auc: 0.5548 - val_loss: 0.6926 - val_auc: 0.5343\n",
      "Epoch 4/192\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6913 - auc: 0.5686 - val_loss: 0.6938 - val_auc: 0.5364\n",
      "Epoch 5/192\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6862 - auc: 0.5792 - val_loss: 0.6948 - val_auc: 0.5368\n",
      "Epoch 6/192\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6826 - auc: 0.5880 - val_loss: 0.6969 - val_auc: 0.5367\n",
      "Epoch 7/192\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6780 - auc: 0.5975 - val_loss: 0.6993 - val_auc: 0.5337\n",
      "Epoch 8/192\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6733 - auc: 0.6059 - val_loss: 0.7036 - val_auc: 0.5324\n",
      "Epoch 9/192\n",
      "182/182 [==============================] - 3s 19ms/step - loss: 0.6692 - auc: 0.6146 - val_loss: 0.7077 - val_auc: 0.5342\n",
      "Epoch 10/192\n",
      "180/182 [============================>.] - ETA: 0s - loss: 0.6645 - auc: 0.6227\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "182/182 [==============================] - 3s 19ms/step - loss: 0.6645 - auc: 0.6227 - val_loss: 0.7104 - val_auc: 0.5314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:28, 43.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "304/304 [==============================] - 7s 22ms/step - loss: 0.7496 - auc: 0.5178 - val_loss: 0.6925 - val_auc: 0.5340\n",
      "Epoch 2/192\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.7028 - auc: 0.5407 - val_loss: 0.6911 - val_auc: 0.5404\n",
      "Epoch 3/192\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 0.6936 - auc: 0.5551 - val_loss: 0.6903 - val_auc: 0.5425\n",
      "Epoch 4/192\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 0.6892 - auc: 0.5648 - val_loss: 0.6917 - val_auc: 0.5406\n",
      "Epoch 5/192\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 0.6858 - auc: 0.5726 - val_loss: 0.6929 - val_auc: 0.5418\n",
      "Epoch 6/192\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 0.6826 - auc: 0.5798 - val_loss: 0.6949 - val_auc: 0.5409\n",
      "Epoch 7/192\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 0.6792 - auc: 0.5868 - val_loss: 0.6974 - val_auc: 0.5426\n",
      "Epoch 8/192\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 0.6757 - auc: 0.5945 - val_loss: 0.6990 - val_auc: 0.5422\n",
      "Epoch 9/192\n",
      "304/304 [==============================] - 6s 19ms/step - loss: 0.6717 - auc: 0.6018 - val_loss: 0.7009 - val_auc: 0.5391\n",
      "Epoch 10/192\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 0.6678 - auc: 0.6095 - val_loss: 0.7044 - val_auc: 0.5376\n",
      "Epoch 11/192\n",
      "301/304 [============================>.] - ETA: 0s - loss: 0.6631 - auc: 0.6184\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 0.6631 - auc: 0.6184 - val_loss: 0.7086 - val_auc: 0.5403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:39, 51.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "435/435 [==============================] - 9s 20ms/step - loss: 0.7321 - auc: 0.5212 - val_loss: 0.6927 - val_auc: 0.5295\n",
      "Epoch 2/192\n",
      "435/435 [==============================] - 11s 26ms/step - loss: 0.6985 - auc: 0.5446 - val_loss: 0.6912 - val_auc: 0.5380\n",
      "Epoch 3/192\n",
      "435/435 [==============================] - 7s 16ms/step - loss: 0.6921 - auc: 0.5552 - val_loss: 0.6913 - val_auc: 0.5393\n",
      "Epoch 4/192\n",
      "435/435 [==============================] - 7s 16ms/step - loss: 0.6886 - auc: 0.5631 - val_loss: 0.6909 - val_auc: 0.5415\n",
      "Epoch 5/192\n",
      "435/435 [==============================] - 7s 16ms/step - loss: 0.6854 - auc: 0.5703 - val_loss: 0.6929 - val_auc: 0.5434\n",
      "Epoch 6/192\n",
      "435/435 [==============================] - 8s 17ms/step - loss: 0.6823 - auc: 0.5767 - val_loss: 0.6954 - val_auc: 0.5408\n",
      "Epoch 7/192\n",
      "435/435 [==============================] - 7s 17ms/step - loss: 0.6788 - auc: 0.5843 - val_loss: 0.6960 - val_auc: 0.5388\n",
      "Epoch 8/192\n",
      "435/435 [==============================] - 7s 17ms/step - loss: 0.6751 - auc: 0.5916 - val_loss: 0.7033 - val_auc: 0.5388\n",
      "Epoch 9/192\n",
      "435/435 [==============================] - 7s 16ms/step - loss: 0.6712 - auc: 0.5998 - val_loss: 0.7092 - val_auc: 0.5362\n",
      "Epoch 10/192\n",
      "435/435 [==============================] - 8s 18ms/step - loss: 0.6673 - auc: 0.6080 - val_loss: 0.7119 - val_auc: 0.5348\n",
      "Epoch 11/192\n",
      "435/435 [==============================] - 7s 16ms/step - loss: 0.6629 - auc: 0.6173 - val_loss: 0.7137 - val_auc: 0.5352\n",
      "Epoch 12/192\n",
      "433/435 [============================>.] - ETA: 0s - loss: 0.6587 - auc: 0.6257\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "435/435 [==============================] - 7s 16ms/step - loss: 0.6587 - auc: 0.6257 - val_loss: 0.7227 - val_auc: 0.5328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [04:19, 66.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "574/574 [==============================] - 11s 20ms/step - loss: 0.7311 - auc: 0.5193 - val_loss: 0.6905 - val_auc: 0.5424\n",
      "Epoch 2/192\n",
      "574/574 [==============================] - 13s 23ms/step - loss: 0.6951 - auc: 0.5439 - val_loss: 0.6888 - val_auc: 0.5498\n",
      "Epoch 3/192\n",
      "574/574 [==============================] - 9s 16ms/step - loss: 0.6904 - auc: 0.5552 - val_loss: 0.6893 - val_auc: 0.5507\n",
      "Epoch 4/192\n",
      "574/574 [==============================] - 10s 17ms/step - loss: 0.6876 - auc: 0.5622 - val_loss: 0.6897 - val_auc: 0.5494\n",
      "Epoch 5/192\n",
      "574/574 [==============================] - 9s 16ms/step - loss: 0.6849 - auc: 0.5684 - val_loss: 0.6905 - val_auc: 0.5512\n",
      "Epoch 6/192\n",
      "574/574 [==============================] - 9s 16ms/step - loss: 0.6818 - auc: 0.5752 - val_loss: 0.6922 - val_auc: 0.5476\n",
      "Epoch 7/192\n",
      "574/574 [==============================] - 9s 16ms/step - loss: 0.6785 - auc: 0.5823 - val_loss: 0.6946 - val_auc: 0.5482\n",
      "Epoch 8/192\n",
      "574/574 [==============================] - 10s 17ms/step - loss: 0.6751 - auc: 0.5908 - val_loss: 0.6988 - val_auc: 0.5466\n",
      "Epoch 9/192\n",
      "574/574 [==============================] - 9s 16ms/step - loss: 0.6711 - auc: 0.5998 - val_loss: 0.7005 - val_auc: 0.5450\n",
      "Epoch 10/192\n",
      "571/574 [============================>.] - ETA: 0s - loss: 0.6670 - auc: 0.6084\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "574/574 [==============================] - 9s 16ms/step - loss: 0.6670 - auc: 0.6084 - val_loss: 0.7059 - val_auc: 0.5449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [06:08, 73.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 38s, sys: 28.4 s, total: 6min 6s\n",
      "Wall time: 6min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "if CV_STRATEGY == 'PurgedGroupTimeSeriesSplit':\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits=FOLDS, group_gap=20)\n",
    "    splits = list(gkf.split(y, groups=train['date'].values))    \n",
    "    \n",
    "elif CV_STRATEGY == \"GroupKFold\":\n",
    "    cv = GroupKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    splits = cv.split(train, train['resp'].values.astype(int), 'date')\n",
    "\n",
    "elif CV_STRATEGY ==  \"StratifiedGroupKFold\":\n",
    "    cv = StratifiedGroupKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    splits = cv.split(train, train['resp'].values.astype(int), 'date')\n",
    "\n",
    "models = []\n",
    "for fold, (train_indices, test_indices) in tqdm(enumerate(splits)):\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    # model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_1dcnn(X.shape[-1], y.shape[-1], encoder)\n",
    "    \n",
    "    # callbacks\n",
    "    er = tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss')\n",
    "    ReduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, mode='min')\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'./model_{SEED}_{fold}.hdf5', save_weights_only=True, verbose=0, monitor='val_loss', save_best_only=True)\n",
    "    nn_callbacks = [er, ReduceLR, model_checkpoint_callback]\n",
    "    \n",
    "    # fit\n",
    "    model.fit(X_train, y_train, validation_data=(X_test,y_test), \n",
    "              epochs=192, batch_size=2048, callbacks=nn_callbacks)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T15:48:08.791924Z",
     "iopub.status.busy": "2021-02-18T15:48:08.790957Z",
     "iopub.status.idle": "2021-02-18T15:48:08.806497Z",
     "shell.execute_reply": "2021-02-18T15:48:08.802900Z"
    },
    "papermill": {
     "duration": 1.901111,
     "end_time": "2021-02-18T15:48:08.806746",
     "exception": false,
     "start_time": "2021-02-18T15:48:06.905635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"f = np.median\\nth = 0.500\\n\\nimport janestreet\\nenv = janestreet.make_env()\\nfor (test_df, pred_df) in tqdm(env.iter_test()):\\n    if test_df['weight'].item() > 0:\\n        x_tt = test_df.loc[:, features].values\\n        \\n        # GBDT inference with treelite\\n        batch = treelite_runtime.Batch.from_npy2d(x_tt)\\n        xgb_pred = predictor.predict(batch)\\n    \\n        # NN inference\\n        if np.isnan(x_tt[:, 1:].sum()):\\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\\n        \\n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\\n        pred = f(pred)\\n        \\n        # ensemble\\n        pred_df.action = np.where(0.9*pred + 0.1*xgb_pred >= th, 1, 0).astype(int)\\n    else:\\n        pred_df.action = 0\\n    env.predict(pred_df)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"f = np.median\n",
    "th = 0.500\n",
    "\n",
    "import janestreet\n",
    "env = janestreet.make_env()\n",
    "for (test_df, pred_df) in tqdm(env.iter_test()):\n",
    "    if test_df['weight'].item() > 0:\n",
    "        x_tt = test_df.loc[:, features].values\n",
    "        \n",
    "        # GBDT inference with treelite\n",
    "        batch = treelite_runtime.Batch.from_npy2d(x_tt)\n",
    "        xgb_pred = predictor.predict(batch)\n",
    "    \n",
    "        # NN inference\n",
    "        if np.isnan(x_tt[:, 1:].sum()):\n",
    "            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "        \n",
    "        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n",
    "        pred = f(pred)\n",
    "        \n",
    "        # ensemble\n",
    "        pred_df.action = np.where(0.9*pred + 0.1*xgb_pred >= th, 1, 0).astype(int)\n",
    "    else:\n",
    "        pred_df.action = 0\n",
    "    env.predict(pred_df)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 542.114202,
   "end_time": "2021-02-18T15:48:12.605485",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-18T15:39:10.491283",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
