{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jane Street Market Prediction Using XGBoost"},{"metadata":{},"cell_type":"markdown","source":"# 1.Overview\n\n## 1.1 Description\nIn a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their “fair values” and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world. Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. \n\n## 1.2 Evaluation\n- Utility score <br>\n    - Each row in the test set represents a trading opportunity for which **you will be predicting an `action` value, 1 to make the trade and 0 to pass on it.** Each trade `j` has an associated `weight` and `resp`, which represents a return.\n\n$$p_i = \\sum_j(weight_{ij} * resp_{ij} * action_{ij}),$$\n$$ t = \\frac{\\sum p_i }{\\sqrt{\\sum p_i^2}} * \\sqrt{\\frac{250}{|i|}},$$\nwhere |i| is the number of unique dates in the test set. The utility is \t\t\t\tthen defined as:\n\n$$u = min(max(t,0), 6)  \\sum p_i.$$\n\n>https://www.kaggle.com/renataghisloti/understanding-the-utility-score-function\n\n- $p_i$\n\n    - Each row or trading opportunity can be chosen (action == 1) or not (action == 0). The variable _pi_ is a indicator for each day _i_, showing how much return we got for that day. **Since we want to maximize u, we also want to maximize _pi_**. To do that, we have to select the **least amount of negative _resp_ values** as possible (since this is the only negative value in my equation and only value that would make the total sum of p going down) and maximize the positive number of positive _resp_ transactions we select.\n\n- $t$\n\n    - **_t_** is **larger** when the return for **each day is better distributed and has lower variation.** It is better to have returns uniformly divided among days than have all of your returns concentrated in just one day. It reminds me a little of a **_L1_** over **_L2_** situation, where the **_L2_** norm penalizes outliers more than **_L1_**. Basically, we want to select uniformly distributed distributed returns over days, maximizing our return but giving a penalty on choosing too many dates.\n\n    - t is simply the annualized sharpe ratio assuming that there are 250 trading days in a year, an important risk adjusted performance measure in investing. If sharpe ratio is negative, utility is zero. A sharpe ratio higher than 6 is very unlikely, so it is capped at 6. The utility function overall try to maximize the product of sharpe ratio and total return.\n\n\n\n>https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning\n\nMarket Basics: Financial market is a dynamic world where investors, speculators, traders, hedgers understand the market by different strategies and use the opportunities to make profit. They may use fundamental, technical analysis, sentimental analysis,etc. to place their bet. As data is growing, many professionals use data to understand and analyse previous trends and predict the future prices to book profit.\n\nCompetition Description: The dataset provided contains set of features, feature_{0...129},representing real stock market data. Each row in the dataset represents a trading opportunity, for which we will be predicting an action value: 1 to make the trade and 0 to pass on it. Each trade has an associated weight and resp, which together represents a return on the trade. In the training set, train.csv, you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons.\n\nIn Test set we don't have resp value, and other resp_{1,2,3,4} data, so we have to use only feature_{0...129} to make prediction.\n\nTrades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation. So we will ignore it.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 2. Implementing\n\n> https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners\n\n> https://www.kaggle.com/eudmar/jane-street-eda-pca-ensemble-methods\n\n> https://www.kaggle.com/muhammadmelsherbini/jane-street-extensive-eda-pca-starter\n"},{"metadata":{},"cell_type":"markdown","source":"## 0) Reduce Memory Usage\n> https://www.kaggle.com/sbunzini/reduce-memory-usage-by-75"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_memory_usage(df):\n    \n    start_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_memory_usage(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1) import library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2) load and clean dataset"},{"metadata":{},"cell_type":"markdown","source":"### Cleaning data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows with 'weight'=0 \n# Trades with weight = 0 were intentionally included in the dataset for completeness, \n# although such trades will not contribute towards the scoring evaluation\ntrain = train[train['weight']!=0]\n\n\n# Create 'action' column (dependent variable)\n# The 'action' column is defined as such because of the evaluation metric used for this project.\n# We want to maximise the utility function and hence pi where pi=∑j(weightij∗respij∗actionij)\n# Positive values of resp will increase pi\ntrain['action'] = train['resp'].apply(lambda x:x>0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in list(train.columns) if 'feature' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = pd.DataFrame()\nmissing_values['column'] = features\nmissing_values['num_missing'] = [train[i].isna().sum() for i in features]\nmissing_values.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRADING_THRESHOLD = 0.500\n#Checking Missing Values in the features\nn_features = 45\nnan_val = train.isna().sum()[train.isna().sum() > 0].sort_values(ascending=False)\nprint(nan_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(10, 10))\n\nsns.barplot(y = nan_val.index[0:n_features], \n            x = nan_val.values[0:n_features], \n            alpha = 0.8\n           )\n\nplt.title(f'NaN values of train dataset (Top {n_features})')\nplt.xlabel('NaN values')\nfig.savefig(f'nan_values_top_{n_features}_features.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Train and Test DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features]\ny = train['action']\n\n# Next, we hold out part of the training data to form the hold-out validation set\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_median = train_x.median()\n# Impute medians in both training set and the hold-out validation set\ntrain_x = train_x.fillna(train_median)\nvalid_x = valid_x.fillna(train_median)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we want to check if the target class is balanced or unbalanced in the training data\nsns.set_palette(\"colorblind\")\nax = sns.barplot(train_y.value_counts().index, train_y.value_counts()/len(train_y))\nax.set_title(\"Proportion of trades with action=0 and action=1\")\nax.set_ylabel(\"Percentage\")\nax.set_xlabel(\"Action\")\nsns.despine();\n# Target class is fairly balanced with almost 50% of trades corresponding to each action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, we plot a diagonal correlation heatmap to see if there are strong correlations between the features\n\n# Compute the correlation matrix\ncorr = train_x.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(20, 230, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n# There are strong correlations between several of the features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = features\np.append('resp')\nlen(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train[p].corr()\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x.abs()\nupper = x.where(np.triu(np.ones(x.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(to_drop, 1, inplace=True)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Resp Analysis\n#Last subplot doesn't mean anything\nresp_df = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\nfig, axes = plt.subplots(nrows=2\n                         , ncols=3,figsize=(20,10))\nfor i, column in enumerate(resp_df):\n    sns.distplot(train[column],ax=axes[i//3,i%3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cumulative return analysis\nfig, ax = plt.subplots(figsize=(16, 8))\n\nresp = train['resp'].cumsum()\nresp_1 = train['resp_1'].cumsum()\nresp_2 = train['resp_2'].cumsum()\nresp_3 = train['resp_3'].cumsum()\nresp_4 = train['resp_4'].cumsum()\n\nresp.plot(linewidth=2)\nresp_1.plot(linewidth=2)\nresp_2.plot(linewidth=2)\nresp_3.plot(linewidth=2)\nresp_4.plot(linewidth=2)\n\nax.set_xlabel (\"Trade\", fontsize=12)\nax.set_title (\"Cumulative Trade Returns\", fontsize=18)\n\nplt.legend(loc=\"upper left\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"resp and resp_4 variable are closely related so we can use this to set our 'action' variable."},{"metadata":{},"cell_type":"markdown","source":"# 3. Principal Components Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# Before we perform PCA, we need to normalise the features so that they have zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(train_x)\ntrain_x_norm = scaler.transform(train_x)\n\npca = PCA()\ncomp = pca.fit(train_x_norm)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# We plot a graph to show how the explained variation in the 129 features varies with the number of principal components\nplt.plot(np.cumsum(comp.explained_variance_ratio_))\nplt.grid()\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Explained Variance')\nsns.despine();\n\n# The first 15 principal components explains about 80% of the variation\n# The first 40 principal components explains about 95% of the variation\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# Using the first 50 principal components, we apply the PCA mapping\n# From here on, we work with only 50 features instead of the full set of 129 features\npca = PCA(n_components=50).fit(train_x_norm)\ntrain_x_transform = pca.transform(train_x_norm)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# Transform the validation set\nvalid_x_transform = pca.transform(scaler.transform(valid_x))\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Train XGBoost classifier + Tune hyperparameters using Optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create the XGboost-specific DMatrix data format from the numpy array. \n# This data structure is optimised for memory efficiency and training speed\ndtrain = xgb.DMatrix(train_x, label=train_y)\ndvalid = xgb.DMatrix(valid_x, label=valid_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The objective function is passed an Optuna specific argument of trial\ndef objective(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 25),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n# trials will be evaluated based on their accuracy on the test set\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=25, timeout=600)\n\n    print(\"Number of finished trials: \", len(study.trials))\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = trial.params\nbest_params['tree_method'] = 'gpu_hist' \nbest_params['objective'] = 'binary:logistic'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the XGBoost classifier with optimal hyperparameters\noptimal_clf = xgb.XGBClassifier(**best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_clf.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot how the best accuracy evolves with number of trials\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can also plot the relative importance of different hyperparameter settings\nfig = optuna.visualization.plot_param_importances(study)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.Fit classifier on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We impute the missing values with the medians\ndef fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"for (test_df, sample_prediction_df) in iter_test:\n    wt = test_df.iloc[0].weight\n    if(wt == 0):\n        sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = optimal_clf.predict(pca.transform(scaler.transform(fillna_npwhere(test_df[features].values,train_median[features].values))))\n    env.predict(sample_prediction_df)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, pred_df) in iter_test:\n    if test_df['weight'].item() > 0:\n        X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n        y_preds = clf.predict(X_test)\n        pred_df.action = y_preds\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"reference\n\n>    https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners\n>    https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}