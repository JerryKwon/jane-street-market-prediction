{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "pytorch-resnet-labs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bdb3abc2a4934f18a08922eb67eae42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c4b329c767c344519dbf1a97eadd8c3e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_52714ab9312b431b8da8d2faa88ecdb2",
              "IPY_MODEL_5676634bf08144538dc05a0d0bdd07e7"
            ]
          }
        },
        "c4b329c767c344519dbf1a97eadd8c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52714ab9312b431b8da8d2faa88ecdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_03745e8ba8ce41be9b1d2426b287bfd8",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11d3af8a27434cff820d34ba79d2aeb4"
          }
        },
        "5676634bf08144538dc05a0d0bdd07e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b89a40d7afe546e7ab19250121fcec93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/100 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da4ec0f7c86a44ff88d69204049e0efa"
          }
        },
        "03745e8ba8ce41be9b1d2426b287bfd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11d3af8a27434cff820d34ba79d2aeb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b89a40d7afe546e7ab19250121fcec93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da4ec0f7c86a44ff88d69204049e0efa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA5j8tuwnevl",
        "outputId": "7175dd65-4342-44e4-ef5f-7cf1d5195785"
      },
      "source": [
        "\"\"\"Colab Drive Connection\"\"\"\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbMkDicinaV9",
        "outputId": "7360720d-8560-4cf0-e692-4cec8efe2960"
      },
      "source": [
        "import warnings\n",
        "import os \n",
        "\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "\n",
        "import networkx as nx\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# install datatable\n",
        "!pip install datatable\n",
        "import datatable as dt\n",
        "\n",
        "from numba import njit\n",
        "\n",
        "import gc\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\")\n",
        "\n",
        "# project_home = \"/kaggle/input/jane-street-market-prediction\"\n",
        "\n",
        "project_home = \"/gdrive/MyDrive/colab/jane-street-market-prediction\"\n",
        "data_home = os.path.join(project_home, \"input/data\")\n",
        "model_home = os.path.join(project_home, \"output/model\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datatable\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/cb/21810c43b687a19d194c372192049f535fba28c55ce76d37e7e407159c52/datatable-0.11.1-cp36-cp36m-manylinux2010_x86_64.whl (83.7MB)\n",
            "\u001b[K     |████████████████████████████████| 83.7MB 65kB/s \n",
            "\u001b[?25hInstalling collected packages: datatable\n",
            "Successfully installed datatable-0.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UAcoB2EqnaV-"
      },
      "source": [
        "entire_seed = 1029\n",
        "\n",
        "def seed_torch(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    \n",
        "seed_torch(entire_seed)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i6690m01naV-"
      },
      "source": [
        "train_file = os.path.join(data_home,'train.csv')\n",
        "features_file = os.path.join(data_home,'features.csv')\n",
        "example_test_file = os.path.join(data_home,'example_test.csv')\n",
        "example_sample_submission_file = os.path.join(data_home,'example_sample_submission.csv')\n",
        "\n",
        "train_data_datatable = dt.fread(train_file)\n",
        "\n",
        "df_train = train_data_datatable.to_pandas()\n",
        "df_features = pd.read_csv(features_file)\n",
        "df_example_test = pd.read_csv(example_test_file)\n",
        "df_example_sample_submission = pd.read_csv(example_sample_submission_file)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "syvZDP5OnaV-"
      },
      "source": [
        "features = [ col for col in df_train.columns if \"feature\" in col ]\n",
        "resps = [ col for col in df_train.columns if \"resp\" in col ]\n",
        "target_resp = [resp_ for resp_ in resps if \"_\" not in resp_]\n",
        "target = [\"weight\"] + target_resp + features "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9irkLVhYnaV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33aeabb4-ba5c-4d89-e61d-090852aaf657"
      },
      "source": [
        "\"\"\"\n",
        "Reduce Memory Usage by 75%\n",
        "https://www.kaggle.com/tomwarrens/nan-values-depending-on-time-of-day\n",
        "\"\"\"\n",
        "\n",
        "## Reduce Memory\n",
        "\n",
        "def reduce_memory_usage(df):\n",
        "    \n",
        "    start_memory = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"Memory usage of dataframe is {start_memory} MB\")\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != 'object':\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            \n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            \n",
        "            else:\n",
        "#                 reducing float16 for calculating numpy.nanmean\n",
        "#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "#                     df[col] = df[col].astype(np.float16)\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    pass\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "    \n",
        "    end_memory = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n",
        "    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\n",
        "    return df\n",
        "\n",
        "df_train = reduce_memory_usage(df_train)\n",
        "df_train.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2489.4869804382324 MB\n",
            "Memory usage of dataframe after reduction 1247.0233011245728 MB\n",
            "Reduced by 49.908422461199 % \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2390491 entries, 0 to 2390490\n",
            "Columns: 138 entries, date to ts_id\n",
            "dtypes: float32(135), int16(1), int32(1), int8(1)\n",
            "memory usage: 1.2 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9zAivGb3naV_"
      },
      "source": [
        "# drop before 85days\n",
        "df_train = df_train.loc[df_train.date>85]\n",
        "# drop weight 0 for training\n",
        "df_train = df_train.loc[df_train.weight > 0]\n",
        "\n",
        "df_labels = df_train[['date','weight','resp_1','resp_2','resp_3','resp_4','resp']]\n",
        "\n",
        "df_train = df_train.drop(df_labels.columns,axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "daU4lp_qnaV_"
      },
      "source": [
        "\"\"\"\n",
        "The codes from 'Optimise Speed of Filling-NaN Function'\n",
        "https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function\n",
        "\"\"\"\n",
        "\n",
        "def for_loop(method, matrix, values):\n",
        "    for i in range(matrix.shape[0]):\n",
        "        matrix[i] = method(matrix[i], values)\n",
        "    return matrix\n",
        "\n",
        "def for_loop_ffill(method, matrix):\n",
        "    tmp = np.zeros(matrix.shape[1],dtype=np.float32)\n",
        "    for i in range(matrix.shape[0]):\n",
        "        matrix[i] = method(matrix[i], tmp)\n",
        "        tmp = matrix[i]\n",
        "    return matrix\n",
        "\n",
        "@njit\n",
        "def fillna_npwhere_njit(array, values):\n",
        "    if np.isnan(array.sum()):\n",
        "        array = np.where(np.isnan(array), values, array)\n",
        "    return array"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vLaxNPOynaV_"
      },
      "source": [
        "# converting numpy for efficient calcualtion.\n",
        "# ft 1~129\n",
        "np_train = df_train.loc[:,features[1:]].values\n",
        "np_train.shape\n",
        "\n",
        "# ft 0\n",
        "np_train_ft0 = df_train.loc[:,features[0]].values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1pgiVhdxQiMg",
        "outputId": "463212bd-0456-4859-927e-ed00e5df3550"
      },
      "source": [
        "data_home"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gdrive/MyDrive/colab/jane-street-market-prediction/input/data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LSIsYjKlnaWA"
      },
      "source": [
        "# nead pre-calculate 1.2GB per action\n",
        "# f_mean = np.nanmean(np_train,axis=0)\n",
        "# create f_mean.npy\n",
        "# np.save(os.path.join(data_home,\"f_mean\"), f_mean)\n",
        "\n",
        "f_mean_path = os.path.join(data_home, \"f_mean.npy\")\n",
        "f_mean = np.load(f_mean_path)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5kp4PuMnaWA",
        "outputId": "ac0e04d9-13b8-4e77-8e28-08b9597e4a9b"
      },
      "source": [
        "print('fillna_npwhere_njit (mean-filling):')\n",
        "np_mf_train = for_loop(fillna_npwhere_njit, np_train, f_mean)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fillna_npwhere_njit (mean-filling):\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "omNSA1JanaWA"
      },
      "source": [
        "np_train = np.concatenate([np_train_ft0.reshape(-1,1),np_mf_train],axis=1)\n",
        "# resp_{1~4}, resp 모두를 고려; 각각을 0과 1로 분류하는 개별적인 Binary Classification 문제로 간주\n",
        "# ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp'] 순서\n",
        "np_targets = np.stack([(df_labels[c] > 0).astype('int') for c in resps]).T"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_JKiwv1pnaWA"
      },
      "source": [
        "class JaneDataset(Dataset):\n",
        "    def __init__(self, np_X, np_y):\n",
        "        super(JaneDataset,self).__init__()\n",
        "        self.X = np_X\n",
        "        self.y = np_y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        X = torch.tensor(self.X[index,:],dtype=torch.float)\n",
        "        y = torch.tensor(self.y[index],dtype=torch.float)\n",
        "        return X,y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "96fx1qJCnaWA"
      },
      "source": [
        "dataset = JaneDataset(np_train, np_targets)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tVfiS1xLnaWB"
      },
      "source": [
        "train_size = int(len(dataset) * 0.8)\n",
        "valid_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(entire_seed))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyoYSPQDnaWB"
      },
      "source": [
        "## CNN(Convolutional Neural Network)\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"imgs/cnn-arch.png\">\n",
        "<p>Basic CNN architecture for image classification</p>\n",
        "</div>\n",
        "\n",
        "### 1. 합성곱층\n",
        "\n",
        "일반적으로 합성곱은 그림과 같이 연속적인 은닉층을 거쳐 특징을 조합하고 압축한다. 첫번째 은닉층에서는 작은 저수준 특성에 집중하고, 다음 은닉층에는 더 큰 고수준 톡성으로 조합해나가도록 한다. 이런 계층적 구조가 CNN이 이미지 인식에 잘 작동하는 이유 중에 하나이다.\n",
        "\n",
        "합성곱층은 2차원 이미지를 기준으로 hxw크기를 가진 수용장(kernel)을 임의의 간격 stride를 두어 특성맵(feature map)을 생성한다. feature map을 생성함에 있어 stride값을 크게 주어 차원을 축소한 특성맵을 얻을 수도 있다. \n",
        "\n",
        "이러한 수용장(filter)를 이동하여 얻은 특성맵은 output filter로 정의한 개수만큼 filter를 가지고, 각각의 feature map은 2차원의 데이터이기 때문에 실질적으로 합성곱층을 거친 데이터는 3차원의 데이터로 표현된다. \n",
        "\n",
        "하나의 특성맵안에서는 모든 뉴런이 같은 파라미터를 공유하고, 다른 특성맵 간에는 다른 파라미터를 사용한다. 다시 말해 하나의 합성곱층이 입력에 여러 필터를 동시에 적용하여 입력에 있는 여러 특성을 감지하는 것이다.\n",
        "\n",
        "### 2. 풀링층\n",
        "\n",
        "풀링층의 목적은 계산량과 메모리 사용량 그리고 파라미터의 수를 줄이기 위해 부표본(축소본)을 만드는 것이다. 합성곱이 특정 feature map의 하나의 포인트(뉴런)를 얻는 것과 같이 이전 layer의 수용장에 있는 출력값과 연결되어 있다. 그러나 풀링 뉴런은 가중치가 없으므로, 합산함수를 사용하여 입력값을 더하는 것이 전부이다.\n",
        "\n",
        "* 최대 풀링층\n",
        "* 평균 풀링층\n",
        "* 전역 평균 풀링층: 각 특성 맵의 평균을 계산함으로써 특성 맵의 대부부분의 정보를 잃게되지만 출력층에는 유용하다.\n",
        "\n",
        "### 3. CNN의 기본 구조\n",
        "\n",
        "1. 합성곱층을 몇개 쌓고(각각에 활성화함수추가)\n",
        "2. 풀링층\n",
        "3. 추가적인 연속적인 합성곱층\n",
        "4. 풀링층\n",
        "\n",
        "이를 통해 네트워크를 거칠 수록 이미지는 점점 작아지지만, 합성곱 층 때문에 일반적으로 점점 더 깊어진다(많은 특성 맵을 가진다). \n",
        "\n",
        "첫번째 층은 필터가 적더라도 그 크기를 크게 잡는다. (kernel_size)\n",
        "\n",
        "이후의 층은 필터의 수를 점차적으로 늘이고, 그에 상응하여 크기를 줄인다.\n",
        "\n",
        "* 1d-cnn\n",
        "\n",
        "https://wikidocs.net/80437\n",
        "\n",
        "https://ratsgo.github.io/deep%20learning/2017/10/09/CNNs/\n",
        "\n",
        "https://medium.com/@Rehan_Sayyad/how-to-use-convolutional-neural-networks-for-time-series-classification-80575131a474\n",
        "\n",
        "https://arxiv.org/abs/1905.03554\n",
        "\n",
        "https://www.kaggle.com/pyoungkangkim/1dcnn-pytorch-jstreet\n",
        "\n",
        "https://www.kaggle.com/a763337092/pytorch-resnet-starter-training\n",
        "\n",
        "https://pulsar-kkaturi.tistory.com/entry/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-%EC%B2%AB%EA%B1%B8%EC%9D%8C-5-%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "It3N1TH-naWB"
      },
      "source": [
        "# class Model_1DCNN(nn.Module):\n",
        "#     def __init__(self, num_features, num_targets, hidden_size):\n",
        "#         super(Model_1DCNN, self).__init__()\n",
        "        \n",
        "#         self.hidden_size = hidden_size\n",
        "#         # num_channel\n",
        "#         self.ch_input = 16\n",
        "#         self.ch_output =  32\n",
        "#         self.points = int(self.hidden_size / self.ch_input) \n",
        "        \n",
        "#         # feature_size to hidden_size\n",
        "#         self.bn_dense1 = nn.BatchNorm1d(num_features)\n",
        "#         self.dropout_dense1 = nn.Dropout(0.2)\n",
        "#         self.dense1 = nn.Linear(num_features, hidden_size)\n",
        "        \n",
        "#         # reshaped hidden_size [input_channel, data] to [input_channel, output_channel] \n",
        "#         self.bn_c1 = nn.BatchNorm1d(self.ch_output)\n",
        "#         self.dropout_c1 = nn.Dropout(0.2)\n",
        "#         self.conv1 = nn.Conv1d(self.ch_input, self.ch_output, kernel_size=5, padding=2, stride=2)\n",
        "#         self.max_pool_c1 = nn.MaxPool1d(kernel_size=2)\n",
        "        \n",
        "#         self.bn_c2 = nn.BatchNorm1d(self.ch_output*2)\n",
        "#         self.dropout_c2 = nn.Dropout(0.2)\n",
        "#         self.conv2 = nn.Conv1d(self.ch_output, self.ch_output*2, kernel_size=3, padding=1, stride=1)\n",
        "        \n",
        "#         self.bn_c2_1 = nn.BatchNorm1d(self.ch_output*2)\n",
        "#         self.dropout_c2_1 = nn.Dropout(0.2)\n",
        "#         self.conv2_1 = nn.Conv1d(self.ch_output*2, self.ch_output*2, kernel_size=3, padding=1, stride=1)\n",
        "        \n",
        "#         self.max_pool_c2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "#         self.flatten = nn.Flatten()\n",
        "        \n",
        "#         self.bn_dense2 = nn.BatchNorm1d(256)\n",
        "#         self.dropout_dense2 = nn.Dropout(0.2)\n",
        "#         self.dense2 = nn.Linear(512,256)\n",
        "\n",
        "#         self.bn_dense3 = nn.BatchNorm1d(num_targets)\n",
        "#         self.dropout_dense3 = nn.Dropout(0.2)\n",
        "#         self.dense3 = nn.Linear(256,num_targets)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.bn_dense1(x)\n",
        "#         x = self.dropout_dense1(x)\n",
        "#         x = self.dense1(x)\n",
        "        \n",
        "#         x = x.reshape(x.size(0), self.ch_input, self.points)\n",
        "#         # print(x.shape)\n",
        "\n",
        "#         x = self.conv1(x)\n",
        "#         x = F.relu(self.bn_c1(x))\n",
        "#         # print(x.shape)\n",
        "#         x = self.dropout_c1(x)\n",
        "#         x = self.max_pool_c1(x)\n",
        "#         # print(x.shape)\n",
        "        \n",
        "#         x = self.conv2(x)\n",
        "#         x = F.relu(self.bn_c2(x))\n",
        "#         x = self.dropout_c2(x)\n",
        "#         # print(x.shape)\n",
        "        \n",
        "#         x = self.conv2_1(x)\n",
        "#         x = F.relu(self.bn_c2_1(x))\n",
        "#         x = self.dropout_c2_1(x)\n",
        "#         # print(x.shape)\n",
        "\n",
        "#         x = self.max_pool_c2(x)\n",
        "#         # print(x.shape)\n",
        "\n",
        "#         x = self.flatten(x)\n",
        "#         # print(x.shape)\n",
        "\n",
        "#         x = self.dense2(x)\n",
        "#         x = self.bn_dense2(x)\n",
        "#         x = self.dropout_dense2(x)\n",
        "\n",
        "#         x = self.dense3(x)\n",
        "#         x = self.bn_dense3(x)\n",
        "#         x = self.dropout_dense3(x)\n",
        "        \n",
        "#         x = F.sigmoid(x)\n",
        "\n",
        "#         return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z6zv8ssD22e"
      },
      "source": [
        "# class Model_1DCNN(nn.Module):\r\n",
        "#     def __init__(self, num_features, num_targets, hidden_size):\r\n",
        "#         super(Model_1DCNN, self).__init__()\r\n",
        "        \r\n",
        "#         self.hidden_size = hidden_size\r\n",
        "#         # num_channel\r\n",
        "#         self.ch_input = 16\r\n",
        "#         self.ch_output =  32\r\n",
        "#         self.points = int(self.hidden_size / self.ch_input) \r\n",
        "        \r\n",
        "#         # feature_size to hidden_size\r\n",
        "#         self.bn_dense1 = nn.BatchNorm1d(num_features)\r\n",
        "#         self.dropout_dense1 = nn.Dropout(0.2)\r\n",
        "#         self.dense1 = nn.Linear(num_features, hidden_size)\r\n",
        "        \r\n",
        "#         # reshaped hidden_size [input_channel, data] to [input_channel, output_channel]\r\n",
        "#         self.c1 = self.make_layers(self.ch_input,self.ch_output, 5, 2, 2, 0.2)\r\n",
        "#         self.max_pool_c1 = nn.MaxPool1d(kernel_size=2)\r\n",
        "\r\n",
        "#         self.c2 = self.make_layers(self.ch_output, self.ch_output*2, 3, 2, 1, 0.2)\r\n",
        "#         self.c2_1 = self.make_layers(self.ch_output*2, self.ch_output*2, 3, 1, 1, 0.2)\r\n",
        "#         self.c2_2 = self.make_layers(self.ch_output*2, self.ch_output*2, 3, 1, 1, 0.2)\r\n",
        "\r\n",
        "#         self.max_pool_c2 = nn.MaxPool1d(kernel_size=2)\r\n",
        "\r\n",
        "#         self.flatten = nn.Flatten()\r\n",
        "        \r\n",
        "#         self.bn_dense2 = nn.BatchNorm1d(64)\r\n",
        "#         self.dropout_dense2 = nn.Dropout(0.2)\r\n",
        "#         self.dense2 = nn.Linear(128,64)\r\n",
        "\r\n",
        "#         self.bn_dense3 = nn.BatchNorm1d(num_targets)\r\n",
        "#         self.dropout_dense3 = nn.Dropout(0.2)\r\n",
        "#         self.dense3 = nn.Linear(64,num_targets)\r\n",
        "\r\n",
        "#     def make_layers(self, ch_in, ch_out, kernel_size, stride, padding, dropout):\r\n",
        "#         cnn_module = nn.Sequential(\r\n",
        "#                         nn.Conv1d(ch_in,ch_out, kernel_size, stride, padding),\r\n",
        "#                         nn.BatchNorm1d(ch_out),\r\n",
        "#                         nn.ReLU(),\r\n",
        "#                         nn.Dropout(dropout)\r\n",
        "#                     )\r\n",
        "        \r\n",
        "#         return cnn_module\r\n",
        "\r\n",
        "#     def forward(self, x):\r\n",
        "#         x = self.bn_dense1(x)\r\n",
        "#         x = self.dropout_dense1(x)\r\n",
        "#         x = self.dense1(x)\r\n",
        "        \r\n",
        "#         x = x.reshape(x.size(0), self.ch_input, self.points)\r\n",
        "#         print(x.shape)\r\n",
        "\r\n",
        "#         x = self.c1(x)\r\n",
        "#         print(x.shape)\r\n",
        "#         x = self.max_pool_c1(x)\r\n",
        "#         print(x.shape)\r\n",
        "        \r\n",
        "#         x = self.c2(x)\r\n",
        "#         print(x.shape)\r\n",
        "#         x = self.c2_1(x)\r\n",
        "#         print(x.shape)\r\n",
        "#         x = self.c2_2(x)\r\n",
        "#         print(x.shape)\r\n",
        "#         x = self.max_pool_c2(x)\r\n",
        "#         print(x.shape)\r\n",
        "\r\n",
        "#         x = self.flatten(x)\r\n",
        "#         print(x.shape)\r\n",
        "\r\n",
        "#         x = self.dense2(x)\r\n",
        "#         x = self.bn_dense2(x)\r\n",
        "#         x = self.dropout_dense2(x)\r\n",
        "\r\n",
        "#         x = self.dense3(x)\r\n",
        "#         x = self.bn_dense3(x)\r\n",
        "#         x = self.dropout_dense3(x)\r\n",
        "        \r\n",
        "#         x = F.sigmoid(x)\r\n",
        "\r\n",
        "#         return x"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7fpbggnM5IU"
      },
      "source": [
        "# class Model_1DCNN(nn.Module):\r\n",
        "#     def __init__(self, num_features, num_targets, hidden_size):\r\n",
        "#         super(Model_1DCNN, self).__init__()\r\n",
        "        \r\n",
        "#         self.hidden_size = hidden_size\r\n",
        "#         # num_channel\r\n",
        "#         self.ch_input = 16\r\n",
        "#         self.ch_output =  32\r\n",
        "#         self.points = int(self.hidden_size / self.ch_input) \r\n",
        "        \r\n",
        "#         # feature_size to hidden_size\r\n",
        "#         self.bn_dense1 = nn.BatchNorm1d(num_features)\r\n",
        "#         self.dropout_dense1 = nn.Dropout(0.2)\r\n",
        "#         self.dense1 = nn.Linear(num_features, hidden_size)\r\n",
        "        \r\n",
        "#         # reshaped hidden_size [input_channel, data] to [input_channel, output_channel]\r\n",
        "#         # self.c1 = self.make_layers(self.ch_input,self.ch_output, 5, 2, 2, 0.2)\r\n",
        "#         # self.max_pool_c1 = nn.MaxPool1d(kernel_size=2)\r\n",
        "\r\n",
        "#         # self.c2 = self.make_layers(self.ch_output, self.ch_output*2, 3, 2, 1, 0.2)\r\n",
        "#         # self.c2_1 = self.make_layers(self.ch_output*2, self.ch_output*2, 3, 1, 1, 0.2)\r\n",
        "#         # self.c2_2 = self.make_layers(self.ch_output*2, self.ch_output*2, 3, 1, 1, 0.2)\r\n",
        "\r\n",
        "#         # self.max_pool_c2 = nn.MaxPool1d(kernel_size=2)\r\n",
        "\r\n",
        "#         # self.flatten = nn.Flatten()\r\n",
        "        \r\n",
        "#         # self.bn_dense2 = nn.BatchNorm1d(64)\r\n",
        "#         # self.dropout_dense2 = nn.Dropout(0.2)\r\n",
        "#         # self.dense2 = nn.Linear(128,64)\r\n",
        "\r\n",
        "#         # self.bn_dense3 = nn.BatchNorm1d(num_targets)\r\n",
        "#         # self.dropout_dense3 = nn.Dropout(0.2)\r\n",
        "#         # self.dense3 = nn.Linear(64,num_targets)\r\n",
        "\r\n",
        "#         self.c1 = self.make_layers(self.ch_input,self.ch_output, 5,2,2,0.2)\r\n",
        "\r\n",
        "#         self.c2 = self.make_layers(self.ch_input,self.ch_output, 3,2,1,0.2)\r\n",
        "#         self.c2_1 = self.make_layers(self.ch_input,self.ch_output, 3,1,1,0.2)\r\n",
        "\r\n",
        "#     def make_layers(self, ch_in, ch_out, kernel_size, stride, padding, dropout):\r\n",
        "#         cnn_module = nn.Sequential(\r\n",
        "#                         nn.Conv1d(ch_in,ch_out, kernel_size, stride, padding),\r\n",
        "#                         nn.BatchNorm1d(ch_out),\r\n",
        "#                         nn.ReLU(),\r\n",
        "#                         nn.Dropout(dropout),\r\n",
        "#                         nn.Conv1d(ch_out, ch_out, 3, 1, 1),\r\n",
        "#                         nn.BatchNorm1d(ch_out)\r\n",
        "#                     )\r\n",
        "        \r\n",
        "#         return cnn_module\r\n",
        "\r\n",
        "#     def forward(self, x):\r\n",
        "#         x = self.bn_dense1(x)\r\n",
        "#         x = self.dropout_dense1(x)\r\n",
        "#         x = self.dense1(x)\r\n",
        "        \r\n",
        "#         x = x.reshape(x.size(0), self.ch_input, self.points)\r\n",
        "#         shortcut = x\r\n",
        "\r\n",
        "#         x = self.c1(x)\r\n",
        "#         bat = nn.BatchNorm1d(32)\r\n",
        "#         cut = nn.Conv1d(16,32,1,2)\r\n",
        "\r\n",
        "#         s_x = bat(cut(shortcut))\r\n",
        "#         print(s_x.shape)\r\n",
        "#         print(x.shape)\r\n",
        "\r\n",
        "#         x += s_x\r\n",
        "#         print(x.shape)\r\n",
        "#         # x = self.c2(x)\r\n",
        "\r\n",
        "#         # x = self.c2_1(x)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#         # x = self.max_pool_c1(x)\r\n",
        "#         # print(x.shape)\r\n",
        "        \r\n",
        "#         # x = self.c2(x)\r\n",
        "#         # print(x.shape)\r\n",
        "#         # x = self.c2_1(x)\r\n",
        "#         # print(x.shape)\r\n",
        "#         # x = self.c2_2(x)\r\n",
        "#         # print(x.shape)\r\n",
        "#         # x = self.max_pool_c2(x)\r\n",
        "#         # print(x.shape)\r\n",
        "\r\n",
        "#         # x = self.flatten(x)\r\n",
        "#         # print(x.shape)\r\n",
        "\r\n",
        "#         # x = self.dense2(x)\r\n",
        "#         # x = self.bn_dense2(x)\r\n",
        "#         # x = self.dropout_dense2(x)\r\n",
        "\r\n",
        "#         # x = self.dense3(x)\r\n",
        "#         # x = self.bn_dense3(x)\r\n",
        "#         # x = self.dropout_dense3(x)\r\n",
        "        \r\n",
        "#         # x = F.sigmoid(x)\r\n",
        "\r\n",
        "#         return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPRSPAr8QO_O"
      },
      "source": [
        "class ResidualBlock(nn.Module):\r\n",
        "    expansion: int=1\r\n",
        "    def __init__(self, inplane, plane, stride=1, dilation=1, dropout=0.2, downsample=None):\r\n",
        "        super(ResidualBlock,self).__init__()\r\n",
        "        \r\n",
        "        self.conv1 = self.conv3x3(inplane, plane, stride, dilation)\r\n",
        "        self.bn1 = nn.BatchNorm1d(plane)\r\n",
        "        self.dropout1 = nn.Dropout(dropout)\r\n",
        "        self.conv2 = self.conv3x3(plane, plane)\r\n",
        "        self.bn2 = nn.BatchNorm1d(plane)\r\n",
        "        \r\n",
        "        # Inplace means in Activation Func\r\n",
        "        # https://discuss.pytorch.org/t/whats-the-difference-between-nn-relu-and-nn-relu-inplace-true/948\r\n",
        "        self.relu = nn.ReLU(inplace=False)\r\n",
        "        \r\n",
        "        self.downsample = downsample\r\n",
        "        \r\n",
        "    def conv3x3(self, in_planes, out_planes, stride=1, dilation=1):\r\n",
        "        return nn.Conv1d(in_planes, out_planes, 3, stride, padding=dilation, bias=False)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        shortcut = x\r\n",
        "        \r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.dropout1(out)\r\n",
        "        \r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "        \r\n",
        "        if self.downsample is not None:\r\n",
        "            shortcut = self.downsample(x)\r\n",
        "            \r\n",
        "        out += shortcut\r\n",
        "        out += self.relu(out)\r\n",
        "        \r\n",
        "        return out"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb5ZAlNsQQnC"
      },
      "source": [
        "from typing import Type, Any, Callable, Union, List, Optional\r\n",
        "\r\n",
        "class CustomResNet(nn.Module):\r\n",
        "    def __init__(self, block: Type[ResidualBlock], layers: List[int], dropout=0.2, num_feature=130, hidden_layers=[512,256], num_classes: int = 5):\r\n",
        "        super(CustomResNet, self).__init__()\r\n",
        "        self.inplanes = 64\r\n",
        "        self.block = block\r\n",
        "        self.dropout = dropout\r\n",
        "        self.num_feature = num_feature\r\n",
        "        self.h1, self.h2 = hidden_layers\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.reshaped_dim = int(self.h1 / self.inplanes)\r\n",
        "\r\n",
        "        self.relu = nn.ReLU(inplace=False)\r\n",
        "        \r\n",
        "        self.bn_d0 = nn.BatchNorm1d(self.num_feature)\r\n",
        "        self.dropout_d0 = nn.Dropout(self.dropout)\r\n",
        "        \r\n",
        "        self.dense1 = nn.Linear(self.num_feature, self.h1)\r\n",
        "        self.bn_d1 = nn.BatchNorm1d(self.h1)\r\n",
        "        self.dropout_d1 = nn.Dropout(self.dropout)\r\n",
        "\r\n",
        "        self.layer1 = self.make_layers(self.block,64, layers[0], stride=1)\r\n",
        "        self.layer2 = self.make_layers(self.block,128,layers[1], stride=2)\r\n",
        "        self.layer3 = self.make_layers(self.block,256,layers[2], stride=2)\r\n",
        "        self.layer4 = self.make_layers(self.block,512,layers[3], stride=2)\r\n",
        " \r\n",
        "        self.avgpool = nn.AvgPool1d(2)\r\n",
        "        self.flt = nn.Flatten()\r\n",
        "\r\n",
        "        self.dense2 = nn.Linear(int(self.h1/2), self.h2)\r\n",
        "        self.bn_d2 = nn.BatchNorm1d(self.h2)\r\n",
        "        self.dropout_d2 = nn.Dropout(self.dropout)\r\n",
        "    \r\n",
        "        self.dense3 = nn.Linear(self.h2, self.num_classes)\r\n",
        "        \r\n",
        "\r\n",
        "    def make_layers(self, block, planes, layer, stride=1):\r\n",
        "        downsample = None\r\n",
        "        \r\n",
        "        if stride > 1:\r\n",
        "            downsample = nn.Sequential(\r\n",
        "                self.conv1x1(self.inplanes, planes * block.expansion, stride),\r\n",
        "                nn.BatchNorm1d(planes * block.expansion),\r\n",
        "            )\r\n",
        "            \r\n",
        "        layers = []\r\n",
        "        layers.append(block(self.inplanes, planes, stride, dilation=1,dropout=self.dropout, downsample=downsample))\r\n",
        "        self.inplanes = planes * block.expansion\r\n",
        "\r\n",
        "        for i in range(layer-1):\r\n",
        "            layers.append(block(self.inplanes, planes))\r\n",
        "\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "            \r\n",
        " \r\n",
        "    def conv1x1(self, in_planes, out_planes, stride=1):\r\n",
        "        return nn.Conv1d(in_planes, out_planes, 1, stride=stride, bias=False)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        # 130\r\n",
        "        x = self.bn_d0(x)\r\n",
        "        x = self.dropout_d0(x)\r\n",
        "        \r\n",
        "        # 1024\r\n",
        "        x = self.dense1(x)\r\n",
        "        x = self.bn_d1(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.dropout_d1(x)\r\n",
        "        \r\n",
        "        # 1024\r\n",
        "        print(x.shape)\r\n",
        "        x = x.reshape(x.size(0), 64, self.reshaped_dim)\r\n",
        "        # 64, 16\r\n",
        "        print(x.shape)\r\n",
        "        x = self.layer1(x)\r\n",
        "        # 64, 16\r\n",
        "        print(x.shape)\r\n",
        "        x = self.layer2(x)\r\n",
        "        # 128, 8\r\n",
        "        print(x.shape)\r\n",
        "        x = self.layer3(x)\r\n",
        "        # 256, 4\r\n",
        "        print(x.shape)\r\n",
        "        x = self.layer4(x)\r\n",
        "        # 512, 2\r\n",
        "        print(x.shape)\r\n",
        "\r\n",
        "\r\n",
        "        x = self.avgpool(x)\r\n",
        "        print(x.shape)\r\n",
        "        x = self.flt(x)\r\n",
        "        print(x.shape)\r\n",
        "        \r\n",
        "        x = self.dense2(x)\r\n",
        "        x = self.bn_d2(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.dropout_d2(x)\r\n",
        "\r\n",
        "        x = self.dense3(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "        "
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDIm-YkQQhix"
      },
      "source": [
        "block = ResidualBlock\r\n",
        "# model = CustomResNet(block=block, layers=[5,5,5,5], hidden_layers=[1024,256])"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGY1zLk9ZfQa"
      },
      "source": [
        "# model"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56fgdsVJhVw5"
      },
      "source": [
        "https://dnddnjs.github.io/cifar10/2018/10/09/resnet/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kfJcmES4naWC"
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 4096\n",
        "learning_rate = 0.001\n",
        "\n",
        "# model = ResNet(num_layers=[1,3,5], kernel_sizes=[(5,5),(3,3),(3,3)], strides=[2,2,2], paddings=[4,1,1], block=block, num_features=130, hidden_size=512, num_targets=5)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-JECNElhnaWC"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) \n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3a8JrIkVnaWC"
      },
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWah5LQJ3oRE"
      },
      "source": [
        "class EarlyStopping:\r\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\r\n",
        "        self.patience = patience\r\n",
        "        self.counter = 0\r\n",
        "        self.mode = mode\r\n",
        "        self.best_score = None\r\n",
        "        self.early_stop = False\r\n",
        "        self.delta = delta\r\n",
        "        if self.mode == \"min\":\r\n",
        "            self.val_score = np.Inf\r\n",
        "        else:\r\n",
        "            self.val_score = -np.Inf\r\n",
        "\r\n",
        "    def __call__(self, epoch_score, model, model_path):\r\n",
        "\r\n",
        "        if self.mode == \"min\":\r\n",
        "            score = -1.0 * epoch_score\r\n",
        "        else:\r\n",
        "            score = np.copy(epoch_score)\r\n",
        "\r\n",
        "        if self.best_score is None:\r\n",
        "            self.best_score = score\r\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\r\n",
        "        elif score < self.best_score: #  + self.delta\r\n",
        "            self.counter += 1\r\n",
        "            # print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\r\n",
        "            if self.counter >= self.patience:\r\n",
        "                self.early_stop = True\r\n",
        "        else:\r\n",
        "            self.best_score = score\r\n",
        "            # ema.apply_shadow()\r\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\r\n",
        "            # ema.restore()\r\n",
        "            self.counter = 0\r\n",
        "\r\n",
        "    def save_checkpoint(self, epoch_score, model, model_path):\r\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\r\n",
        "            print(f\"Validation score improved ({self.val_score:.4f} --> {epoch_score:.4f}). Saving model!\")\r\n",
        "            # if not DEBUG:\r\n",
        "            torch.save(model.state_dict(), model_path)\r\n",
        "        self.val_score = epoch_score"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768,
          "referenced_widgets": [
            "bdb3abc2a4934f18a08922eb67eae42c",
            "c4b329c767c344519dbf1a97eadd8c3e",
            "52714ab9312b431b8da8d2faa88ecdb2",
            "5676634bf08144538dc05a0d0bdd07e7",
            "03745e8ba8ce41be9b1d2426b287bfd8",
            "11d3af8a27434cff820d34ba79d2aeb4",
            "b89a40d7afe546e7ab19250121fcec93",
            "da4ec0f7c86a44ff88d69204049e0efa"
          ]
        },
        "id": "1lCkzFX_naWC",
        "outputId": "46d2c3e9-2e2b-48c9-9f9b-5f61beee97dd"
      },
      "source": [
        "NFOLDS = 5\n",
        "EARLYSTOP_NUM = 7\n",
        "CACHE_PATH = model_home\n",
        "\n",
        "for _fold in range(NFOLDS):\n",
        "    print(f'Fold{_fold}:')\n",
        "    seed_torch(seed=entire_seed+_fold)\n",
        "    torch.cuda.empty_cache()\n",
        "    model = CustomResNet(block=block, layers=[5,5,5,5], hidden_layers=[1024,256])\n",
        "    model = model.to(device)\n",
        "\n",
        "    es = EarlyStopping(EARLYSTOP_NUM, mode=\"max\")\n",
        "    for epoch in tqdm_notebook(range(epochs)):\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        running_auc = 0.0\n",
        "        model.train()\n",
        "        \n",
        "        for idx, (inputs, labels) in enumerate(train_dataloader):\n",
        "        \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            true = labels.detach().cpu().numpy()[:,-1]\n",
        "            target = np.array(list(map(lambda x: 1 if x > 0.5 else 0, outputs.detach().cpu().numpy()[:,-1])),dtype=np.float)\n",
        "            \n",
        "            acc = (true == target).sum() / outputs.shape[0]\n",
        "            auc = roc_auc_score(true, outputs.detach().cpu().numpy()[:,-1])\n",
        "    \n",
        "            running_acc += acc\n",
        "            running_auc += auc\n",
        "\n",
        "            loss = criterion(outputs,labels)\n",
        "            print(outputs)\n",
        "            print(loss)\n",
        "            running_loss += loss.detach().item() * inputs.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "        epoch_acc = running_acc / len(train_dataloader)\n",
        "        epoch_auc = running_auc / len(train_dataloader)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            running_auc = 0.0\n",
        "            for idx, (inputs, labels) in enumerate(valid_dataloader):\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                true = labels.detach().cpu().numpy()[:,-1]\n",
        "                target = np.array(list(map(lambda x: 1 if x > 0.5 else 0, outputs.detach().cpu().numpy()[:,-1])),dtype=np.float)\n",
        "                \n",
        "                acc = (true == target).sum() / outputs.shape[0]\n",
        "                auc = roc_auc_score(true, outputs.detach().cpu().numpy()[:,-1])\n",
        "\n",
        "                running_acc += acc\n",
        "                running_auc += auc\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.detach().item() * inputs.size(0)\n",
        "                \n",
        "            valid_loss = running_loss / len(valid_dataloader.dataset)\n",
        "            valid_acc = running_acc / len(valid_dataloader)\n",
        "            valid_auc = running_auc / len(valid_dataloader)\n",
        "\n",
        "        print(f\"EPOCH:{epoch+1}|{epochs}; loss(train/valid):{epoch_loss:.4f}/{valid_loss:.4f}; acc(train/valid):{epoch_acc:.4f}/{valid_acc:.4f}; auc(train/valid):{epoch_auc:.4f}/{valid_auc:.4f}\")\n",
        "        \n",
        "        model_weights = os.path.join(model_home,f\"online_model_{_fold}.pth\")\n",
        "        es(valid_auc, model, model_path=model_weights)\n",
        "        if es.early_stop:\n",
        "          print(\"Early stopping\")\n",
        "          break"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold0:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdb3abc2a4934f18a08922eb67eae42c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4096, 1024])\n",
            "torch.Size([4096, 64, 16])\n",
            "torch.Size([4096, 64, 16])\n",
            "torch.Size([4096, 128, 8])\n",
            "torch.Size([4096, 256, 4])\n",
            "torch.Size([4096, 512, 2])\n",
            "torch.Size([4096, 512, 1])\n",
            "torch.Size([4096, 512])\n",
            "tensor([[-0.1138, -0.0725, -0.4713, -0.3764, -0.3333],\n",
            "        [ 0.0156, -0.3070, -0.3076, -0.1792, -0.2712],\n",
            "        [-0.3998,  0.2552, -0.2037, -0.1469,  0.3872],\n",
            "        ...,\n",
            "        [-0.8133,  0.6118, -0.6493,  0.3717, -0.0791],\n",
            "        [-0.4607,  0.1901, -0.1577, -0.0470, -0.3716],\n",
            "        [-0.2299, -0.1175, -0.2071, -0.2636,  0.3940]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor(0.7205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-7d37b510fbbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [4096, 512, 2]], which is output 0 of AddBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Pj-ubF0enaWC"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import janestreet\n",
        "env = janestreet.make_env()\n",
        "\n",
        "# learn.model.eval()\n",
        "preds = []\n",
        "for (test_df, pred_df) in tqdm_notebook(env.iter_test()):\n",
        "    if test_df['weight'].item() > 0:\n",
        "        test_np = test_df.loc[:, features].values\n",
        "        test_np[:, 1:] = for_loop(fillna_npwhere_njit, test_np[:, 1:], f_mean)\n",
        "        pred = torch.mean(model(torch.tensor(test_np, dtype=torch.float).cuda(device))).item()\n",
        "        preds.append(pred)\n",
        "        action = 1 if pred >= .5 else 0\n",
        "        pred_df.action = action\n",
        "    else:\n",
        "        pred_df.action = 0\n",
        "    env.predict(pred_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IDwfbrhunaWD"
      },
      "source": [
        "preds = np.array(preds)\n",
        "preds.mean(), preds.std(), sum(preds >= .5), sum(preds < 5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}