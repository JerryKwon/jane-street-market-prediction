{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "combine-pct-labs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axwQxgUQkq1O",
        "outputId": "4715a50b-8f6c-4d3d-9fb9-58a595560ecf"
      },
      "source": [
        "\"\"\"Colab Drive Connection\"\"\"\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvBKWfYilB0q",
        "outputId": "5da7a3fc-13a1-4742-8aa4-f81f541ba97e"
      },
      "source": [
        "!rm /etc/localtime\r\n",
        "!ln -s /usr/share/zoneinfo/Asia/Seoul /etc/localtime\r\n",
        "!date"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 22 21:00:52 KST 2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnt4c3mflBwd",
        "outputId": "93e667c1-58cf-4bcd-cc4a-02991b52f5dc"
      },
      "source": [
        "import warnings\r\n",
        "import os \r\n",
        "\r\n",
        "from collections import defaultdict\r\n",
        "from copy import deepcopy\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.gridspec as gridspec\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "import networkx as nx\r\n",
        "import scipy.cluster.hierarchy as sch\r\n",
        "from scipy.cluster.hierarchy import fcluster\r\n",
        "\r\n",
        "import random\r\n",
        "\r\n",
        "from tqdm import tqdm_notebook\r\n",
        "\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "# install datatable\r\n",
        "!pip install datatable\r\n",
        "import datatable as dt\r\n",
        "\r\n",
        "from numba import njit\r\n",
        "\r\n",
        "import gc\r\n",
        "\r\n",
        "warnings.simplefilter(action=\"ignore\")\r\n",
        "\r\n",
        "# project_home = \"/kaggle/input/jane-street-market-prediction\"\r\n",
        "# model_home = \"/kaggle/working\"\r\n",
        "# data_home = project_home\r\n",
        "\r\n",
        "project_home = \"/gdrive/MyDrive/colab/jane-street-market-prediction\"\r\n",
        "data_home = os.path.join(project_home, \"input/data\")\r\n",
        "model_home = os.path.join(project_home, \"output/model\")\r\n",
        "gs_home = os.path.join(project_home, 'output/grid_search')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datatable\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/cb/21810c43b687a19d194c372192049f535fba28c55ce76d37e7e407159c52/datatable-0.11.1-cp36-cp36m-manylinux2010_x86_64.whl (83.7MB)\n",
            "\u001b[K     |████████████████████████████████| 83.7MB 64kB/s \n",
            "\u001b[?25hInstalling collected packages: datatable\n",
            "Successfully installed datatable-0.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj867UqXlLLZ"
      },
      "source": [
        "entire_seed = 1029\r\n",
        "\r\n",
        "def seed_torch(seed=1029):\r\n",
        "    random.seed(seed)\r\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\r\n",
        "    torch.backends.cudnn.benchmark = False\r\n",
        "#     torch.backends.cudnn.deterministic = True\r\n",
        "    torch.backends.cudnn.deterministic = False\r\n",
        "    \r\n",
        "seed_torch(entire_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbuuQw-vlNO7"
      },
      "source": [
        "train_file = os.path.join(data_home,'train.csv')\r\n",
        "features_file = os.path.join(data_home,'features.csv')\r\n",
        "example_test_file = os.path.join(data_home,'example_test.csv')\r\n",
        "example_sample_submission_file = os.path.join(data_home,'example_sample_submission.csv')\r\n",
        "\r\n",
        "train_data_datatable = dt.fread(train_file)\r\n",
        "\r\n",
        "df_train = train_data_datatable.to_pandas()\r\n",
        "df_features = pd.read_csv(features_file)\r\n",
        "df_example_test = pd.read_csv(example_test_file)\r\n",
        "df_example_sample_submission = pd.read_csv(example_sample_submission_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm17uQ3NlQfO"
      },
      "source": [
        "features = [ col for col in df_train.columns if \"feature\" in col ]\r\n",
        "resps = [ col for col in df_train.columns if \"resp\" in col ]\r\n",
        "target_resp = [resp_ for resp_ in resps if \"_\" not in resp_]\r\n",
        "target = [\"weight\"] + target_resp + features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s8J7dA3lR7o",
        "outputId": "b5e0fc0d-dabd-4a15-f892-324c4b59343b"
      },
      "source": [
        "\"\"\"\r\n",
        "Reduce Memory Usage by 75%\r\n",
        "https://www.kaggle.com/tomwarrens/nan-values-depending-on-time-of-day\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "## Reduce Memory\r\n",
        "\r\n",
        "def reduce_memory_usage(df):\r\n",
        "    \r\n",
        "    start_memory = df.memory_usage().sum() / 1024**2\r\n",
        "    print(f\"Memory usage of dataframe is {start_memory} MB\")\r\n",
        "    \r\n",
        "    for col in df.columns:\r\n",
        "        col_type = df[col].dtype\r\n",
        "        \r\n",
        "        if col_type != 'object':\r\n",
        "            c_min = df[col].min()\r\n",
        "            c_max = df[col].max()\r\n",
        "            \r\n",
        "            if str(col_type)[:3] == 'int':\r\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\r\n",
        "                    df[col] = df[col].astype(np.int8)\r\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\r\n",
        "                    df[col] = df[col].astype(np.int16)\r\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\r\n",
        "                    df[col] = df[col].astype(np.int32)\r\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\r\n",
        "                    df[col] = df[col].astype(np.int64)\r\n",
        "            \r\n",
        "            else:\r\n",
        "#                 reducing float16 for calculating numpy.nanmean\r\n",
        "#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\r\n",
        "#                     df[col] = df[col].astype(np.float16)\r\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\r\n",
        "                    df[col] = df[col].astype(np.float32)\r\n",
        "                else:\r\n",
        "                    pass\r\n",
        "        else:\r\n",
        "            df[col] = df[col].astype('category')\r\n",
        "    \r\n",
        "    end_memory = df.memory_usage().sum() / 1024**2\r\n",
        "    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\r\n",
        "    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\r\n",
        "    return df\r\n",
        "\r\n",
        "df_train = reduce_memory_usage(df_train)\r\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2489.4869804382324 MB\n",
            "Memory usage of dataframe after reduction 1247.0233011245728 MB\n",
            "Reduced by 49.908422461199 % \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2390491 entries, 0 to 2390490\n",
            "Columns: 138 entries, date to ts_id\n",
            "dtypes: float32(135), int16(1), int32(1), int8(1)\n",
            "memory usage: 1.2 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD04KYvPlTM5"
      },
      "source": [
        "# drop before 85days\r\n",
        "df_train = df_train.loc[df_train.date>85]\r\n",
        "# drop weight 0 for training\r\n",
        "df_train = df_train.loc[df_train.weight > 0]\r\n",
        "\r\n",
        "# df_labels = df_train[['date','weight','resp_1','resp_2','resp_3','resp_4','resp']]\r\n",
        "\r\n",
        "# df_train = df_train.drop(df_labels.columns,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdNbR4NdlUGo"
      },
      "source": [
        "\"\"\"\r\n",
        "The codes from 'Optimise Speed of Filling-NaN Function'\r\n",
        "https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "def for_loop(method, matrix, values):\r\n",
        "    for i in range(matrix.shape[0]):\r\n",
        "        matrix[i] = method(matrix[i], values)\r\n",
        "    return matrix\r\n",
        "\r\n",
        "def for_loop_ffill(method, matrix):\r\n",
        "    tmp = np.zeros(matrix.shape[1],dtype=np.float32)\r\n",
        "    for i in range(matrix.shape[0]):\r\n",
        "        matrix[i] = method(matrix[i], tmp)\r\n",
        "        tmp = matrix[i]\r\n",
        "    return matrix\r\n",
        "\r\n",
        "@njit\r\n",
        "def fillna_npwhere_njit(array, values):\r\n",
        "    if np.isnan(array.sum()):\r\n",
        "        array = np.where(np.isnan(array), values, array)\r\n",
        "    return array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMxjgM88lVL6",
        "outputId": "cf897a3c-f0ea-4b59-e621-78481a6f0ddc"
      },
      "source": [
        "# converting numpy for efficient calcualtion.\r\n",
        "# ft 1~129\r\n",
        "np_ft_train = df_train.loc[:,features[1:]].values\r\n",
        "np_ft_train.shape\r\n",
        "\r\n",
        "# ft 0\r\n",
        "# np_train_ft0 = df_train.loc[:,features[0]].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1571415, 129)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGN14DQMlV9N"
      },
      "source": [
        "f_mean = np.nanmean(np_ft_train,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6CE-axIlWtZ"
      },
      "source": [
        "np_train = df_train.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNIJBY6slXjD",
        "outputId": "0d66101a-9e42-4bbd-cf5d-01748193c094"
      },
      "source": [
        "print('fillna_npwhere_njit (mean-filling):')\r\n",
        "np_train[:,8:-1] = for_loop(fillna_npwhere_njit, np_train[:,8:-1], f_mean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fillna_npwhere_njit (mean-filling):\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmG1Y4jplYg3"
      },
      "source": [
        "dict_features = {col:idx for idx, col in enumerate(df_train.columns.tolist())}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf-bjG9PlZYN"
      },
      "source": [
        "np_d_w = np_train[:,:2]\r\n",
        "# ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']\r\n",
        "idx_resps = list()\r\n",
        "for resp in resps:\r\n",
        "    idx_col = dict_features[resp]\r\n",
        "    idx_resps.append(idx_col)\r\n",
        "np_resps = np_train[:,idx_resps]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuYHpyB2laKZ",
        "outputId": "d2de916d-1046-4fd5-dec3-e3052e1659eb"
      },
      "source": [
        "resps_prcntls = [50, 49, 49, 50, 50]\r\n",
        "resps_prcntls = [np.percentile(np_resps[:,idx], prcntls) for idx, prcntls in enumerate(resps_prcntls)]\r\n",
        "resps_prcntls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.3540282199974172e-05,\n",
              " -2.6968382262566605e-05,\n",
              " -6.920687970705338e-05,\n",
              " 7.239638944156468e-05,\n",
              " 4.7192643251037225e-05]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccvO-PRMlbGV"
      },
      "source": [
        "list_resps = list()\r\n",
        "for idx, resps_prcntl in enumerate(resps_prcntls):\r\n",
        "    result = list(map(lambda x: 1 if x > resps_prcntl else 0, np_resps[:,idx]))\r\n",
        "    list_resps.append(result)\r\n",
        "np_targets = np.stack(list_resps).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rao7wXSCm4iB"
      },
      "source": [
        "idx_target = [(\"resp_\" not in key) and (\"ts_\" not in key) for key in dict_features.keys()]\r\n",
        "idx_target = np.arange(np_train.shape[1])[idx_target]\r\n",
        "X_np_train = np_train[:,idx_target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDMH8WB1lbyT"
      },
      "source": [
        "X = X_np_train\r\n",
        "y = np_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTW0wxfVlqLB"
      },
      "source": [
        "from collections import Counter, defaultdict\r\n",
        "from sklearn import model_selection\r\n",
        "\r\n",
        "# ---- StratifiedGroupKFold ----\r\n",
        "class StratifiedGroupKFold(object):\r\n",
        "    \"\"\"\r\n",
        "    StratifiedGroupKFold with random shuffle with a sklearn-like structure\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, n_splits=4, shuffle=True, random_state=42):\r\n",
        "        self.n_splits = n_splits\r\n",
        "        self.shuffle = shuffle\r\n",
        "        self.random_state = random_state\r\n",
        "\r\n",
        "    def get_n_splits(self, X=None, y=None, group=None):\r\n",
        "        return self.n_splits\r\n",
        "\r\n",
        "    def split(self, X, y, group):\r\n",
        "        labels_num = np.max(y) + 1\r\n",
        "        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\r\n",
        "        y_distr = Counter()\r\n",
        "        # groups = X[group].values\r\n",
        "        groups = group\r\n",
        "        for label, g in zip(y, groups):\r\n",
        "            y_counts_per_group[g][label] += 1\r\n",
        "            y_distr[label] += 1\r\n",
        "\r\n",
        "        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\r\n",
        "        groups_per_fold = defaultdict(set)\r\n",
        "\r\n",
        "        def eval_y_counts_per_fold(y_counts, fold):\r\n",
        "            y_counts_per_fold[fold] += y_counts\r\n",
        "            std_per_label = []\r\n",
        "            for label in range(labels_num):\r\n",
        "                label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(self.n_splits)])\r\n",
        "                std_per_label.append(label_std)\r\n",
        "            y_counts_per_fold[fold] -= y_counts\r\n",
        "            return np.mean(std_per_label)\r\n",
        "        \r\n",
        "        groups_and_y_counts = list(y_counts_per_group.items())\r\n",
        "        random.Random(self.random_state).shuffle(groups_and_y_counts)\r\n",
        "\r\n",
        "        for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\r\n",
        "            best_fold = None\r\n",
        "            min_eval = None\r\n",
        "            for i in range(self.n_splits):\r\n",
        "                fold_eval = eval_y_counts_per_fold(y_counts, i)\r\n",
        "                if min_eval is None or fold_eval < min_eval:\r\n",
        "                    min_eval = fold_eval\r\n",
        "                    best_fold = i\r\n",
        "            y_counts_per_fold[best_fold] += y_counts\r\n",
        "            groups_per_fold[best_fold].add(g)\r\n",
        "\r\n",
        "        all_groups = set(groups)\r\n",
        "        for i in range(self.n_splits):\r\n",
        "            train_groups = all_groups - groups_per_fold[i]\r\n",
        "            test_groups = groups_per_fold[i]\r\n",
        "\r\n",
        "            train_idx = [i for i, g in enumerate(groups) if g in train_groups]\r\n",
        "            test_idx = [i for i, g in enumerate(groups) if g in test_groups]\r\n",
        "\r\n",
        "            yield train_idx, test_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLcVureqlrUT"
      },
      "source": [
        "cv = StratifiedGroupKFold(n_splits=10, random_state=entire_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqz2MHZ6ls0r",
        "outputId": "9d39dcfd-9879-407c-8617-65a337235ec5"
      },
      "source": [
        "cv_idxes = [ (train_idx, test_idx) for train_idx, test_idx in cv.split(X, y[:,-1], group=X[:,0])]\r\n",
        "for idx, cv_idx in enumerate(cv_idxes):\r\n",
        "    train_idx, test_idx = cv_idx\r\n",
        "    train_dates = np.unique(X[train_idx, 0]) \r\n",
        "    test_dates = np.unique(X[test_idx, 0])\r\n",
        "    print(f\"fold {idx+1}\"+\"*\"*30)\r\n",
        "    print(train_dates)\r\n",
        "    print(test_dates)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1******************************\n",
            "[ 86.  87.  89.  90.  91.  93.  95.  96.  97.  99. 100. 101. 102. 104.\n",
            " 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118.\n",
            " 119. 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132.\n",
            " 133. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147.\n",
            " 148. 149. 150. 152. 153. 154. 155. 157. 158. 159. 160. 162. 163. 164.\n",
            " 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178.\n",
            " 179. 180. 181. 182. 183. 184. 186. 187. 188. 189. 190. 191. 192. 193.\n",
            " 194. 195. 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207.\n",
            " 208. 209. 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221.\n",
            " 223. 224. 225. 226. 228. 229. 230. 231. 232. 233. 234. 235. 237. 239.\n",
            " 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252. 253. 254.\n",
            " 255. 256. 257. 258. 259. 260. 261. 262. 265. 266. 267. 268. 269. 270.\n",
            " 271. 272. 273. 274. 275. 276. 277. 278. 279. 280. 281. 282. 283. 284.\n",
            " 285. 286. 287. 288. 291. 292. 293. 295. 296. 297. 299. 300. 301. 302.\n",
            " 304. 305. 306. 307. 308. 309. 310. 311. 312. 313. 314. 315. 316. 317.\n",
            " 318. 320. 321. 322. 323. 324. 325. 326. 327. 330. 331. 332. 333. 334.\n",
            " 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 348. 349. 350.\n",
            " 351. 352. 353. 354. 355. 356. 357. 358. 359. 361. 362. 363. 364. 365.\n",
            " 366. 367. 368. 370. 371. 372. 373. 374. 375. 376. 378. 379. 381. 382.\n",
            " 383. 384. 385. 386. 387. 388. 389. 390. 391. 392. 394. 395. 396. 397.\n",
            " 398. 399. 400. 401. 402. 403. 404. 406. 408. 409. 410. 411. 412. 413.\n",
            " 414. 415. 416. 417. 418. 419. 420. 421. 422. 423. 424. 427. 428. 429.\n",
            " 430. 431. 433. 434. 435. 436. 437. 438. 439. 440. 441. 443. 444. 445.\n",
            " 446. 447. 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459.\n",
            " 460. 461. 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473.\n",
            " 474. 475. 476. 477. 478. 481. 483. 484. 485. 486. 487. 488. 489. 490.\n",
            " 491. 493. 494. 495. 496. 497. 498. 499.]\n",
            "[ 88.  92.  94.  98. 103. 134. 151. 156. 161. 185. 222. 227. 236. 238.\n",
            " 240. 263. 264. 289. 290. 294. 298. 303. 319. 328. 329. 335. 347. 360.\n",
            " 369. 377. 380. 393. 405. 407. 425. 426. 432. 442. 479. 480. 482. 492.]\n",
            "fold 2******************************\n",
            "[ 86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.  99.\n",
            " 100. 101. 102. 103. 104. 106. 107. 108. 109. 110. 111. 112. 113. 114.\n",
            " 115. 117. 118. 119. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130.\n",
            " 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144.\n",
            " 145. 146. 147. 148. 149. 151. 152. 153. 154. 155. 156. 158. 159. 161.\n",
            " 163. 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176.\n",
            " 177. 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190.\n",
            " 191. 192. 193. 194. 195. 196. 197. 198. 199. 201. 202. 203. 204. 205.\n",
            " 206. 207. 208. 210. 211. 212. 213. 214. 216. 217. 219. 220. 221. 222.\n",
            " 224. 225. 226. 227. 228. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n",
            " 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252.\n",
            " 254. 255. 256. 257. 258. 260. 261. 262. 263. 264. 265. 267. 268. 269.\n",
            " 273. 274. 275. 277. 278. 280. 281. 282. 283. 284. 285. 286. 287. 288.\n",
            " 289. 290. 291. 292. 293. 294. 295. 296. 297. 298. 299. 300. 302. 303.\n",
            " 304. 305. 306. 307. 308. 309. 310. 311. 312. 313. 314. 316. 317. 318.\n",
            " 319. 320. 321. 322. 323. 324. 325. 326. 327. 328. 329. 330. 332. 333.\n",
            " 334. 335. 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347.\n",
            " 348. 349. 350. 351. 352. 353. 355. 356. 357. 358. 359. 360. 362. 363.\n",
            " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
            " 378. 379. 380. 381. 384. 385. 386. 387. 388. 390. 391. 392. 393. 394.\n",
            " 395. 396. 398. 399. 400. 401. 403. 404. 405. 406. 407. 408. 409. 411.\n",
            " 412. 413. 414. 416. 417. 418. 419. 420. 421. 422. 424. 425. 426. 427.\n",
            " 428. 429. 430. 432. 433. 434. 435. 436. 437. 439. 440. 441. 442. 443.\n",
            " 444. 445. 446. 447. 448. 450. 451. 452. 453. 454. 455. 456. 457. 459.\n",
            " 460. 461. 462. 463. 464. 465. 466. 468. 469. 470. 471. 472. 474. 476.\n",
            " 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489. 490. 491.\n",
            " 492. 493. 494. 495. 496. 497. 498. 499.]\n",
            "[105. 116. 120. 150. 157. 160. 162. 200. 209. 215. 218. 223. 229. 253.\n",
            " 259. 266. 270. 271. 272. 276. 279. 301. 315. 331. 354. 361. 382. 383.\n",
            " 389. 397. 402. 410. 415. 423. 431. 438. 449. 458. 467. 473. 475. 477.]\n",
            "fold 3******************************\n",
            "[ 86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  98.  99. 100.\n",
            " 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 112. 113. 114. 115.\n",
            " 116. 117. 118. 119. 120. 121. 122. 123. 125. 127. 128. 129. 130. 131.\n",
            " 132. 134. 135. 136. 137. 138. 139. 140. 141. 143. 144. 145. 146. 147.\n",
            " 148. 149. 150. 151. 152. 153. 155. 156. 157. 158. 159. 160. 161. 162.\n",
            " 163. 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 176. 178. 179.\n",
            " 180. 181. 182. 183. 184. 185. 186. 189. 190. 191. 192. 193. 194. 195.\n",
            " 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 209. 210.\n",
            " 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n",
            " 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 236. 237. 238. 239.\n",
            " 240. 241. 242. 243. 245. 246. 247. 248. 250. 251. 252. 253. 254. 256.\n",
            " 259. 260. 261. 262. 263. 264. 265. 266. 267. 268. 269. 270. 271. 272.\n",
            " 273. 275. 276. 278. 279. 280. 281. 282. 283. 284. 285. 286. 288. 289.\n",
            " 290. 291. 292. 294. 295. 296. 297. 298. 299. 300. 301. 303. 304. 305.\n",
            " 306. 307. 308. 309. 310. 311. 313. 314. 315. 316. 317. 318. 319. 320.\n",
            " 321. 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 335.\n",
            " 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n",
            " 350. 351. 352. 353. 354. 356. 357. 358. 359. 360. 361. 362. 364. 365.\n",
            " 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377. 379. 380.\n",
            " 381. 382. 383. 384. 385. 386. 387. 388. 389. 391. 392. 393. 394. 395.\n",
            " 396. 397. 398. 399. 400. 401. 402. 403. 404. 405. 406. 407. 408. 410.\n",
            " 411. 412. 413. 414. 415. 417. 418. 419. 420. 421. 422. 423. 424. 425.\n",
            " 426. 427. 428. 429. 431. 432. 433. 434. 435. 436. 437. 438. 439. 441.\n",
            " 442. 443. 444. 445. 446. 447. 448. 449. 450. 451. 454. 455. 456. 457.\n",
            " 458. 459. 460. 461. 463. 464. 465. 466. 467. 468. 469. 470. 471. 473.\n",
            " 474. 475. 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 488.\n",
            " 489. 490. 491. 492. 493. 494. 495. 496. 497. 498. 499.]\n",
            "[ 97. 111. 124. 126. 133. 142. 154. 174. 175. 177. 187. 188. 208. 235.\n",
            " 244. 249. 255. 257. 258. 274. 277. 287. 293. 302. 312. 334. 355. 363.\n",
            " 378. 390. 409. 416. 430. 440. 452. 453. 462. 472. 487.]\n",
            "fold 4******************************\n",
            "[ 86.  87.  88.  89.  91.  92.  93.  94.  95.  96.  97.  98.  99. 100.\n",
            " 103. 105. 106. 107. 108. 109. 110. 111. 114. 115. 116. 117. 118. 119.\n",
            " 120. 121. 122. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133. 134.\n",
            " 136. 137. 138. 139. 140. 142. 143. 144. 145. 146. 147. 148. 149. 150.\n",
            " 151. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 166.\n",
            " 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 181.\n",
            " 182. 183. 184. 185. 186. 187. 188. 191. 192. 193. 195. 196. 197. 199.\n",
            " 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213.\n",
            " 215. 216. 217. 218. 219. 221. 222. 223. 224. 225. 226. 227. 228. 229.\n",
            " 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241. 242. 243.\n",
            " 244. 246. 248. 249. 250. 251. 252. 253. 254. 255. 256. 257. 258. 259.\n",
            " 260. 261. 263. 264. 265. 266. 267. 268. 269. 270. 271. 272. 274. 275.\n",
            " 276. 277. 278. 279. 280. 281. 282. 283. 285. 286. 287. 289. 290. 291.\n",
            " 292. 293. 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305.\n",
            " 306. 308. 309. 310. 311. 312. 313. 314. 315. 316. 318. 319. 320. 321.\n",
            " 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n",
            " 336. 337. 338. 339. 341. 342. 343. 344. 346. 347. 348. 349. 350. 351.\n",
            " 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363. 364. 365.\n",
            " 366. 367. 368. 369. 370. 372. 373. 374. 375. 376. 377. 378. 379. 380.\n",
            " 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391. 392. 393. 394.\n",
            " 395. 396. 397. 398. 399. 400. 401. 402. 404. 405. 406. 407. 408. 409.\n",
            " 410. 411. 412. 413. 414. 415. 416. 417. 418. 419. 420. 421. 422. 423.\n",
            " 424. 425. 426. 427. 428. 429. 430. 431. 432. 434. 435. 436. 437. 438.\n",
            " 439. 440. 442. 443. 444. 446. 447. 448. 449. 451. 452. 453. 454. 455.\n",
            " 457. 458. 459. 460. 461. 462. 463. 465. 467. 469. 470. 471. 472. 473.\n",
            " 474. 475. 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487.\n",
            " 488. 491. 492. 494. 495. 496. 498. 499.]\n",
            "[ 90. 101. 102. 104. 112. 113. 123. 135. 141. 152. 165. 180. 189. 190.\n",
            " 194. 198. 214. 220. 245. 247. 262. 273. 284. 288. 307. 317. 340. 345.\n",
            " 371. 403. 433. 441. 445. 450. 456. 464. 466. 468. 489. 490. 493. 497.]\n",
            "fold 5******************************\n",
            "[ 86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98. 100.\n",
            " 101. 102. 103. 104. 105. 107. 109. 110. 111. 112. 113. 114. 115. 116.\n",
            " 118. 119. 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131.\n",
            " 132. 133. 134. 135. 136. 137. 138. 139. 141. 142. 143. 144. 146. 147.\n",
            " 148. 149. 150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161.\n",
            " 162. 163. 164. 165. 166. 167. 169. 170. 171. 172. 173. 174. 175. 176.\n",
            " 177. 178. 179. 180. 181. 183. 184. 185. 187. 188. 189. 190. 192. 193.\n",
            " 194. 195. 196. 197. 198. 199. 200. 201. 202. 205. 206. 207. 208. 209.\n",
            " 210. 211. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n",
            " 225. 226. 227. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238. 240.\n",
            " 241. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252. 253. 255. 257.\n",
            " 258. 259. 260. 261. 262. 263. 264. 265. 266. 267. 268. 269. 270. 271.\n",
            " 272. 273. 274. 275. 276. 277. 279. 280. 281. 282. 283. 284. 285. 287.\n",
            " 288. 289. 290. 291. 292. 293. 294. 295. 296. 297. 298. 299. 300. 301.\n",
            " 302. 303. 304. 305. 306. 307. 308. 310. 311. 312. 313. 314. 315. 316.\n",
            " 317. 318. 319. 321. 323. 324. 326. 327. 328. 329. 330. 331. 333. 334.\n",
            " 335. 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348.\n",
            " 349. 350. 351. 352. 353. 354. 355. 356. 357. 358. 360. 361. 362. 363.\n",
            " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 376. 377. 378. 379.\n",
            " 380. 382. 383. 385. 386. 387. 388. 389. 390. 391. 392. 393. 394. 395.\n",
            " 396. 397. 398. 399. 400. 401. 402. 403. 405. 407. 408. 409. 410. 411.\n",
            " 413. 414. 415. 416. 417. 418. 419. 420. 421. 423. 424. 425. 426. 427.\n",
            " 428. 429. 430. 431. 432. 433. 436. 437. 438. 439. 440. 441. 442. 444.\n",
            " 445. 446. 447. 448. 449. 450. 451. 452. 453. 454. 456. 458. 459. 460.\n",
            " 462. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475. 476.\n",
            " 477. 478. 479. 480. 481. 482. 484. 485. 486. 487. 488. 489. 490. 491.\n",
            " 492. 493. 494. 495. 496. 497. 498. 499.]\n",
            "[ 99. 106. 108. 117. 140. 145. 168. 182. 186. 191. 203. 204. 212. 228.\n",
            " 239. 242. 254. 256. 278. 286. 309. 320. 322. 325. 332. 359. 374. 375.\n",
            " 381. 384. 404. 406. 412. 422. 434. 435. 443. 455. 457. 461. 463. 483.]\n",
            "fold 6******************************\n",
            "[ 87.  88.  89.  90.  91.  92.  94.  95.  96.  97.  98.  99. 101. 102.\n",
            " 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116.\n",
            " 117. 118. 119. 120. 121. 123. 124. 126. 127. 128. 129. 130. 131. 132.\n",
            " 133. 134. 135. 136. 138. 139. 140. 141. 142. 145. 148. 149. 150. 151.\n",
            " 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165.\n",
            " 166. 168. 170. 171. 172. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n",
            " 184. 185. 186. 187. 188. 189. 190. 191. 192. 194. 196. 198. 199. 200.\n",
            " 201. 202. 203. 204. 205. 207. 208. 209. 210. 211. 212. 213. 214. 215.\n",
            " 217. 218. 220. 221. 222. 223. 224. 225. 226. 227. 228. 229. 231. 232.\n",
            " 234. 235. 236. 238. 239. 240. 241. 242. 243. 244. 245. 247. 249. 250.\n",
            " 251. 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264.\n",
            " 265. 266. 267. 268. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n",
            " 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n",
            " 294. 295. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307. 308.\n",
            " 309. 310. 311. 312. 314. 315. 316. 317. 318. 319. 320. 321. 322. 323.\n",
            " 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335. 337. 338. 339.\n",
            " 340. 341. 342. 343. 344. 345. 346. 347. 348. 350. 351. 352. 353. 354.\n",
            " 355. 356. 357. 358. 359. 360. 361. 363. 364. 365. 367. 368. 369. 370.\n",
            " 371. 372. 373. 374. 375. 376. 377. 378. 380. 381. 382. 383. 384. 387.\n",
            " 388. 389. 390. 392. 393. 394. 395. 396. 397. 398. 399. 401. 402. 403.\n",
            " 404. 405. 406. 407. 408. 409. 410. 412. 413. 414. 415. 416. 417. 419.\n",
            " 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n",
            " 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 447. 449.\n",
            " 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461. 462. 463.\n",
            " 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475. 476. 477.\n",
            " 478. 479. 480. 481. 482. 483. 486. 487. 488. 489. 490. 491. 492. 493.\n",
            " 494. 495. 496. 497. 498.]\n",
            "[ 86.  93. 100. 122. 125. 137. 143. 144. 146. 147. 167. 169. 173. 183.\n",
            " 193. 195. 197. 206. 216. 219. 230. 233. 237. 246. 248. 269. 296. 313.\n",
            " 324. 336. 349. 362. 366. 379. 385. 386. 391. 400. 411. 418. 446. 448.\n",
            " 484. 485. 499.]\n",
            "fold 7******************************\n",
            "[ 86.  88.  89.  90.  92.  93.  94.  95.  96.  97.  98.  99. 100. 101.\n",
            " 102. 103. 104. 105. 106. 107. 108. 110. 111. 112. 113. 115. 116. 117.\n",
            " 118. 119. 120. 121. 122. 123. 124. 125. 126. 129. 130. 131. 132. 133.\n",
            " 134. 135. 136. 137. 140. 141. 142. 143. 144. 145. 146. 147. 148. 149.\n",
            " 150. 151. 152. 154. 155. 156. 157. 159. 160. 161. 162. 164. 165. 166.\n",
            " 167. 168. 169. 171. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n",
            " 183. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196. 197.\n",
            " 198. 199. 200. 201. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212.\n",
            " 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224. 226. 227. 228.\n",
            " 229. 230. 231. 233. 235. 236. 237. 238. 239. 240. 242. 243. 244. 245.\n",
            " 246. 247. 248. 249. 250. 252. 253. 254. 255. 256. 257. 258. 259. 260.\n",
            " 262. 263. 264. 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276.\n",
            " 277. 278. 279. 280. 281. 283. 284. 285. 286. 287. 288. 289. 290. 291.\n",
            " 292. 293. 294. 295. 296. 298. 299. 300. 301. 302. 303. 304. 306. 307.\n",
            " 308. 309. 311. 312. 313. 314. 315. 317. 318. 319. 320. 322. 323. 324.\n",
            " 325. 327. 328. 329. 330. 331. 332. 333. 334. 335. 336. 337. 338. 339.\n",
            " 340. 342. 343. 345. 346. 347. 348. 349. 350. 351. 353. 354. 355. 356.\n",
            " 357. 358. 359. 360. 361. 362. 363. 364. 365. 366. 368. 369. 370. 371.\n",
            " 372. 373. 374. 375. 377. 378. 379. 380. 381. 382. 383. 384. 385. 386.\n",
            " 388. 389. 390. 391. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402.\n",
            " 403. 404. 405. 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416.\n",
            " 417. 418. 419. 420. 421. 422. 423. 424. 425. 426. 428. 430. 431. 432.\n",
            " 433. 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446.\n",
            " 447. 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460.\n",
            " 461. 462. 463. 464. 465. 466. 467. 468. 469. 471. 472. 473. 475. 476.\n",
            " 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489. 490.\n",
            " 492. 493. 494. 495. 496. 497. 499.]\n",
            "[ 87.  91. 109. 114. 127. 128. 138. 139. 153. 158. 163. 170. 172. 184.\n",
            " 202. 213. 225. 232. 234. 241. 251. 261. 265. 282. 297. 305. 310. 316.\n",
            " 321. 326. 341. 344. 352. 367. 376. 387. 392. 427. 429. 470. 474. 491.\n",
            " 498.]\n",
            "fold 8******************************\n",
            "[ 86.  87.  88.  90.  91.  92.  93.  94.  96.  97.  98.  99. 100. 101.\n",
            " 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115.\n",
            " 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127. 128. 129.\n",
            " 130. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144.\n",
            " 145. 146. 147. 148. 149. 150. 151. 152. 153. 154. 155. 156. 157. 158.\n",
            " 159. 160. 161. 162. 163. 164. 165. 167. 168. 169. 170. 171. 172. 173.\n",
            " 174. 175. 176. 177. 179. 180. 182. 183. 184. 185. 186. 187. 188. 189.\n",
            " 190. 191. 192. 193. 194. 195. 196. 197. 198. 199. 200. 202. 203. 204.\n",
            " 206. 207. 208. 209. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220.\n",
            " 221. 222. 223. 224. 225. 227. 228. 229. 230. 231. 232. 233. 234. 235.\n",
            " 236. 237. 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249.\n",
            " 250. 251. 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263.\n",
            " 264. 265. 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277.\n",
            " 278. 279. 280. 282. 283. 284. 286. 287. 288. 289. 290. 293. 294. 296.\n",
            " 297. 298. 299. 300. 301. 302. 303. 304. 305. 307. 309. 310. 311. 312.\n",
            " 313. 314. 315. 316. 317. 319. 320. 321. 322. 323. 324. 325. 326. 327.\n",
            " 328. 329. 330. 331. 332. 334. 335. 336. 337. 339. 340. 341. 342. 344.\n",
            " 345. 346. 347. 348. 349. 352. 354. 355. 356. 357. 358. 359. 360. 361.\n",
            " 362. 363. 364. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376.\n",
            " 377. 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390.\n",
            " 391. 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404.\n",
            " 405. 406. 407. 409. 410. 411. 412. 413. 414. 415. 416. 418. 421. 422.\n",
            " 423. 424. 425. 426. 427. 429. 430. 431. 432. 433. 434. 435. 436. 438.\n",
            " 439. 440. 441. 442. 443. 444. 445. 446. 448. 449. 450. 451. 452. 453.\n",
            " 455. 456. 457. 458. 459. 461. 462. 463. 464. 466. 467. 468. 469. 470.\n",
            " 472. 473. 474. 475. 477. 479. 480. 482. 483. 484. 485. 487. 488. 489.\n",
            " 490. 491. 492. 493. 494. 495. 496. 497. 498. 499.]\n",
            "[ 89.  95. 131. 166. 178. 181. 201. 205. 210. 226. 281. 285. 291. 292.\n",
            " 295. 306. 308. 318. 333. 338. 343. 350. 351. 353. 365. 408. 417. 419.\n",
            " 420. 428. 437. 447. 454. 460. 465. 471. 476. 478. 481. 486.]\n",
            "fold 9******************************\n",
            "[ 86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  97.  98.  99. 100.\n",
            " 101. 102. 103. 104. 105. 106. 108. 109. 111. 112. 113. 114. 115. 116.\n",
            " 117. 118. 119. 120. 122. 123. 124. 125. 126. 127. 128. 131. 133. 134.\n",
            " 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147. 148.\n",
            " 149. 150. 151. 152. 153. 154. 156. 157. 158. 160. 161. 162. 163. 165.\n",
            " 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 177. 178. 179. 180.\n",
            " 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 193. 194. 195.\n",
            " 196. 197. 198. 200. 201. 202. 203. 204. 205. 206. 208. 209. 210. 212.\n",
            " 213. 214. 215. 216. 218. 219. 220. 222. 223. 225. 226. 227. 228. 229.\n",
            " 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241. 242. 244.\n",
            " 245. 246. 247. 248. 249. 251. 252. 253. 254. 255. 256. 257. 258. 259.\n",
            " 260. 261. 262. 263. 264. 265. 266. 267. 269. 270. 271. 272. 273. 274.\n",
            " 275. 276. 277. 278. 279. 281. 282. 284. 285. 286. 287. 288. 289. 290.\n",
            " 291. 292. 293. 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304.\n",
            " 305. 306. 307. 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318.\n",
            " 319. 320. 321. 322. 323. 324. 325. 326. 328. 329. 330. 331. 332. 333.\n",
            " 334. 335. 336. 338. 340. 341. 342. 343. 344. 345. 347. 348. 349. 350.\n",
            " 351. 352. 353. 354. 355. 357. 358. 359. 360. 361. 362. 363. 365. 366.\n",
            " 367. 369. 371. 374. 375. 376. 377. 378. 379. 380. 381. 382. 383. 384.\n",
            " 385. 386. 387. 389. 390. 391. 392. 393. 394. 397. 398. 399. 400. 401.\n",
            " 402. 403. 404. 405. 406. 407. 408. 409. 410. 411. 412. 414. 415. 416.\n",
            " 417. 418. 419. 420. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431.\n",
            " 432. 433. 434. 435. 436. 437. 438. 440. 441. 442. 443. 444. 445. 446.\n",
            " 447. 448. 449. 450. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n",
            " 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n",
            " 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n",
            " 490. 491. 492. 493. 496. 497. 498. 499.]\n",
            "[ 96. 107. 110. 121. 129. 130. 132. 155. 159. 164. 176. 192. 199. 207.\n",
            " 211. 217. 221. 224. 243. 250. 268. 280. 283. 327. 337. 339. 346. 356.\n",
            " 364. 368. 370. 372. 373. 388. 395. 396. 413. 421. 439. 451. 494. 495.]\n",
            "fold 10******************************\n",
            "[ 86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.  99.\n",
            " 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113.\n",
            " 114. 116. 117. 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130.\n",
            " 131. 132. 133. 134. 135. 137. 138. 139. 140. 141. 142. 143. 144. 145.\n",
            " 146. 147. 150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161.\n",
            " 162. 163. 164. 165. 166. 167. 168. 169. 170. 172. 173. 174. 175. 176.\n",
            " 177. 178. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191.\n",
            " 192. 193. 194. 195. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206.\n",
            " 207. 208. 209. 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220.\n",
            " 221. 222. 223. 224. 225. 226. 227. 228. 229. 230. 232. 233. 234. 235.\n",
            " 236. 237. 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249.\n",
            " 250. 251. 253. 254. 255. 256. 257. 258. 259. 261. 262. 263. 264. 265.\n",
            " 266. 268. 269. 270. 271. 272. 273. 274. 276. 277. 278. 279. 280. 281.\n",
            " 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293. 294. 295.\n",
            " 296. 297. 298. 301. 302. 303. 305. 306. 307. 308. 309. 310. 312. 313.\n",
            " 315. 316. 317. 318. 319. 320. 321. 322. 324. 325. 326. 327. 328. 329.\n",
            " 331. 332. 333. 334. 335. 336. 337. 338. 339. 340. 341. 343. 344. 345.\n",
            " 346. 347. 349. 350. 351. 352. 353. 354. 355. 356. 359. 360. 361. 362.\n",
            " 363. 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376.\n",
            " 377. 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390.\n",
            " 391. 392. 393. 395. 396. 397. 400. 402. 403. 404. 405. 406. 407. 408.\n",
            " 409. 410. 411. 412. 413. 415. 416. 417. 418. 419. 420. 421. 422. 423.\n",
            " 425. 426. 427. 428. 429. 430. 431. 432. 433. 434. 435. 437. 438. 439.\n",
            " 440. 441. 442. 443. 445. 446. 447. 448. 449. 450. 451. 452. 453. 454.\n",
            " 455. 456. 457. 458. 460. 461. 462. 463. 464. 465. 466. 467. 468. 470.\n",
            " 471. 472. 473. 474. 475. 476. 477. 478. 479. 480. 481. 482. 483. 484.\n",
            " 485. 486. 487. 489. 490. 491. 492. 493. 494. 495. 497. 498. 499.]\n",
            "[115. 118. 119. 136. 148. 149. 171. 179. 196. 231. 252. 260. 267. 275.\n",
            " 299. 300. 304. 311. 314. 323. 330. 342. 348. 357. 358. 394. 398. 399.\n",
            " 401. 414. 424. 436. 444. 459. 469. 488. 496.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFkkMcrZnCIS"
      },
      "source": [
        "class JaneDataset(Dataset):\r\n",
        "    def __init__(self, np_X, np_y):\r\n",
        "        super(JaneDataset,self).__init__()\r\n",
        "        self.X = np_X\r\n",
        "        self.y = np_y\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return self.X.shape[0]\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        # date, weight, resp\r\n",
        "        X_util = self.X[index, :3]\r\n",
        "        X = torch.tensor(self.X[index, 3:],dtype=torch.float)\r\n",
        "        y = torch.tensor(self.y[index],dtype=torch.float)\r\n",
        "        return X_util, X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LYPKB_Roiu1"
      },
      "source": [
        "def utility_score(X_d_w,X_r,y):\r\n",
        "    # X for date, weight, resp numpy.array\r\n",
        "    # y for binary action by random threshold or prediction\r\n",
        "    \r\n",
        "    # date\r\n",
        "#     date_min, date_max = np.min(X_d_w[:,0]), np.max(X_d_w[:,0])\r\n",
        "    unq_dates = np.unique(X_d_w[:,0])\r\n",
        "    period = len(unq_dates)\r\n",
        "#     dates = np.arange(date_min, date_max+1)\r\n",
        "    \r\n",
        "    list_p = list()\r\n",
        "    \r\n",
        "    for date in unq_dates:\r\n",
        "        idx_date = X_d_w[:,0] == date\r\n",
        "        X_d = X_d_w[idx_date,0]\r\n",
        "        y_d = y[idx_date]\r\n",
        "        w_d = X_d_w[idx_date,1]\r\n",
        "        r_d = X_r[idx_date]\r\n",
        "        \r\n",
        "        p_d = w_d * r_d * y_d\r\n",
        "        p = p_d.sum()\r\n",
        "        \r\n",
        "        list_p.append(p)\r\n",
        "    \r\n",
        "    np_p = np.array(list_p)\r\n",
        "    \r\n",
        "\r\n",
        "    t = np.sum(np_p) / np.sqrt(np.sum(np.power(np_p,2))) * np.sqrt(250/period)\r\n",
        "    utility_score = min(max(t,0),6)*np_p.sum()\r\n",
        "    return utility_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PaAlHzUoj61"
      },
      "source": [
        "class EarlyStopping:\r\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\r\n",
        "        self.patience = patience\r\n",
        "        self.counter = 0\r\n",
        "        self.mode = mode\r\n",
        "        self.best_score = None\r\n",
        "        self.early_stop = False\r\n",
        "        self.delta = delta\r\n",
        "        if self.mode == \"min\":\r\n",
        "            self.val_score = np.Inf\r\n",
        "        else:\r\n",
        "            self.val_score = -np.Inf\r\n",
        "\r\n",
        "    def __call__(self, epoch_score, model, model_path):\r\n",
        "\r\n",
        "        if self.mode == \"min\":\r\n",
        "            score = -1.0 * epoch_score\r\n",
        "        else:\r\n",
        "            score = np.copy(epoch_score)\r\n",
        "\r\n",
        "        if self.best_score is None:\r\n",
        "            self.best_score = score\r\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\r\n",
        "        elif score < self.best_score: #  + self.delta\r\n",
        "            self.counter += 1\r\n",
        "            # print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\r\n",
        "            if self.counter >= self.patience:\r\n",
        "                self.early_stop = True\r\n",
        "        else:\r\n",
        "            self.best_score = score\r\n",
        "            # ema.apply_shadow()\r\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\r\n",
        "            # ema.restore()\r\n",
        "            self.counter = 0\r\n",
        "\r\n",
        "    def save_checkpoint(self, epoch_score, model, model_path):\r\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\r\n",
        "            print(f\"Validation score improved ({self.val_score:.4f} --> {epoch_score:.4f})\")\r\n",
        "            # if not DEBUG:\r\n",
        "            torch.save(model.state_dict(), model_path)\r\n",
        "        self.val_score = epoch_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xRC4lKUooCF"
      },
      "source": [
        "class EarlyStopping_GS:\r\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\r\n",
        "        self.patience = patience\r\n",
        "        self.counter = 0\r\n",
        "        self.mode = mode\r\n",
        "        self.best_score = None\r\n",
        "        self.early_stop = False\r\n",
        "        self.delta = delta\r\n",
        "        if self.mode == \"min\":\r\n",
        "            self.val_score = np.Inf\r\n",
        "        else:\r\n",
        "            self.val_score = -np.Inf\r\n",
        "\r\n",
        "    def __call__(self, epoch_score, model):\r\n",
        "\r\n",
        "        if self.mode == \"min\":\r\n",
        "            score = -1.0 * epoch_score\r\n",
        "        else:\r\n",
        "            score = np.copy(epoch_score)\r\n",
        "\r\n",
        "        if self.best_score is None:\r\n",
        "            self.best_score = score\r\n",
        "            self.save_checkpoint(epoch_score, model)\r\n",
        "        elif score < self.best_score: #  + self.delta\r\n",
        "            self.counter += 1\r\n",
        "            # print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\r\n",
        "            if self.counter >= self.patience:\r\n",
        "                self.early_stop = True\r\n",
        "        else:\r\n",
        "            self.best_score = score\r\n",
        "            # ema.apply_shadow()\r\n",
        "            # print(f\"Validation score improved ({self.val_score:.4f} --> {epoch_score:.4f}). Saving model!\")\r\n",
        "            self.save_checkpoint(epoch_score, model)\r\n",
        "            # ema.restore()\r\n",
        "            self.counter = 0\r\n",
        "\r\n",
        "    def save_checkpoint(self, epoch_score, model):\r\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\r\n",
        "            print(f\"Validation score improved ({self.val_score:.4f} --> {epoch_score:.4f})\")\r\n",
        "            # if not DEBUG:\r\n",
        "            # torch.save(model.state_dict(), model_path)\r\n",
        "        self.val_score = epoch_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48uCIdZxnTq_"
      },
      "source": [
        "epochs = 100\r\n",
        "batch_size = 4096\r\n",
        "\r\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAR8qSaSnXf0"
      },
      "source": [
        "class ResnetLinear(nn.Module):\r\n",
        "    def __init__(self, num_features, num_classes, df_features, device, verbose=False):\r\n",
        "        super(ResnetLinear,self).__init__()\r\n",
        "\r\n",
        "        self.hidden_layer = 256\r\n",
        "        self.num_layers = 2\r\n",
        "        self.decreasing = False\r\n",
        "        \r\n",
        "        self.f_act = nn.SiLU()\r\n",
        "        self.dropout = 0.49627361377205387\r\n",
        "        \r\n",
        "        self.embed_dim = 0\r\n",
        "\r\n",
        "        self.num_features = num_features\r\n",
        "        self.num_classes = num_classes\r\n",
        "        # self.hidden_layers = hidden_layers\r\n",
        "        # self.dropout = dropout\r\n",
        "        # self.embed_dim = 0\r\n",
        "        self.hidden_layers = None\r\n",
        "        self.emb_mode = None\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print(\"ResnetLinear Trial\")\r\n",
        "            print(f\"hidden_layer:{self.hidden_layer}; num_layers:{self.num_layers}; decreasing:{self.decreasing}; f_act:{self.f_act}; dropout:{self.dropout}; embed_dim:{self.embed_dim}\")        \r\n",
        "\r\n",
        "        if self.embed_dim == 0:\r\n",
        "            self.emb_mode = False\r\n",
        "\r\n",
        "        else:\r\n",
        "            self.emb_mode = True\r\n",
        "\r\n",
        "            # df_features tag num is 29(fixed value)\r\n",
        "            self.n_feat_tags = 29\r\n",
        "            # self.embed_dim = selfembed_dim\r\n",
        "            self.device = device\r\n",
        "            \r\n",
        "            self.df_features = df_features.loc[:,df_features.columns[1:]]\r\n",
        "            self.df_features[\"tag_29\"] = np.array([1]+[0] * (self.df_features.shape[0]-1))\r\n",
        "            self.df_features = self.df_features.astype(\"int8\")\r\n",
        "            self.features_tag_matrix = torch.tensor(self.df_features.values).to(self.device)\r\n",
        "            \r\n",
        "            self.n_feat_tags += 1\r\n",
        "            self.tag_embedding = nn.Embedding(self.n_feat_tags+1, self.embed_dim)\r\n",
        "            self.tag_weights = nn.Linear(self.n_feat_tags, 1)\r\n",
        "            \r\n",
        "\r\n",
        "        self.bn_d0 = nn.BatchNorm1d(self.num_features+ self.embed_dim)\r\n",
        "                \r\n",
        "        if self.decreasing:\r\n",
        "          self.hidden_layers = [int(self.hidden_layer/2**(i)) for i in range(self.num_layers)]\r\n",
        "        else:\r\n",
        "          self.hidden_layers = [int(self.hidden_layer) for i in range(self.num_layers)]\r\n",
        "\r\n",
        "        self.hidden_layers = [int(self.num_features + self.embed_dim)] + self.hidden_layers\r\n",
        "\r\n",
        "        denses = list()\r\n",
        "        \r\n",
        "        for i in range(len(self.hidden_layers)-1):\r\n",
        "            if i==0:\r\n",
        "                denses.append(self.make_layers(self.hidden_layers[i], self.hidden_layers[i+1], self.dropout, self.f_act))\r\n",
        "            else:\r\n",
        "                denses.append(self.make_layers(self.hidden_layers[i-1]+self.hidden_layers[i], self.hidden_layers[i+1], self.dropout, self.f_act))\r\n",
        "\r\n",
        "        self.denses = nn.Sequential(*denses)\r\n",
        "        \r\n",
        "        self.out_dense = nn.Linear(self.hidden_layers[-1] + self.hidden_layers[-2], self.num_classes)\r\n",
        "\r\n",
        "    def make_layers(self, in_channels, out_channels, dropout=None, f_act=nn.ReLU()):\r\n",
        "        layers = list()\r\n",
        "        layers.append(nn.Linear(in_channels, out_channels))\r\n",
        "        layers.append(nn.BatchNorm1d(out_channels))\r\n",
        "        layers.append(f_act)\r\n",
        "        \r\n",
        "        if dropout:\r\n",
        "            layers.append(nn.Dropout(dropout))\r\n",
        "        \r\n",
        "        module = nn.Sequential(*layers)\r\n",
        "        \r\n",
        "        return module\r\n",
        "    \r\n",
        "    # function to make embedding vector of Tag information per Features_0...129\r\n",
        "    def features2emb(self):\r\n",
        "        # one tag embedding to embed_dim dimension (1,embed_dim) per element\r\n",
        "        all_tag_idxs = torch.LongTensor(np.arange(self.n_feat_tags)).to(self.device)\r\n",
        "        tag_bools = self.features_tag_matrix\r\n",
        "        f_emb = self.tag_embedding(all_tag_idxs).repeat(130,1,1)\r\n",
        "        # f_emb에서 tag에 해당하는 값만 f_emb에 남김.\r\n",
        "        f_emb = f_emb * tag_bools[:,:,None]\r\n",
        "        \r\n",
        "        # 각 feature 별로 먗개의 tag가 속하는가?\r\n",
        "        s = torch.sum(tag_bools,dim=1)\r\n",
        "        # 각 feature 별로 tag값에 해당하여 남겨진 embedding 값을 dimension 별로 합산(1,1,29) / 각 featrue별로 구해진 tag 개수와 division\r\n",
        "        f_emb = torch.sum(f_emb, dim=-2) / s[:,None]\r\n",
        "        \r\n",
        "        return f_emb\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        # if embedding\r\n",
        "        if self.emb_mode:\r\n",
        "            f_emb = self.features2emb()\r\n",
        "            x = x.view(-1, self.num_features)\r\n",
        "            x_emb = torch.matmul(x,f_emb)\r\n",
        "            x = torch.hstack((x,x_emb))\r\n",
        "        \r\n",
        "        # num_features + embed_dim \r\n",
        "        x = self.bn_d0(x)\r\n",
        "        \r\n",
        "        x_prev = None\r\n",
        "        x_now = None\r\n",
        "\r\n",
        "        for idx, dense in enumerate(self.denses):\r\n",
        "            if idx == 0:\r\n",
        "                x_prev = x\r\n",
        "                x_now = dense(x_prev)\r\n",
        "                x = torch.cat([x_prev,x_now],1)\r\n",
        "                x_prev = x_now\r\n",
        "            else:\r\n",
        "                x_now = dense(x)\r\n",
        "                x = torch.cat([x_prev,x_now],1)\r\n",
        "                x_prev = x_now\r\n",
        "\r\n",
        "        x5 = self.out_dense(x)\r\n",
        "        \r\n",
        "        return x5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QFSYosin4oN"
      },
      "source": [
        "class FFN(nn.Module):\r\n",
        "    def __init__(self, num_features, num_classes, hidden_layers, dropout, f_act, is_op_act =False):\r\n",
        "        super(FFN,self).__init__()\r\n",
        "        \r\n",
        "        self.bn_d0 = nn.BatchNorm1d(num_features)\r\n",
        "\r\n",
        "        self.hidden_layers = [num_features] + hidden_layers\r\n",
        "\r\n",
        "        denses = list()\r\n",
        "        for i in range(len(self.hidden_layers)-1):\r\n",
        "            denses.append(self.make_layers(self.hidden_layers[i],self.hidden_layers[i+1],f_act,dropout))\r\n",
        "\r\n",
        "        self.denses = nn.Sequential(*denses)\r\n",
        "\r\n",
        "        self.out_dense = None\r\n",
        "        \r\n",
        "        if num_classes > 0:\r\n",
        "            self.out_dense = nn.Linear(self.hidden_layers[-1], num_classes)\r\n",
        "            \r\n",
        "        self.out_activ = None\r\n",
        "        \r\n",
        "        if is_op_act:\r\n",
        "            if num_classes == 1 or num_classes == 2:\r\n",
        "                self.out_active = nn.Sigmoid()\r\n",
        "            elif num_classes > 2:\r\n",
        "                self.out_active = nn.Softmax(dim=-1)\r\n",
        "    \r\n",
        "    def make_layers(self, in_channels, out_channels, f_act, dropout):\r\n",
        "        layers = list()\r\n",
        "        layers.append(nn.Linear(in_channels, out_channels))\r\n",
        "        layers.append(nn.BatchNorm1d(out_channels))\r\n",
        "        layers.append(f_act)\r\n",
        "\r\n",
        "        if dropout:\r\n",
        "            layers.append(nn.Dropout(dropout))\r\n",
        "            \r\n",
        "        module = nn.Sequential(*layers)\r\n",
        "        \r\n",
        "        return module\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        x = self.bn_d0(x)\r\n",
        "\r\n",
        "        x = self.denses(x)\r\n",
        "\r\n",
        "        if self.out_dense:\r\n",
        "            x = self.out_dense(x)\r\n",
        "        if self.out_activ:\r\n",
        "            x = self.out_active(x)\r\n",
        "            \r\n",
        "        return x\r\n",
        "\r\n",
        "class Emb_NN_Model(nn.Module):\r\n",
        "    def __init__(self, num_features, num_tags, num_classes, df_features, device, verbose=False):\r\n",
        "        super(Emb_NN_Model,self).__init__()\r\n",
        "\r\n",
        "        self.num_features = num_features\r\n",
        "        self.n_feat_tags = num_tags\r\n",
        "        self.num_classes = num_classes\r\n",
        "\r\n",
        "        # self.hidden_layers = hidden_layers\r\n",
        "        # self.embed_dim = embed_dim\r\n",
        "        self.hidden_layer = 256\r\n",
        "        self.num_layers = 4\r\n",
        "        self.decreasing = True\r\n",
        "\r\n",
        "        if self.decreasing:\r\n",
        "          self.hidden_layers = [int(self.hidden_layer/2**(i)) for i in range(self.num_layers)]\r\n",
        "        else:\r\n",
        "          self.hidden_layers = [int(self.hidden_layer) for i in range(self.num_layers)]\r\n",
        "\r\n",
        "        self.f_act = nn.SiLU()\r\n",
        "        self.dropout = 0.17971171427796284\r\n",
        "\r\n",
        "        self.embed_dim = 5\r\n",
        "\r\n",
        "        self.embed_mode = None\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print(\"Embed-NN Trial\")\r\n",
        "            print(f\"hidden_layer:{self.hidden_layer}; num_layers:{self.num_layers}; decraesing:{self.decreasing}; f_act:{self.f_act}; dropout:{self.dropout}; embed_dim:{self.embed_dim}\")\r\n",
        "\r\n",
        "        if self.embed_dim == 0:\r\n",
        "            self.embed_mode = False\r\n",
        "        if self.embed_dim > 0:\r\n",
        "            self.embed_mode = True\r\n",
        "\r\n",
        "            self.device = device\r\n",
        "        \r\n",
        "            self.df_features = df_features.loc[:,df_features.columns[1:]]\r\n",
        "            self.df_features[\"tag_29\"] = np.array([1]+[0] * (self.df_features.shape[0]-1))\r\n",
        "            self.df_features = self.df_features.astype(\"int8\")\r\n",
        "            self.features_tag_matrix = torch.tensor(self.df_features.values).to(self.device)\r\n",
        "        \r\n",
        "            self.n_feat_tags += 1\r\n",
        "            self.tag_embedding = nn.Embedding(self.n_feat_tags+1, self.embed_dim)\r\n",
        "            self.tag_weights = nn.Linear(self.n_feat_tags, 1)\r\n",
        "        \r\n",
        "        self.ffn = FFN(num_features=(self.num_features+self.embed_dim), num_classes=0, hidden_layers=self.hidden_layers, f_act=self.f_act, dropout=self.dropout)\r\n",
        "        self.dense = nn.Linear(self.hidden_layers[-1], self.num_classes)\r\n",
        "\r\n",
        "    # function to make embedding vector of Tag information per Features_0...129\r\n",
        "    def features2emb(self):\r\n",
        "        # one tag embedding to embed_dim dimension (1,embed_dim) per element\r\n",
        "        all_tag_idxs = torch.LongTensor(np.arange(self.n_feat_tags)).to(self.device)\r\n",
        "        tag_bools = self.features_tag_matrix\r\n",
        "        f_emb = self.tag_embedding(all_tag_idxs).repeat(130,1,1)\r\n",
        "        # f_emb에서 tag에 해당하는 값만 f_emb에 남김.\r\n",
        "        f_emb = f_emb * tag_bools[:,:,None]\r\n",
        "        \r\n",
        "        # 각 feature 별로 먗개의 tag가 속하는가?\r\n",
        "        s = torch.sum(tag_bools,dim=1)\r\n",
        "        # 각 feature 별로 tag값에 해당하여 남겨진 embedding 값을 dimension 별로 합산(1,1,29) / 각 featrue별로 구해진 tag 개수와 division\r\n",
        "        f_emb = torch.sum(f_emb, dim=-2) / s[:,None]\r\n",
        "        \r\n",
        "        return f_emb\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        if self.embed_mode:\r\n",
        "            x = x.view(-1, self.num_features)\r\n",
        "            # 130 X 5\r\n",
        "            f_emb = self.features2emb()\r\n",
        "            # N X 130 x 130 X 5 => N x 5 => \r\n",
        "            x_emb = torch.matmul(x, f_emb)    \r\n",
        "            # N X 130 + N X 5 => \r\n",
        "            x = torch.hstack((x,x_emb))\r\n",
        "\r\n",
        "        x = self.ffn(x)\r\n",
        "        x = self.dense(x)\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFr9h_TZo4iW"
      },
      "source": [
        "lr_emb = 0.000663767918321238\r\n",
        "wd_emb = 2.6504094565959894e-07\r\n",
        "\r\n",
        "lr_reslin = 2.9521544108896628e-05\r\n",
        "wd_reslin = 5.679142529741758e-05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5dXM7JsosGa"
      },
      "source": [
        "# NFOLDS = 1\r\n",
        "# EARLYSTOP_NUM = 3\r\n",
        "CACHE_PATH = model_home\r\n",
        "\r\n",
        "dict_combine_loss = defaultdict(lambda: 0)\r\n",
        "dict_combine_auc = defaultdict(lambda: 0)\r\n",
        "dict_combine_util = defaultdict(lambda: 0)\r\n",
        "\r\n",
        "train_idx, valid_idx = cv_idxes[0]\r\n",
        "X_train = X[train_idx, :]\r\n",
        "y_train = y[train_idx, :]\r\n",
        "\r\n",
        "X_valid = X[valid_idx, :]\r\n",
        "y_valid = y[valid_idx, :]\r\n",
        "\r\n",
        "train_dataset = JaneDataset(X_train, y_train)\r\n",
        "valid_dataset = JaneDataset(X_valid, y_valid)\r\n",
        "\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\r\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\r\n",
        "\r\n",
        "seed_torch(seed=entire_seed)\r\n",
        "torch.cuda.empty_cache()\r\n",
        "\r\n",
        "emb_model = Emb_NN_Model(130, 29, 5, df_features, device)\r\n",
        "emb_model = emb_model.to(device)\r\n",
        "emb_file_name = \"Embed-nn.pth\"\r\n",
        "emb_file_path = os.path.join(CACHE_PATH, emb_file_name)\r\n",
        "emb_model.load_state_dict(torch.load(emb_file_path))\r\n",
        "\r\n",
        "reslin_model = ResnetLinear(130,5,df_features,device)\r\n",
        "reslin_model = reslin_model.to(device)\r\n",
        "reslin_file_name = \"ResnetLinear.pth\"\r\n",
        "reslin_file_path = os.path.join(CACHE_PATH, reslin_file_name)\r\n",
        "reslin_model.load_state_dict(torch.load(reslin_file_path))\r\n",
        "\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "\r\n",
        "# es = EarlyStopping(EARLYSTOP_NUM, mode=\"max\")\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    emb_model.eval()\r\n",
        "    reslin_model.eval()\r\n",
        "\r\n",
        "    for idx, (X_utils, inputs, labels) in enumerate(tqdm_notebook(valid_dataloader)):\r\n",
        "\r\n",
        "        X_d_w = X_utils[:,:-1].detach().cpu().numpy()\r\n",
        "        X_r = X_utils[:,-1].detach().cpu().numpy()\r\n",
        "\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        emb_outputs = emb_model(inputs)\r\n",
        "        reslin_outputs = reslin_model(inputs)\r\n",
        "\r\n",
        "        true = labels.detach().cpu().numpy()[:,-1]\r\n",
        "\r\n",
        "        for i in range(1,10):\r\n",
        "          emb_pct = i*0.1\r\n",
        "          reslin_pct = 1-emb_pct\r\n",
        "          \r\n",
        "          outputs = (emb_pct * emb_outputs.detach()) + (reslin_pct * reslin_outputs.detach()) \r\n",
        "\r\n",
        "          combined_output = (emb_pct * emb_outputs.detach().sigmoid().cpu().numpy()[:,-1]) + (reslin_pct * reslin_outputs.detach().sigmoid().cpu().numpy()[:,-1])\r\n",
        "          combined_target = np.array(list(map(lambda x: 1 if x > 0.5 else 0, combined_output)),dtype=np.float)\r\n",
        "        \r\n",
        "          # acc = (true == combined_target).sum() / outputs.shape[0]\r\n",
        "          auc = roc_auc_score(true, outputs.cpu().numpy()[:,-1])\r\n",
        "          util = utility_score(X_d_w,X_r,combined_target)\r\n",
        "\r\n",
        "          dict_combine_auc[(emb_pct, reslin_pct)] += auc\r\n",
        "          dict_combine_util[(emb_pct, reslin_pct)] += util\r\n",
        "\r\n",
        "          loss = criterion(outputs, labels)\r\n",
        "          dict_combine_loss[(emb_pct, reslin_pct)] += loss.detach().item() * inputs.size(0)\r\n",
        "\r\n",
        "    # dict_combine_loss = running_loss / len(valid_dataloader.dataset)\r\n",
        "\r\n",
        "    # dict_combine_auc = running_auc / len(valid_dataloader)\r\n",
        "\r\n",
        "\r\n",
        "# print(f\"EPOCH:{epoch+1}|{epochs}; loss(train/valid):{epoch_loss:.4f}/{valid_loss:.4f}; acc(train/valid):{epoch_acc:.4f}/{valid_acc:.4f}; auc(train/valid):{epoch_auc:.4f}/{valid_auc:.4f}; utility(train/valid):{epoch_util:.4f}/{valid_util:.4f}\")\r\n",
        "\r\n",
        "# model_weights = os.path.join(CACHE_PATH,f\"tuned1-embed-nn_{_fold}.pth\")\r\n",
        "# es(valid_auc, model, model_path=model_weights)\r\n",
        "# if es.early_stop:\r\n",
        "#   print(\"Early stopping\")\r\n",
        "#   break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCZHK_Nqu_9f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}