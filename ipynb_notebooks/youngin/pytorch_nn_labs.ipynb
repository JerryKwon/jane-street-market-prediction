{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "pytorch-nn-labs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA5j8tuwnevl",
        "outputId": "9d6d1c4f-b7ae-4406-a753-244b59c3903d"
      },
      "source": [
        "\"\"\"Colab Drive Connection\"\"\"\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbMkDicinaV9",
        "outputId": "a6c5f5fa-5ff3-4d08-bf4b-988707687669"
      },
      "source": [
        "import warnings\n",
        "import os \n",
        "\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "\n",
        "import networkx as nx\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# install datatable\n",
        "!pip install datatable\n",
        "import datatable as dt\n",
        "\n",
        "from numba import njit\n",
        "\n",
        "import gc\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\")\n",
        "\n",
        "# project_home = \"/kaggle/input/jane-street-market-prediction\"\n",
        "\n",
        "project_home = \"/gdrive/MyDrive/colab/jane-street-market-prediction\"\n",
        "data_home = os.path.join(project_home, \"input/data\")\n",
        "model_home = os.path.join(project_home, \"output/model\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datatable\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/cb/21810c43b687a19d194c372192049f535fba28c55ce76d37e7e407159c52/datatable-0.11.1-cp36-cp36m-manylinux2010_x86_64.whl (83.7MB)\n",
            "\u001b[K     |████████████████████████████████| 83.7MB 63kB/s \n",
            "\u001b[?25hInstalling collected packages: datatable\n",
            "Successfully installed datatable-0.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UAcoB2EqnaV-"
      },
      "source": [
        "entire_seed = 1029\n",
        "\n",
        "def seed_torch(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    \n",
        "seed_torch(entire_seed)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i6690m01naV-"
      },
      "source": [
        "train_file = os.path.join(data_home,'train.csv')\n",
        "features_file = os.path.join(data_home,'features.csv')\n",
        "example_test_file = os.path.join(data_home,'example_test.csv')\n",
        "example_sample_submission_file = os.path.join(data_home,'example_sample_submission.csv')\n",
        "\n",
        "train_data_datatable = dt.fread(train_file)\n",
        "\n",
        "df_train = train_data_datatable.to_pandas()\n",
        "df_features = pd.read_csv(features_file)\n",
        "df_example_test = pd.read_csv(example_test_file)\n",
        "df_example_sample_submission = pd.read_csv(example_sample_submission_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "syvZDP5OnaV-"
      },
      "source": [
        "features = [ col for col in df_train.columns if \"feature\" in col ]\n",
        "resps = [ col for col in df_train.columns if \"resp\" in col ]\n",
        "target_resp = [resp_ for resp_ in resps if \"_\" not in resp_]\n",
        "target = [\"weight\"] + target_resp + features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9irkLVhYnaV_"
      },
      "source": [
        "\"\"\"\n",
        "Reduce Memory Usage by 75%\n",
        "https://www.kaggle.com/tomwarrens/nan-values-depending-on-time-of-day\n",
        "\"\"\"\n",
        "\n",
        "## Reduce Memory\n",
        "\n",
        "def reduce_memory_usage(df):\n",
        "    \n",
        "    start_memory = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"Memory usage of dataframe is {start_memory} MB\")\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != 'object':\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            \n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            \n",
        "            else:\n",
        "#                 reducing float16 for calculating numpy.nanmean\n",
        "#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "#                     df[col] = df[col].astype(np.float16)\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    pass\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "    \n",
        "    end_memory = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n",
        "    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\n",
        "    return df\n",
        "\n",
        "df_train = reduce_memory_usage(df_train)\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9zAivGb3naV_"
      },
      "source": [
        "# drop before 85days\n",
        "df_train = df_train.loc[df_train.date>85]\n",
        "# drop weight 0 for training\n",
        "df_train = df_train.loc[df_train.weight > 0]\n",
        "\n",
        "df_labels = df_train[['date','weight','resp_1','resp_2','resp_3','resp_4','resp']]\n",
        "\n",
        "df_train = df_train.drop(df_labels.columns,axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "daU4lp_qnaV_"
      },
      "source": [
        "\"\"\"\n",
        "The codes from 'Optimise Speed of Filling-NaN Function'\n",
        "https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function\n",
        "\"\"\"\n",
        "\n",
        "def for_loop(method, matrix, values):\n",
        "    for i in range(matrix.shape[0]):\n",
        "        matrix[i] = method(matrix[i], values)\n",
        "    return matrix\n",
        "\n",
        "def for_loop_ffill(method, matrix):\n",
        "    tmp = np.zeros(matrix.shape[1],dtype=np.float32)\n",
        "    for i in range(matrix.shape[0]):\n",
        "        matrix[i] = method(matrix[i], tmp)\n",
        "        tmp = matrix[i]\n",
        "    return matrix\n",
        "\n",
        "@njit\n",
        "def fillna_npwhere_njit(array, values):\n",
        "    if np.isnan(array.sum()):\n",
        "        array = np.where(np.isnan(array), values, array)\n",
        "    return array"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vLaxNPOynaV_"
      },
      "source": [
        "# converting numpy for efficient calcualtion.\n",
        "# ft 1~129\n",
        "np_train = df_train.loc[:,features[1:]].values\n",
        "np_train.shape\n",
        "\n",
        "# ft 0\n",
        "np_train_ft0 = df_train.loc[:,features[0]].values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LSIsYjKlnaWA"
      },
      "source": [
        "# nead pre-calculate 1.2GB per action\n",
        "f_mean = np.nanmean(np_train,axis=0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5kp4PuMnaWA",
        "outputId": "e0de6438-ff7a-4628-84be-9cd376d22350"
      },
      "source": [
        "print('fillna_npwhere_njit (mean-filling):')\n",
        "np_mf_train = for_loop(fillna_npwhere_njit, np_train, f_mean)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fillna_npwhere_njit (mean-filling):\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "omNSA1JanaWA"
      },
      "source": [
        "np_train = np.concatenate([np_train_ft0.reshape(-1,1),np_mf_train],axis=1)\n",
        "# resp_{1~4}, resp 모두를 고려; 각각을 0과 1로 분류하는 개별적인 Binary Classification 문제로 간주\n",
        "# ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp'] 순서\n",
        "np_targets = np.stack([(df_labels[c] > 0).astype('int') for c in resps]).T"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_JKiwv1pnaWA"
      },
      "source": [
        "class JaneDataset(Dataset):\n",
        "    def __init__(self, np_X, np_y):\n",
        "        super(JaneDataset,self).__init__()\n",
        "        self.X = np_X\n",
        "        self.y = np_y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        X = torch.tensor(self.X[index,:],dtype=torch.float)\n",
        "        y = torch.tensor(self.y[index],dtype=torch.float)\n",
        "        return X,y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "96fx1qJCnaWA"
      },
      "source": [
        "dataset = JaneDataset(np_train, np_targets)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tVfiS1xLnaWB"
      },
      "source": [
        "train_size = int(len(dataset) * 0.8)\n",
        "valid_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(entire_seed))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyoYSPQDnaWB"
      },
      "source": [
        "## CNN(Convolutional Neural Network)\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"imgs/cnn-arch.png\">\n",
        "<p>Basic CNN architecture for image classification</p>\n",
        "</div>\n",
        "\n",
        "### 1. 합성곱층\n",
        "\n",
        "일반적으로 합성곱은 그림과 같이 연속적인 은닉층을 거쳐 특징을 조합하고 압축한다. 첫번째 은닉층에서는 작은 저수준 특성에 집중하고, 다음 은닉층에는 더 큰 고수준 톡성으로 조합해나가도록 한다. 이런 계층적 구조가 CNN이 이미지 인식에 잘 작동하는 이유 중에 하나이다.\n",
        "\n",
        "합성곱층은 2차원 이미지를 기준으로 hxw크기를 가진 수용장(kernel)을 임의의 간격 stride를 두어 특성맵(feature map)을 생성한다. feature map을 생성함에 있어 stride값을 크게 주어 차원을 축소한 특성맵을 얻을 수도 있다. \n",
        "\n",
        "이러한 수용장(filter)를 이동하여 얻은 특성맵은 output filter로 정의한 개수만큼 filter를 가지고, 각각의 feature map은 2차원의 데이터이기 때문에 실질적으로 합성곱층을 거친 데이터는 3차원의 데이터로 표현된다. \n",
        "\n",
        "하나의 특성맵안에서는 모든 뉴런이 같은 파라미터를 공유하고, 다른 특성맵 간에는 다른 파라미터를 사용한다. 다시 말해 하나의 합성곱층이 입력에 여러 필터를 동시에 적용하여 입력에 있는 여러 특성을 감지하는 것이다.\n",
        "\n",
        "### 2. 풀링층\n",
        "\n",
        "풀링층의 목적은 계산량과 메모리 사용량 그리고 파라미터의 수를 줄이기 위해 부표본(축소본)을 만드는 것이다. 합성곱이 특정 feature map의 하나의 포인트(뉴런)를 얻는 것과 같이 이전 layer의 수용장에 있는 출력값과 연결되어 있다. 그러나 풀링 뉴런은 가중치가 없으므로, 합산함수를 사용하여 입력값을 더하는 것이 전부이다.\n",
        "\n",
        "* 최대 풀링층\n",
        "* 평균 풀링층\n",
        "* 전역 평균 풀링층: 각 특성 맵의 평균을 계산함으로써 특성 맵의 대부부분의 정보를 잃게되지만 출력층에는 유용하다.\n",
        "\n",
        "### 3. CNN의 기본 구조\n",
        "\n",
        "1. 합성곱층을 몇개 쌓고(각각에 활성화함수추가)\n",
        "2. 풀링층\n",
        "3. 추가적인 연속적인 합성곱층\n",
        "4. 풀링층\n",
        "\n",
        "이를 통해 네트워크를 거칠 수록 이미지는 점점 작아지지만, 합성곱 층 때문에 일반적으로 점점 더 깊어진다(많은 특성 맵을 가진다). \n",
        "\n",
        "첫번째 층은 필터가 적더라도 그 크기를 크게 잡는다. (kernel_size)\n",
        "\n",
        "이후의 층은 필터의 수를 점차적으로 늘이고, 그에 상응하여 크기를 줄인다.\n",
        "\n",
        "* 1d-cnn\n",
        "\n",
        "https://wikidocs.net/80437\n",
        "\n",
        "https://ratsgo.github.io/deep%20learning/2017/10/09/CNNs/\n",
        "\n",
        "https://medium.com/@Rehan_Sayyad/how-to-use-convolutional-neural-networks-for-time-series-classification-80575131a474\n",
        "\n",
        "https://arxiv.org/abs/1905.03554\n",
        "\n",
        "https://www.kaggle.com/pyoungkangkim/1dcnn-pytorch-jstreet\n",
        "\n",
        "https://www.kaggle.com/a763337092/pytorch-resnet-starter-training\n",
        "\n",
        "https://pulsar-kkaturi.tistory.com/entry/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-%EC%B2%AB%EA%B1%B8%EC%9D%8C-5-%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "It3N1TH-naWB"
      },
      "source": [
        "# class Model_1DCNN(nn.Module):\n",
        "#     def __init__(self, num_features, num_targets, hidden_size):\n",
        "#         super(Model_1DCNN, self).__init__()\n",
        "        \n",
        "#         self.hidden_size = hidden_size\n",
        "#         # num_channel\n",
        "#         self.ch_input = 16\n",
        "#         self.ch_output =  32\n",
        "#         self.points = int(self.hidden_size / self.ch_input) \n",
        "        \n",
        "#         # feature_size to hidden_size\n",
        "#         self.bn_dense1 = nn.BatchNorm1d(num_features)\n",
        "#         self.dropout_dense1 = nn.Dropout(0.2)\n",
        "#         self.dense1 = nn.Linear(num_features, hidden_size)\n",
        "        \n",
        "#         # reshaped hidden_size [input_channel, data] to [input_channel, output_channel] \n",
        "#         self.bn_c1 = nn.BatchNorm1d(self.ch_output)\n",
        "#         self.dropout_c1 = nn.Dropout(0.2)\n",
        "#         self.conv1 = nn.Conv1d(self.ch_input, self.ch_output, kernel_size=5, padding=2, stride=2)\n",
        "#         self.max_pool_c1 = nn.MaxPool1d(kernel_size=2)\n",
        "        \n",
        "#         self.bn_c2 = nn.BatchNorm1d(self.ch_output*2)\n",
        "#         self.dropout_c2 = nn.Dropout(0.2)\n",
        "#         self.conv2 = nn.Conv1d(self.ch_output, self.ch_output*2, kernel_size=3, padding=1, stride=1)\n",
        "        \n",
        "#         self.bn_c2_1 = nn.BatchNorm1d(self.ch_output*2)\n",
        "#         self.dropout_c2_1 = nn.Dropout(0.2)\n",
        "#         self.conv2_1 = nn.Conv1d(self.ch_output*2, self.ch_output*2, kernel_size=3, padding=1, stride=1)\n",
        "        \n",
        "#         self.max_pool_c2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "#         self.flatten = nn.Flatten()\n",
        "        \n",
        "#         self.bn_dense2 = nn.BatchNorm1d(256)\n",
        "#         self.dropout_dense2 = nn.Dropout(0.2)\n",
        "#         self.dense2 = nn.Linear(512,256)\n",
        "\n",
        "#         self.bn_dense3 = nn.BatchNorm1d(num_targets)\n",
        "#         self.dropout_dense3 = nn.Dropout(0.2)\n",
        "#         self.dense3 = nn.Linear(256,num_targets)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.bn_dense1(x)\n",
        "#         x = self.dropout_dense1(x)\n",
        "#         x = self.dense1(x)\n",
        "        \n",
        "#         x = x.reshape(x.size(0), self.ch_input, self.points)\n",
        "#         # print(x.shape)\n",
        "\n",
        "#         x = self.conv1(x)\n",
        "#         x = F.relu(self.bn_c1(x))\n",
        "#         # print(x.shape)\n",
        "#         x = self.dropout_c1(x)\n",
        "#         x = self.max_pool_c1(x)\n",
        "#         # print(x.shape)\n",
        "        \n",
        "#         x = self.conv2(x)\n",
        "#         x = F.relu(self.bn_c2(x))\n",
        "#         x = self.dropout_c2(x)\n",
        "#         # print(x.shape)\n",
        "        \n",
        "#         x = self.conv2_1(x)\n",
        "#         x = F.relu(self.bn_c2_1(x))\n",
        "#         x = self.dropout_c2_1(x)\n",
        "#         # print(x.shape)\n",
        "\n",
        "#         x = self.max_pool_c2(x)\n",
        "#         # print(x.shape)\n",
        "\n",
        "#         x = self.flatten(x)\n",
        "#         # print(x.shape)\n",
        "\n",
        "#         x = self.dense2(x)\n",
        "#         x = self.bn_dense2(x)\n",
        "#         x = self.dropout_dense2(x)\n",
        "\n",
        "#         x = self.dense3(x)\n",
        "#         x = self.bn_dense3(x)\n",
        "#         x = self.dropout_dense3(x)\n",
        "        \n",
        "#         x = F.sigmoid(x)\n",
        "\n",
        "#         return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z6zv8ssD22e"
      },
      "source": [
        "# class Model_1DCNN(nn.Module):\r\n",
        "#     def __init__(self, num_features, num_targets, hidden_size):\r\n",
        "#         super(Model_1DCNN, self).__init__()\r\n",
        "        \r\n",
        "#         self.hidden_size = hidden_size\r\n",
        "#         # num_channel\r\n",
        "#         self.ch_input = 16\r\n",
        "#         self.ch_output =  32\r\n",
        "#         self.points = int(self.hidden_size / self.ch_input) \r\n",
        "        \r\n",
        "#         # feature_size to hidden_size\r\n",
        "#         self.bn_dense1 = nn.BatchNorm1d(num_features)\r\n",
        "#         self.dropout_dense1 = nn.Dropout(0.2)\r\n",
        "#         self.dense1 = nn.Linear(num_features, hidden_size)\r\n",
        "        \r\n",
        "#         # reshaped hidden_size [input_channel, data] to [input_channel, output_channel]\r\n",
        "#         self.c1 = self.make_layers(self.ch_input,self.ch_output, 5, 2, 2, 0.2)\r\n",
        "#         self.max_pool_c1 = nn.MaxPool1d(kernel_size=2)\r\n",
        "\r\n",
        "#         self.c2 = self.make_layers(self.ch_output, self.ch_output*2, 3, 2, 1, 0.2)\r\n",
        "#         self.c2_1 = self.make_layers(self.ch_output*2, self.ch_output*2, 3, 1, 1, 0.2)\r\n",
        "#         self.c2_2 = self.make_layers(self.ch_output*2, self.ch_output*2, 3, 1, 1, 0.2)\r\n",
        "\r\n",
        "#         self.max_pool_c2 = nn.MaxPool1d(kernel_size=2)\r\n",
        "\r\n",
        "#         self.flatten = nn.Flatten()\r\n",
        "        \r\n",
        "#         self.bn_dense2 = nn.BatchNorm1d(64)\r\n",
        "#         self.dropout_dense2 = nn.Dropout(0.2)\r\n",
        "#         self.dense2 = nn.Linear(128,64)\r\n",
        "\r\n",
        "#         self.bn_dense3 = nn.BatchNorm1d(num_targets)\r\n",
        "#         self.dropout_dense3 = nn.Dropout(0.2)\r\n",
        "#         self.dense3 = nn.Linear(64,num_targets)\r\n",
        "\r\n",
        "#     def make_layers(self, ch_in, ch_out, kernel_size, stride, padding, dropout):\r\n",
        "#         cnn_module = nn.Sequential(\r\n",
        "#                         nn.Conv1d(ch_in,ch_out, kernel_size, stride, padding),\r\n",
        "#                         nn.BatchNorm1d(ch_out),\r\n",
        "#                         nn.ReLU(),\r\n",
        "#                         nn.Dropout(dropout)\r\n",
        "#                     )\r\n",
        "        \r\n",
        "#         return cnn_module\r\n",
        "\r\n",
        "#     def forward(self, x):\r\n",
        "#         x = self.bn_dense1(x)\r\n",
        "#         x = self.dropout_dense1(x)\r\n",
        "#         x = self.dense1(x)\r\n",
        "        \r\n",
        "#         x = x.reshape(x.size(0), self.ch_input, self.points)\r\n",
        "#         print(x.shape)\r\n",
        "\r\n",
        "#         x = self.c1(x)\r\n",
        "#         print(x.shape)\r\n",
        "#         x = self.max_pool_c1(x)\r\n",
        "#         print(x.shape)\r\n",
        "        \r\n",
        "#         x = self.c2(x)\r\n",
        "#         print(x.shape)\r\n",
        "#         x = self.c2_1(x)\r\n",
        "#         print(x.shape)\r\n",
        "#         x = self.c2_2(x)\r\n",
        "#         print(x.shape)\r\n",
        "#         x = self.max_pool_c2(x)\r\n",
        "#         print(x.shape)\r\n",
        "\r\n",
        "#         x = self.flatten(x)\r\n",
        "#         print(x.shape)\r\n",
        "\r\n",
        "#         x = self.dense2(x)\r\n",
        "#         x = self.bn_dense2(x)\r\n",
        "#         x = self.dropout_dense2(x)\r\n",
        "\r\n",
        "#         x = self.dense3(x)\r\n",
        "#         x = self.bn_dense3(x)\r\n",
        "#         x = self.dropout_dense3(x)\r\n",
        "        \r\n",
        "#         x = F.sigmoid(x)\r\n",
        "\r\n",
        "#         return x"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7fpbggnM5IU"
      },
      "source": [
        "# class Model_1DCNN(nn.Module):\r\n",
        "#     def __init__(self, num_features, num_targets, hidden_size):\r\n",
        "#         super(Model_1DCNN, self).__init__()\r\n",
        "        \r\n",
        "#         self.hidden_size = hidden_size\r\n",
        "#         # num_channel\r\n",
        "#         self.ch_input = 16\r\n",
        "#         self.ch_output =  32\r\n",
        "#         self.points = int(self.hidden_size / self.ch_input) \r\n",
        "        \r\n",
        "#         # feature_size to hidden_size\r\n",
        "#         self.bn_dense1 = nn.BatchNorm1d(num_features)\r\n",
        "#         self.dropout_dense1 = nn.Dropout(0.2)\r\n",
        "#         self.dense1 = nn.Linear(num_features, hidden_size)\r\n",
        "        \r\n",
        "#         # reshaped hidden_size [input_channel, data] to [input_channel, output_channel]\r\n",
        "#         # self.c1 = self.make_layers(self.ch_input,self.ch_output, 5, 2, 2, 0.2)\r\n",
        "#         # self.max_pool_c1 = nn.MaxPool1d(kernel_size=2)\r\n",
        "\r\n",
        "#         # self.c2 = self.make_layers(self.ch_output, self.ch_output*2, 3, 2, 1, 0.2)\r\n",
        "#         # self.c2_1 = self.make_layers(self.ch_output*2, self.ch_output*2, 3, 1, 1, 0.2)\r\n",
        "#         # self.c2_2 = self.make_layers(self.ch_output*2, self.ch_output*2, 3, 1, 1, 0.2)\r\n",
        "\r\n",
        "#         # self.max_pool_c2 = nn.MaxPool1d(kernel_size=2)\r\n",
        "\r\n",
        "#         # self.flatten = nn.Flatten()\r\n",
        "        \r\n",
        "#         # self.bn_dense2 = nn.BatchNorm1d(64)\r\n",
        "#         # self.dropout_dense2 = nn.Dropout(0.2)\r\n",
        "#         # self.dense2 = nn.Linear(128,64)\r\n",
        "\r\n",
        "#         # self.bn_dense3 = nn.BatchNorm1d(num_targets)\r\n",
        "#         # self.dropout_dense3 = nn.Dropout(0.2)\r\n",
        "#         # self.dense3 = nn.Linear(64,num_targets)\r\n",
        "\r\n",
        "#         self.c1 = self.make_layers(self.ch_input,self.ch_output, 5,2,2,0.2)\r\n",
        "\r\n",
        "#         self.c2 = self.make_layers(self.ch_input,self.ch_output, 3,2,1,0.2)\r\n",
        "#         self.c2_1 = self.make_layers(self.ch_input,self.ch_output, 3,1,1,0.2)\r\n",
        "\r\n",
        "#     def make_layers(self, ch_in, ch_out, kernel_size, stride, padding, dropout):\r\n",
        "#         cnn_module = nn.Sequential(\r\n",
        "#                         nn.Conv1d(ch_in,ch_out, kernel_size, stride, padding),\r\n",
        "#                         nn.BatchNorm1d(ch_out),\r\n",
        "#                         nn.ReLU(),\r\n",
        "#                         nn.Dropout(dropout),\r\n",
        "#                         nn.Conv1d(ch_out, ch_out, 3, 1, 1),\r\n",
        "#                         nn.BatchNorm1d(ch_out)\r\n",
        "#                     )\r\n",
        "        \r\n",
        "#         return cnn_module\r\n",
        "\r\n",
        "#     def forward(self, x):\r\n",
        "#         x = self.bn_dense1(x)\r\n",
        "#         x = self.dropout_dense1(x)\r\n",
        "#         x = self.dense1(x)\r\n",
        "        \r\n",
        "#         x = x.reshape(x.size(0), self.ch_input, self.points)\r\n",
        "#         shortcut = x\r\n",
        "\r\n",
        "#         x = self.c1(x)\r\n",
        "#         bat = nn.BatchNorm1d(32)\r\n",
        "#         cut = nn.Conv1d(16,32,1,2)\r\n",
        "\r\n",
        "#         s_x = bat(cut(shortcut))\r\n",
        "#         print(s_x.shape)\r\n",
        "#         print(x.shape)\r\n",
        "\r\n",
        "#         x += s_x\r\n",
        "#         print(x.shape)\r\n",
        "#         # x = self.c2(x)\r\n",
        "\r\n",
        "#         # x = self.c2_1(x)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#         # x = self.max_pool_c1(x)\r\n",
        "#         # print(x.shape)\r\n",
        "        \r\n",
        "#         # x = self.c2(x)\r\n",
        "#         # print(x.shape)\r\n",
        "#         # x = self.c2_1(x)\r\n",
        "#         # print(x.shape)\r\n",
        "#         # x = self.c2_2(x)\r\n",
        "#         # print(x.shape)\r\n",
        "#         # x = self.max_pool_c2(x)\r\n",
        "#         # print(x.shape)\r\n",
        "\r\n",
        "#         # x = self.flatten(x)\r\n",
        "#         # print(x.shape)\r\n",
        "\r\n",
        "#         # x = self.dense2(x)\r\n",
        "#         # x = self.bn_dense2(x)\r\n",
        "#         # x = self.dropout_dense2(x)\r\n",
        "\r\n",
        "#         # x = self.dense3(x)\r\n",
        "#         # x = self.bn_dense3(x)\r\n",
        "#         # x = self.dropout_dense3(x)\r\n",
        "        \r\n",
        "#         # x = F.sigmoid(x)\r\n",
        "\r\n",
        "#         return x"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56fgdsVJhVw5"
      },
      "source": [
        "https://dnddnjs.github.io/cifar10/2018/10/09/resnet/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuNwlEY0lGA5"
      },
      "source": [
        "class IdentityPadding(nn.Module):\r\n",
        "    def __init__(self, in_channels, out_channels, stride):\r\n",
        "        super(IdentityPadding, self).__init__()\r\n",
        "\r\n",
        "        self.pooling = nn.MaxPool1d(stride)\r\n",
        "        self.add_channels = out_channels - in_channels\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        out = F.pad(x, (0, 0, 0, self.add_channels))\r\n",
        "        out = self.pooling(out)\r\n",
        "        return out"
      ],
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYVVjAynSv9B"
      },
      "source": [
        "class ResidualBlock(nn.Module):\r\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=(3,3), stride=1, padding=1, down_sample=False):\r\n",
        "      super(ResidualBlock, self).__init__()\r\n",
        "      self.in_channels = in_channels\r\n",
        "      self.out_channels = out_channels \r\n",
        "      \r\n",
        "      self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size[0], stride, padding)\r\n",
        "      self.bn1 = nn.BatchNorm1d(out_channels)\r\n",
        "      self.relu = nn.ReLU()\r\n",
        "\r\n",
        "      self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size[1], 1, 1)\r\n",
        "      self.bn2 = nn.BatchNorm1d(out_channels)\r\n",
        "\r\n",
        "      self.stride = stride\r\n",
        "\r\n",
        "      # shorcut의 input channel이 output과 비교했을 때 큰 경우\r\n",
        "      if down_sample:\r\n",
        "          self.down_sample = IdentityPadding(in_channels, out_channels, stride)\r\n",
        "      else:\r\n",
        "          self.down_sample = None\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "      shortcut = x\r\n",
        "      out = self.conv1(x)\r\n",
        "      out = self.bn1(out)\r\n",
        "      out = self.relu(out)\r\n",
        "\r\n",
        "      out = self.conv2(out)\r\n",
        "\r\n",
        "      out = self.bn2(out)\r\n",
        "\r\n",
        "      if self.down_sample is not None:\r\n",
        "        shortcut = self.down_sample(x)\r\n",
        "        out += shortcut\r\n",
        "      \r\n",
        "      # out = self.relu(out)\r\n",
        "      return out"
      ],
      "execution_count": 482,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA3FWg93p9bA"
      },
      "source": [
        "class ResNet(nn.Module):\r\n",
        "    def __init__(self, num_layers, kernel_sizes, strides, paddings, block, num_features, hidden_size, num_targets):\r\n",
        "      super(ResNet,self).__init__()\r\n",
        "      self.num_layers = num_layers\r\n",
        "      self.hidden_size = hidden_size\r\n",
        "      self.num_targets = num_targets  \r\n",
        "      self.init_channels = 16\r\n",
        "\r\n",
        "      self.bn_d0 =  nn.BatchNorm1d(num_features)\r\n",
        "      self.dropout_d0 = nn.Dropout(0.2)\r\n",
        "\r\n",
        "      self.dense1 = nn.Linear(num_features, hidden_size)\r\n",
        "      self.bn_d1 = nn.BatchNorm1d(hidden_size)\r\n",
        "      self.f_d1 = nn.ReLU()\r\n",
        "      self.dropout_d1 = nn.Dropout(0.2)\r\n",
        "      self.shortcut_d1 = IdentityPadding(self.init_channels, self.init_channels * 2, 2)\r\n",
        "\r\n",
        "      self.c1 = self.get_layers(num_layers[0], block, self.init_channels, self.init_channels * 2, kernel_sizes[0], strides[0], paddings[0])\r\n",
        "      self.shortcut_c1 = IdentityPadding(self.init_channels * 2, self.init_channels * 4, 2)\r\n",
        "      self.f_c1 = nn.ReLU()\r\n",
        "\r\n",
        "      self.c2 = self.get_layers(num_layers[1], block, self.init_channels * 2, self.init_channels * 4 , kernel_sizes[1], strides[1], paddings[1])\r\n",
        "      self.shortcut_c2 = IdentityPadding(self.init_channels * 4, self.init_channels * 8, 2)\r\n",
        "      self.f_c2 = nn.ReLU()\r\n",
        "      \r\n",
        "      self.c3 = self.get_layers(num_layers[2], block, self.init_channels * 4, self.init_channels * 8, kernel_sizes[2], strides[2], paddings[2])\r\n",
        "      self.f_c3 = nn.ReLU()\r\n",
        "\r\n",
        "      self.avg_pool = nn.AvgPool1d(2)\r\n",
        "      self.flt = nn.Flatten()\r\n",
        "\r\n",
        "      self.dense2 = nn.Linear(256,64)\r\n",
        "      self.bn_d2 = nn.BatchNorm1d(64)\r\n",
        "      self.f_d2 = nn.ReLU()\r\n",
        "      self.dropout_d2 = nn.Dropout(0.2)\r\n",
        "\r\n",
        "      self.dense3 = nn.Linear(64,num_targets)\r\n",
        "\r\n",
        "    def get_layers(self, num_layer, block, in_channels, out_channels, kernel_size=(3,3), stride=1, padding=1):\r\n",
        "      if stride > 1:\r\n",
        "        down_sample = True\r\n",
        "      else:\r\n",
        "        down_sample = False\r\n",
        "\r\n",
        "      layers_list = nn.ModuleList(\r\n",
        "          [block(in_channels, out_channels, kernel_size, stride, padding, down_sample=False)]\r\n",
        "      )\r\n",
        "      for _ in range(num_layer-1):\r\n",
        "          layers_list.append(nn.ReLU())\r\n",
        "          layers_list.append(block(out_channels,out_channels))\r\n",
        "\r\n",
        "      return nn.Sequential(*layers_list)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "      x = self.bn_d0(x)\r\n",
        "      x = self.dropout_d0(x)\r\n",
        "\r\n",
        "      x = self.dense1(x)\r\n",
        "      x = self.bn_d1(x)\r\n",
        "      x = self.f_d1(x)\r\n",
        "      x = self.dropout_d1(x)\r\n",
        "\r\n",
        "      x = x.reshape(x.size(0), self.init_channels, int(self.hidden_size/self.init_channels))\r\n",
        "      short_cut_d1 = self.shortcut_d1(x)\r\n",
        "      x = self.c1(x)\r\n",
        "      x += short_cut_d1\r\n",
        "      x = self.f_c1(x)\r\n",
        "\r\n",
        "      short_cut_c1 = self.shortcut_c1(x)\r\n",
        "      x = self.c2(x)\r\n",
        "      x += short_cut_c1\r\n",
        "      x = self.f_c2(x)\r\n",
        "\r\n",
        "      short_cut_c2 = self.shortcut_c2(x)\r\n",
        "      x = self.c3(x)\r\n",
        "      x += short_cut_c2\r\n",
        "      x = self.f_c3(x)\r\n",
        "\r\n",
        "      x = self.avg_pool(x)\r\n",
        "\r\n",
        "      x = self.flt(x)\r\n",
        "\r\n",
        "      x = self.dense2(x)\r\n",
        "      x = self.bn_d2(x)\r\n",
        "      x = self.f_d2(x)\r\n",
        "      x = self.dropout_d2(x)\r\n",
        "\r\n",
        "      x = self.dense3(x)\r\n",
        "\r\n",
        "      return x"
      ],
      "execution_count": 496,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vha08RV--SV"
      },
      "source": [
        "block = ResidualBlock"
      ],
      "execution_count": 497,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkTvCLQQ9GNG"
      },
      "source": [
        "# resnet= ResNet(num_layers=[1,3,5], kernel_sizes=[(5,5),(3,3),(3,3)], strides=[2,2,2], paddings=[4,1,1], block=block, num_features=130, hidden_size=512, num_targets=5)"
      ],
      "execution_count": 498,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u1EwKBqOfDf",
        "outputId": "4c6a77d8-2cfa-4687-8963-2ede7fd724d7"
      },
      "source": [
        "# resnet"
      ],
      "execution_count": 499,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (bn_d0): BatchNorm1d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout_d0): Dropout(p=0.2, inplace=False)\n",
              "  (dense1): Linear(in_features=130, out_features=512, bias=True)\n",
              "  (bn_d1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (f_d1): ReLU()\n",
              "  (dropout_d1): Dropout(p=0.2, inplace=False)\n",
              "  (shortcut_d1): IdentityPadding(\n",
              "    (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (c1): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(4,))\n",
              "      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (shortcut_c1): IdentityPadding(\n",
              "    (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (f_c1): ReLU()\n",
              "  (c2): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ReLU()\n",
              "    (2): ResidualBlock(\n",
              "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): ReLU()\n",
              "    (4): ResidualBlock(\n",
              "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (shortcut_c2): IdentityPadding(\n",
              "    (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (f_c2): ReLU()\n",
              "  (c3): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ReLU()\n",
              "    (2): ResidualBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): ReLU()\n",
              "    (4): ResidualBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): ReLU()\n",
              "    (6): ResidualBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (7): ReLU()\n",
              "    (8): ResidualBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (f_c3): ReLU()\n",
              "  (avg_pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
              "  (flt): Flatten(start_dim=1, end_dim=-1)\n",
              "  (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
              "  (bn_d2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (f_d2): ReLU()\n",
              "  (dropout_d2): Dropout(p=0.2, inplace=False)\n",
              "  (dense3): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 499
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kfJcmES4naWC"
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 4096\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = ResNet(num_layers=[1,3,5], kernel_sizes=[(5,5),(3,3),(3,3)], strides=[2,2,2], paddings=[4,1,1], block=block, num_features=130, hidden_size=512, num_targets=5)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 504,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dddmmApqmYv1",
        "outputId": "c726482b-0050-4d29-d59c-47d5d16f35f6"
      },
      "source": [
        "model"
      ],
      "execution_count": 505,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (bn_d0): BatchNorm1d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout_d0): Dropout(p=0.2, inplace=False)\n",
              "  (dense1): Linear(in_features=130, out_features=512, bias=True)\n",
              "  (bn_d1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (f_d1): ReLU()\n",
              "  (dropout_d1): Dropout(p=0.2, inplace=False)\n",
              "  (shortcut_d1): IdentityPadding(\n",
              "    (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (c1): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(4,))\n",
              "      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (shortcut_c1): IdentityPadding(\n",
              "    (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (f_c1): ReLU()\n",
              "  (c2): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ReLU()\n",
              "    (2): ResidualBlock(\n",
              "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): ReLU()\n",
              "    (4): ResidualBlock(\n",
              "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (shortcut_c2): IdentityPadding(\n",
              "    (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (f_c2): ReLU()\n",
              "  (c3): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ReLU()\n",
              "    (2): ResidualBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): ReLU()\n",
              "    (4): ResidualBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): ReLU()\n",
              "    (6): ResidualBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (7): ReLU()\n",
              "    (8): ResidualBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (f_c3): ReLU()\n",
              "  (avg_pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
              "  (flt): Flatten(start_dim=1, end_dim=-1)\n",
              "  (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
              "  (bn_d2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (f_d2): ReLU()\n",
              "  (dropout_d2): Dropout(p=0.2, inplace=False)\n",
              "  (dense3): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 505
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoogQrbqutcV"
      },
      "source": [
        "# model(torch.tensor(dummy))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-JECNElhnaWC"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) \n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 506,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3a8JrIkVnaWC"
      },
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 507,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWah5LQJ3oRE"
      },
      "source": [
        "class EarlyStopping:\r\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\r\n",
        "        self.patience = patience\r\n",
        "        self.counter = 0\r\n",
        "        self.mode = mode\r\n",
        "        self.best_score = None\r\n",
        "        self.early_stop = False\r\n",
        "        self.delta = delta\r\n",
        "        if self.mode == \"min\":\r\n",
        "            self.val_score = np.Inf\r\n",
        "        else:\r\n",
        "            self.val_score = -np.Inf\r\n",
        "\r\n",
        "    def __call__(self, epoch_score, model, model_path):\r\n",
        "\r\n",
        "        if self.mode == \"min\":\r\n",
        "            score = -1.0 * epoch_score\r\n",
        "        else:\r\n",
        "            score = np.copy(epoch_score)\r\n",
        "\r\n",
        "        if self.best_score is None:\r\n",
        "            self.best_score = score\r\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\r\n",
        "        elif score < self.best_score: #  + self.delta\r\n",
        "            self.counter += 1\r\n",
        "            # print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\r\n",
        "            if self.counter >= self.patience:\r\n",
        "                self.early_stop = True\r\n",
        "        else:\r\n",
        "            self.best_score = score\r\n",
        "            # ema.apply_shadow()\r\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\r\n",
        "            # ema.restore()\r\n",
        "            self.counter = 0\r\n",
        "\r\n",
        "    def save_checkpoint(self, epoch_score, model, model_path):\r\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\r\n",
        "            print(f\"Validation score improved ({self.val_score:.4f} --> {epoch_score:.4f}). Saving model!\")\r\n",
        "            # if not DEBUG:\r\n",
        "            torch.save(model.state_dict(), model_path)\r\n",
        "        self.val_score = epoch_score"
      ],
      "execution_count": 508,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "1lCkzFX_naWC",
        "outputId": "edb21d02-b057-46a5-f39b-dfac951902f7"
      },
      "source": [
        "NFOLDS = 5\n",
        "EARLYSTOP_NUM = 7\n",
        "CACHE_PATH = model_home\n",
        "\n",
        "for _fold in range(NFOLDS):\n",
        "    print(f'Fold{_fold}:')\n",
        "    seed_everything(seed=entire_seed+_fold)\n",
        "    torch.cuda.empty_cache()\n",
        "    model = Model_1DCNN(num_features=130, num_targets=5, hidden_size=512)\n",
        "    model = model.to(device)\n",
        "\n",
        "    es = EarlyStopping(EARLYSTOP_NUM, mode=\"max\")\n",
        "    for epoch in tqdm_notebook(range(epochs)):\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        running_auc = 0.0\n",
        "        model.train()\n",
        "        \n",
        "        for idx, (inputs, labels) in enumerate(train_dataloader):\n",
        "        \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            true = labels.detach().cpu().numpy()[:,-1]\n",
        "            target = np.array(list(map(lambda x: 1 if x > 0.5 else 0, outputs.detach().cpu().numpy()[:,-1])),dtype=np.float)\n",
        "            \n",
        "            acc = (true == target).sum() / outputs.shape[0]\n",
        "            auc = roc_auc_score(true, outputs.detach().cpu().numpy()[:,-1])\n",
        "    \n",
        "            running_acc += acc\n",
        "            running_auc += auc\n",
        "\n",
        "            loss = criterion(outputs,labels)\n",
        "            running_loss += loss.detach().item() * inputs.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "        epoch_acc = running_acc / len(train_dataloader)\n",
        "        epoch_auc = running_auc / len(train_dataloader)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            running_auc = 0.0\n",
        "            for idx, (inputs, labels) in enumerate(valid_dataloader):\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                true = labels.detach().cpu().numpy()[:,-1]\n",
        "                target = np.array(list(map(lambda x: 1 if x > 0.5 else 0, outputs.detach().cpu().numpy()[:,-1])),dtype=np.float)\n",
        "                \n",
        "                acc = (true == target).sum() / outputs.shape[0]\n",
        "                auc = roc_auc_score(true, outputs.detach().cpu().numpy()[:,-1])\n",
        "\n",
        "                running_acc += acc\n",
        "                running_auc += auc\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.detach().item() * inputs.size(0)\n",
        "                \n",
        "            valid_loss = running_loss / len(valid_dataloader.dataset)\n",
        "            valid_acc = running_acc / len(valid_dataloader)\n",
        "            valid_auc = running_auc / len(valid_dataloader)\n",
        "\n",
        "        print(f\"EPOCH:{epoch+1}|{epochs}; loss(train/valid):{epoch_loss:.4f}/{valid_loss:.4f}; acc(train/valid):{epoch_acc:.4f}/{valid_acc:.4f}; auc(train/valid):{epoch_auc:.4f}/{valid_auc:.4f}\")\n",
        "        \n",
        "        model_weights = os.path.join(model_home,f\"online_model_{_fold}.pth\")\n",
        "        es(valid_auc, model, model_path=model_weights)\n",
        "        if es.early_stop:\n",
        "          print(\"Early stopping\")\n",
        "          break"
      ],
      "execution_count": 509,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold0:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-509-b52020b65740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_fold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNFOLDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Fold{_fold}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentire_seed\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0m_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_1DCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_targets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seed_everything' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "Pj-ubF0enaWC",
        "outputId": "c7126c66-fe9d-4518-9d6c-5dc5c4a945e8"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import janestreet\n",
        "env = janestreet.make_env()\n",
        "\n",
        "# learn.model.eval()\n",
        "preds = []\n",
        "for (test_df, pred_df) in tqdm_notebook(env.iter_test()):\n",
        "    if test_df['weight'].item() > 0:\n",
        "        test_np = test_df.loc[:, features].values\n",
        "        test_np[:, 1:] = for_loop(fillna_npwhere_njit, test_np[:, 1:], f_mean)\n",
        "        pred = torch.mean(model(torch.tensor(test_np, dtype=torch.float).cuda(device))).item()\n",
        "        preds.append(pred)\n",
        "        action = 1 if pred >= .5 else 0\n",
        "        pred_df.action = action\n",
        "    else:\n",
        "        pred_df.action = 0\n",
        "    env.predict(pred_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-bd6a4cb6b875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjanestreet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjanestreet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janestreet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IDwfbrhunaWD"
      },
      "source": [
        "preds = np.array(preds)\n",
        "preds.mean(), preds.std(), sum(preds >= .5), sum(preds < 5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}